Тест производительности OpenCL-Benchmark
==============================


## Сборка и запуск

Сборка теста выполняется с использованием Makefile. В оригинальной версии используются библиотеки OpenCL в составе портируемого пакета, в нашей версии используются системные библиотеки. Для запуска теста необходимо установить `opencl-headers` и `opencl-icd` и драйвера от производителя. 

* см. [OpenCL-Benchmark](https://github.com/ProjectPhysX/OpenCL-Benchmark)

MSYS2: Установка под Windows префикс `ucrt64/mingw-w64-ucrt-x86_64-` или `clang64/mingw-w64-clang-x86_64-`
```sh
pacman -S %{PREFIX}-opencl-clhpp   #    OpenCL C++ header files
pacman -S %{PREFIX}-opencl-headers #    OpenCL (Open Computing Language) header files
pacman -S %{PREFIX}-opencl-icd     #    OpenCL ICD Loader
```

Linux (Ubuntu 24, 25)
```sh
apt install opencl-headers 
apt install opencl-c-headers 
apt install opencl-clhpp-headers 
apt install ocl-icd-opencl-dev
clinfo
```
-- условием нормальной работы является отображение устройства в `clinfo`

Компиляция с использованием `g++`, при запуске используется порядковый номер карты (0..N).
```sh
$ make
$ ./benchmark 0 1
```
```text
|----------------.------------------------------------------------------------|
| Device ID    0 | Intel(R) Arc(TM) A770 Graphics                             |
| Device ID    1 | Intel(R) Arc(TM) A770 Graphics                             |
|----------------'------------------------------------------------------------|
|----------------.------------------------------------------------------------|
| Device ID      | 0                                                          |
| Device Name    | Intel(R) Arc(TM) A770 Graphics                             |
| Device Vendor  | Intel(R) Corporation                                       |
| Device Driver  | 25.31.34666 (Linux)                                        |
| OpenCL Version | OpenCL C 3.0                                               |
| Compute Units  | 512 at 2400 MHz (4096 cores, 19.661 TFLOPs/s)              |
| Memory, Cache  | 8127 MB VRAM, 16384 KB global / 64 KB local                |
| Buffer Limits  | 3860 MB global, 3953458 KB constant                        |
|----------------'------------------------------------------------------------|
| Info: OpenCL C code successfully compiled.                                  |
| FP64  compute                                          not supported        |
| FP32  compute                                        12.152 TFLOPs/s (2/3 ) |
| FP16  compute                                        18.241 TFLOPs/s ( 1x ) |
| INT64 compute                                         1.147  TIOPs/s (1/16) |
| INT32 compute                                         5.760  TIOPs/s (1/3 ) |
| INT16 compute                                        34.046  TIOPs/s ( 2x ) |
| DP4A  compute                                        36.200  TIOPs/s ( 2x ) |
| SUM   compute                                         1.625  TBOPs/s (1/12) |
| MOD   compute                                         6.163  TBOPs/s (1/3 ) |
| TNN   compute                                         5.325  TBOPs/s (1/4 ) |
| MMA.h compute                                       210.865  TIOPs/s ( 12x) |
| Memory Bandwidth ( block read          )                       1605.63 GB/s |
| Memory Bandwidth ( block write         )                       3608.06 GB/s |
| Memory Bandwidth ( coalesced read      )                        239.34 GB/s |
| Memory Bandwidth ( coalesced      write)                        405.01 GB/s |
| Memory Bandwidth (misaligned read      )                        394.13 GB/s |
| Memory Bandwidth (misaligned      write)                        433.52 GB/s |
| PCIe   Bandwidth (send                 )                          7.76 GB/s |
| PCIe   Bandwidth (   receive           )                          8.48 GB/s |
| PCIe   Bandwidth (        bidirectional)            (Gen3 x16)    9.25 GB/s |
|-----------------------------------------------------------------------------|
```

Мы ориентируемся на поддержку спецификации OpenCL C 3.0, минимальные требования OpenCL C 2.1 Обусловлены использованием групповых операций
`__opencl_c_subgroups` и `cl_khr_subgroups`. Групповые операции играют центральную роль и отвечают наивысшей производительности, учитывают архитектуру вычислительного ядра CU GPU. 


**Философия теста**

Производительность в рабочей группе (CU - вычислительного узла в GPU) ограничена производительностью операций с памятью. Тесты производительности операций строятся таким образом, чтобы использовать только приватные переменные и избегать операций с памятью внутри цикла. 

В тестах используются операции с рабочей группой для повышения эффективности загрузки. Надо понимать что оптимизированный код всегда содержит операции в рабочей группе. Рабочая группа использует общий кэш, GRF (общий регистровый файл). Загрузка регистров выполняется по строкам GRF в котором каждому ядру соответствует свой индекс в векторе. Если не учитывать правила загрузки данных в рабочей группе производительность операции падает катастрофически. Для тестирования пропускной способности памяти используется несколько режимов. Мы добавили в тест режим блочного чтения/записи. 

### FMA - fused multiply accumulate

Методика тестирования производительности операций с плавающей точкой FP64, FP32 FP16, основана на запутывании компилятора через две взаимосвязанных операции в цикле, чтобы компилятор не выполнял оптимизацию (kernel). Для запуска операций требуется не менее 1024 циклов иначе производительность существенно зависит от числа циклов. При малом числе циклов наблюдается снижение производительности. Операции с разрядностью меньше 32 запускаются в виде вектора, например  `half2` или `short2`. При этом число арифметических операций в расчете удваивается.

Пример методики для 
```c
	for(uint i=0u; i<512u; i++) {
		x = fma(y, x, y); // 2 operations
		y = fma(x, y, x); // 2 operations
	}
```
Операции FMA рассматриваются, как две арифметические операции: умножение и сложение.

### IFMA - Integer fused multiply accumulate

### DP4A - Integer Dot Product 

Требуется поддержка `__opencl_c_integer_dot_product_input_4x8bit` и `cl_khr_integer_dot_product`
Операция над упакованными данными может выполняться в одну инструкцию.

### SUM - nonlinear bit algorithm

Для теста операций типа: сдвиг, ротация бит и побитовых логических операций используются примеры нелинейных операции из SHA256
```c
static inline uint Sum0(uint x) {
	return rotate(x,32u-2) ^ rotate(x,32u-13) ^ rotate(x,32u-22);
}
static inline uint Sum1(uint x) {
	return rotate(x,32u-6) ^ rotate(x,32u-11) ^ rotate(x,32u-25);
}
static inline uint Sbox(uint x) {
	return (x ^ (rotate(~x,1u) & rotate(x,2u)));
}
```
Производительность оценивается по числу таких операций в цикле
```c
	for(uint i=0u; i<512u; i++) {
		x = Sum0(x)+y;
		y = Sum1(y)+x;
  }
```
### MOD - Modular Arithmetic

Операции модульной арифметики используются при редуцировании по модулю простого числа вида $P = A \cdot 2^{16}+1$, простые числа Прота. 
```c
	for(uint i=0u; i<256u; i++) {
		short2 d = as_short2(data[ix]);
		x+= d.s0;
		x = as_short2(x).s1 - as_ushort2(x).s0*A; // 2 operations
		x+= d.s1;
		x = as_short2(x).s1 - as_ushort2(x).s0*A; // 2 operations
	}
```
Модульная арифметика основана на операциях редуцирования Montgomery и Barrett. Для модулей специального вида применяются операции редуцирования специального вида построенные на операциях `mul_hi`, `mul` и `mad_hi`. Для редуцирования Монтгомери и Барретта требуется не менее 3х операций умножения и сложения и коррекция результата. В данном случае используется оптимизированная операция редуцирования, которая пригодна для генерации последовательности случайных чисел, контрольных сумм и кодов коррекции ошибок ECC. 

### TNN - Ternary weights

Операция в поддержку нейросетей с троичной логикой. Операция использует `popcount()` и логические битовые операции. Число циклов рассчитывается по числу операций `popcount()` в цикле.
```c
  ushort2 r0 = x&y;
  ushort2 r1 = x&y.s10;
  d += popcount(r0);
  d -= popcount(r1);
```
Арифметическая операция собрана для QNN квантизованных нейронных сетей с тернарными весовыми коэффициентами $\mathbb{T}:\{-1,0,1\}$. Значения кодируются двумя битами $(x^{+},x^{-})$: $-1=(0,1), 0=(0,0), 1=(1,0)$.
Операция умножения элементов векторов рассчитывается, как 
```math
\begin{aligned}
(c^{+},c^{-}) &= (a^{+},a^{-})\cdot (a^{+},a^{-})~, \text{ где } \\
c^{+} &= (a^{+} \land b^{+}) \lor (a^{-} \land b^{-})\\
c^{-} &= (a^{+} \land b^{-}) \lor (a^{-} \land b^{+})
\end{aligned}
```

Операция _dot product_ определяется через подсчет числа единиц в результате троичного умножения с использованием инструкции `popcount()`. 
```math
\langle a,b\rangle = \sum_{i=0}^{N-1} a_i b_i = \sum_{i=0}^{N-1} c^{+} - \sum_{i=0}^{N-1} c^{-}
```
### MMA - Matrix Multiply and Accumulate

Матричные (тензорные) операции с весовыми коэффициентами пониженной разрядности 
* `MMA.h`  - использует коэффициенты FP16 (`half`), Float16 E5M10
* `MMA.tf` - TensorFloat32 E8M10 (Intel B580)
* `MMA.bf` - BFloat16 E8M7
* `MMA.f8` - Float8  E4M3 (в настоящей версии не реализован)
* `MMA.b8` - BFloat8 E5M2 (в настоящей версии не реализован)
* `MMA.f4` - MX Float4 E2M1 операции с общей экспонентой
* `MMA.i8` - над байтами- основная операция при квантизации `Q8_1`, результат операции `i32`.
* `MMA.i4` - Целочисленные матричные операции могут выполняться в комбинации, например `u4_i8`. 
* `MMA.i2` - использует коэффициенты i2 - бинарные коэффициенты со знаком, соответствуют троичной логике. 

Тензорные ядра построены на 10 битных операциях с мантиссой, с использованием экспоненты. Целочисленные операции выполняются на той же разрядности. Ускорение достигается за счет параллельного исполнения нескольких операций пониженной разрядности подобно DP4A. Таким образом ускорение при загрузке тензорных операций над `i8` в два раза выше чем над `bf16`.

Матричные операции выполняются в рабочей группе на конвейере (на волне или на варпе, кто как называет). Intel называет операцией `DPAS`. Существенной особенностью выполнения операций является оптимизированный характер загрузки общих векторов. Эмуляция операции выполняется с использованием _dot product_ и `sub_group_broadcast()`. Каждое ядро в группе считает только один элемент операции _dot product_, результат операции суммируется по группе `sub_group_reduce_add()`. Документация на операции в OpenCL полностью отсутствует. Матричные операции реализуются через intrinsic и макросы производителей или никак. Макросы разгадываются методом реверс-инжиниринга через исходники описания языка в сборке компилятора LLVM или IGC.

### Memory Bandwidth block 

-- чтение/запись выполняется в рабочей группе (warp-group) в режиме Interleaved. Это наиболее производительный режим работы. Чтение реализовано с использованием операций sub_group. Требуется поддержка `cl_intel_subgroups` или  `__opencl_c_subgroups` и `cl_khr_subgroups` для OpenCL 2.1 и выше.