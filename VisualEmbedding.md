_–ì–µ–æ—Ä–≥–∏–µ–≤—Å–∫–∏–π –ê.–ú._

–í –æ–±–∑–æ—Ä–µ —Å–æ–±—Ä–∞–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∏–∑—É–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π. –¶–µ–ª—å—é —Ä–∞–±–æ—Ç—ã —è–≤–ª—è–µ—Ç—Å—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º. –ê —Ç–∞–∫–∂–µ —Å–æ–≤–º–µ—â–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∑–∞–¥–∞—á–µ–π –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤. –†–∞—Å—Å–º–æ—Ç—Ä–µ–Ω—ã –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è.

–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö. –í –Ω–∞—à–∏—Ö —Ç–µ—Å—Ç–∞—Ö –º–µ—Ç–æ–¥—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≥—Ä–∞–Ω–∏—Ü —Å–º–µ–Ω—ã —Å—Ü–µ–Ω—ã, –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–ø–æ—Ä–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ –∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –¢–∞–∫–∂–µ –º—ã –ø–ª–∞–Ω–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º –æ–ø–æ—Ä–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ –≤ –≤–∏–¥–µ–æ –ø–æ—Ç–æ–∫–µ. 


**LLM** - Large Language Models, —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Transformer.\
**MLLM** - Multimodal Large Language Models\
**VLM –∏ VLA** - Visual Language and Actions –º–æ–¥–µ–ª–∏\
**ViT** - Vision Transformer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\
**CNN** - Convolutional Neural Networks\
**RNN** - Recurrent Neural Networks: GRU, LSTM, STU\
**PINN** - Physic-informed Neural Networks\
**U-Net** - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–≤–µ—Ä—Ç–æ—á–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\
**KSN** - Kolmogorov-Spline Network –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\
**KAN** - Kolmogorov-Arnold Network –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞


## VLM –∏ VLA - Visual Language and Actions –º–æ–¥–µ–ª–∏

VLA –º–æ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –≤ –∑–∞–¥–∞—á–∞—Ö –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–æ–±–æ—Ç–∞–º–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º–∏ –∏ –≤ –∑–∞–¥–∞—á–∞—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –±–µ—Å–ø–∏–ª–æ—Ç–Ω—ã–º –ª–µ—Ç–∞—Ç–µ–ª—å–Ω—ã–º –∞–ø–ø–∞—Ä–∞—Ç–æ–º. –ú–µ—Ç–æ–¥ —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å–µ—Ç—è—Ö, –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ —ç—Ç–æ —Å–µ—Ç–∏ CNN –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CNN —Å–µ—Ç–µ–π –≤–æ–∑–º–æ–∂–Ω–æ –æ–±—É—á–∏—Ç—å —Å–µ—Ç–∏ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π ViT –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Å–ª–æ–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –∫–∞–Ω–∞–ª–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –í—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–µ—à–∞–µ—Ç—Å—è –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ KSN (—Å–ø–ª–∞–π–Ω–æ–≤—ã—Ö —Å–µ—Ç—è—Ö –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤–∞). –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ø–ª–∞–π–Ω–æ–≤—ã—Ö —Å–µ—Ç–µ–π - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–µ—Ç –∑–∞–º–µ–Ω–∏—Ç—å MLP –Ω–∞ KAN. –ü–æ—á–µ–º—É?! –ü–æ—Ç–æ–º—É —á—Ç–æ –µ—Å—Ç—å –ø—Ä—è–º–æ–π –ø—É—Ç—å –º–µ–∂–¥—É –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–º –≤ –≤–∏–¥–µ —Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π —Å–µ—Ç–∏. 

–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è VLA –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –¥–≤—É—Ö (–∏–ª–∏ –±–æ–ª–µ–µ) –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –¥–≤–∏–∂–µ–Ω–∏—è, –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é (LLM). –í—ã—Ö–æ–¥ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–µ–∫–æ–¥–µ—Ä –∫–æ–º–∞–Ω–¥ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–≤–æ–¥–∞–º–∏. 
–ù–∏–∂–µ –ø—Ä–∏–≤–æ–∂—É –ø–æ–¥–±–æ—Ä–∫—É —Å—Ç–∞—Ç–µ–π –ø–æ –≤–∏–∑—É–∞–ª—å–Ω—ã–º –º–æ–¥–µ–ª—è–º –¥–ª—è –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏—è –≤ –∫–æ–Ω–µ—á–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ. –°—Ä–µ–¥–∏ –º–æ–¥–µ–ª–µ–π –≤—ã–¥–µ–ª—è—é—Ç—Å—è SmolVL2 –∏ InternVL —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π —ç–Ω–∫–æ–¥–µ—Ä. 

–í –æ–±–∑–æ—Ä–µ —É–¥–µ–ª—è–µ—Ç—Å—è –≤–Ω–∏–º–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞–º —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤—É—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –¢—É—Ç –º–æ–∂–Ω–æ –≤—ã–¥–µ–ª–∏—Ç—å –º–µ—Ç–æ–¥—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ä–∞—Å—Ç—Ä–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –∫–∞—Å–∫–∞–¥–Ω–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π CNN (—Ç.–Ω –ø–∏—Ä–∞–º–∏–¥—ã), —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–≤–ª—è–µ—Ç—Å—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, Feature map, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–æ–ª–∂–Ω–æ –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –ú–∞—Å–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã–¥–µ–ª—è—é—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ motion estimation, HOG –∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤. –†—è–¥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–∞–Ω–∞–ª—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. 

* [[1911.05722](https://arxiv.org/pdf/1911.05722)] Momentum Contrast for Unsupervised Visual Representation Learning
* [[2103.00020](https://arxiv.org/pdf/2103.00020)] Learning Transferable Visual Models From Natural Language Supervision
* [[2304.07193](https://arxiv.org/pdf/2304.07193)] DINOv2: Learning Robust Visual Features without Supervision
* [[2502.19645](https://arxiv.org/pdf/2502.19645)] Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success

–í–∏–∑—É–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –∏ —ç–Ω–∫–æ–¥–µ—Ä—ã:
* [[2406.09246](https://arxiv.org/pdf/2406.09246)] OpenVLA: An Open-Source Vision-Language-Action Model
* [[2409.12191](https://arxiv.org/pdf/2409.12191)] Qwen2-VL: Enhancing Vision-Language Model‚Äôs Perception of the World at Any Resolution
* [[2504.05299](https://arxiv.org/pdf/2504.05299)] SmolVLM: Redefining small and efficient multimodal models
* [[2504.07491](https://arxiv.org/pdf/2504.07491)] Kimi-VL Technical Report
* [[2504.10479](https://arxiv.org/pdf/2504.10479)] InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models
* [[2505.04601](https://arxiv.org/pdf/2505.04601)] OpenVision : A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning
* [[2505.22159](https://arxiv.org/pdf/2505.22159)] ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation
* [[2506.01844](https://arxiv.org/pdf/2506.01844)] SmolVLA: A vision-language-action model for affordable and efficient robotics
* [[2506.07900](https://arxiv.org/pdf/2506.07900)] MiniCPM4: Ultra-Efficient LLMs on End Device


* (https://ucsc-vlaa.github.io/OpenVision)
* (https://huggingface.co/openvla/openvla-7b)
* (https://learnopencv.com/smolvla-lerobot-vision-language-action-model/)

–ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π OpenCV, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –±—ç–∫–µ–Ω–¥–æ–º FFmpeg –∏ GStreamer –¥–ª—è –≤–≤–æ–¥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ. OpenCV –≤–∫–ª—é—á–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ (feature detection), –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–æ–Ω—Ç—É—Ä–æ–≤, –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥–æ–º–æ–≥—Ä–∞—Ñ–∏–∏, –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ OpenCV —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏.
–ú—ã –≤—ã–¥–µ–ª—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –º–µ—Ç–æ–¥—ã –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π. 

–° –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–æ–≤ OpenCV –∏ CNN –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Ä–µ—à–∞—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏:

1. [–û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–º–∫–∏](#–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ-—Ä–∞–º–∫–∏) - –ø–æ–∑–≤–æ–ª—è—é—Ç –≤—ã–¥–µ–ª–∏—Ç—å –æ–±—ä–µ–∫—Ç—ã (–∫–ª–∞—Å—Å—ã) –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å —É—á–µ—Ç–æ–º –∏—Ö –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏. –ö–ª–∞—Å—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. 
2. [–ü—Ä–∏–Ω—Ü–∏–ø—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π](#–ø—Ä–∏–Ω—Ü–∏–ø—ã-—Å—Ä–∞–≤–Ω–µ–Ω–∏—è-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π) –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.
3. Motion estimation –í—ã–¥–µ–ª–µ–Ω–∏–µ –º–∞—Å–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Ü–µ–Ω–∫–∏ –¥–≤–∏–∂–µ–Ω–∏—è. 
4. [Semantic Segmentation](#semantic-segmentation) –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
5. [Visual Embeddings](#visual-embeddings)

## –û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–º–∫–∏

    *OBB* - oriented bounding box, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–º–∫–∞ –≥—Ä–∞–Ω–∏—Ü –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    *GBB* - Gaussian bounding box, –≥–∞—É—Å—Å–æ–≤–∞ —Ä–∞–º–∫–∞ –≥—Ä–∞–Ω–∏—Ü –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

–ú–∞—Å–∫–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è GBB –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –≥–∞—É—Å—Å–æ–≤–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –æ–ø–∏—Å—ã–≤–∞—é—â–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

* [[2007.09584](https://arxiv.org/pdf/2007.09584)] PIoU Loss: Towards Accurate Oriented
Object Detection in Complex Environments
* [[2106.06072](https://arxiv.org/pdf/2106.06072)] Gaussian Bounding Boxes and Probabilistic
Intersection-over-Union for Object Detection

* (https://github.com/zhen6618/RotaYolo)

* (https://arxiv.org/pdf/2101.11952) Rethinking Rotated Object Detection with Gaussian Wasserstein Distance Loss

{–ú–∞—Å–∫–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, –∫–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è? (L)GBB. –î–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ}
{–ü—Ä–∏—á–µ–º —Ç—É—Ç —Ä–µ–≥—Ä–µ—Å—Å–∏—è? –î–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ –ø–æ—è—Å–Ω–∏—Ç—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ}

–†–µ–≥—Ä–µ—Å—Å–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–∞—É—Å—Å–æ–≤–æ–π —Ä–∞–º–∫–∏ $(\mu_x, \mu_y, \sigma_x, \sigma_y, \theta)$ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ–π—Ä–æ—Å–µ—Ç—å (–æ–±—ã—á–Ω–æ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π CNN –∏–ª–∏ ViT) –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ü–µ–Ω—Ç—Ä–∞, —Ä–∞–∑–º–µ—Ä—ã –∏ —É–≥–æ–ª –ø–æ–≤–æ—Ä–æ—Ç–∞ —Ä–∞–º–∫–∏. –≠—Ç–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –≥–¥–µ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π –∫–ª–∞—Å—Å –æ–±—ä–µ–∫—Ç–∞. –ü—Ä–∏–º–µ—Ä: –≤ –∑–∞–¥–∞—á–µ –≤—ã–¥–µ–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ —Ä–∞–º–∫–∏ –≤–æ–∫—Ä—É–≥ –æ–±—ä–µ–∫—Ç–∞, –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–æ –≤ [2007.09584]. 

## –ì–æ–º–æ–≥—Ä–∞—Ñ–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤

–ì–æ–º–æ–≥—Ä–∞—Ñ–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –ø–æ –Ω–∞–±–æ—Ä—É –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –Ω–∞ –¥–≤—É—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö. –ö–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –≤—ã–¥–µ–ª–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ OpenCV: SURF, ORB, AKAZE. 

SURF, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –∏–º–µ–µ—Ç –ø–∞—Ç–µ–Ω—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –ø–æ—ç—Ç–æ–º—É –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö —á–∞—â–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è ORB –∏–ª–∏ AKAZE, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. 

–°–æ–≤–º–µ—â–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –º–µ—Ç–æ–¥–æ–º RANSAC. –ì–æ–º–æ–≥—Ä–∞—Ñ–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥–ª—É–±–∏–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. 

* (https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html)
* (https://docs.opencv.org/4.x/dc/d16/tutorial_akaze_tracking.html)

## Feature Extraction
* [[1408.5093](https://arxiv.org/pdf/1408.5093)] Caffe: Convolutional Architecture for Fast Feature Embedding

* [[1504.06066](https://arxiv.org/pdf/1504.06066)] Object Detection Networks on Convolutional Feature Maps
* [[1505.04597](https://arxiv.org/pdf/1505.04597)] U-Net: Convolutional Networks for Biomedical Image Segmentation
>*U-Net Network Architecture*\
–í —Ä–∞–±–æ—Ç–µ —Ä–∞–∑–æ–±—Ä–∞–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏ (fig.1). It consists of a contracting
path (left side) and an expansive path (right side). The contracting path follows
the typical architecture of a convolutional network. It consists of the repeated
application of two 3x3 convolutions (unpadded convolutions), each followed by
a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2
for downsampling. At each downsampling step we double the number of feature
channels. Every step in the expansive path consists of an upsampling of the
feature map followed by a 2x2 convolution (‚Äúup-convolution‚Äù) that halves the
number of feature channels, a concatenation with the correspondingly cropped
feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in
every convolution. At the final layer a 1x1 convolution is used to map each 64-
component feature vector to the desired number of classes. 

* [[1504.08083](https://arxiv.org/pdf/1504.08083)] Fast R-CNN
* [[1506.01497](https://arxiv.org/pdf/1506.01497)] Faster R-CNN: Towards Real-Time Object
Detection with Region Proposal Networks
* [[1612.03144](https://arxiv.org/pdf/1612.03144)] Feature Pyramid Networks for Object Detection
* [[1612.06370](https://arxiv.org/pdf/1612.06370)] Learning Features by Watching Objects Move
* [[1703.06870](https://arxiv.org/pdf/1703.06870)] Mask R-CNN\
-- –º–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç - —ç—Ç–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–≤—É—Ö —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π: –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –º–∞—Å–∫–∏
* [[2112.09133](https://arxiv.org/pdf/2112.09133)] Masked Feature Prediction for Self-Supervised Visual Pre-Training\
 -- Histograms of Oriented Gradients (HOG)
* [[2408.00714](https://arxiv.org/pdf/2408.00714)] SAM 2: Segment Anything in Images and Videos

* (https://learnopencv.com/histogram-of-oriented-gradients/)

HOG: –ú–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ —Å–≤–µ—Ä—Ç–∫–∏ (–ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ) $[-1,0,1]$ –∏ $[-1,0,1]^{\mathsf{T}}$ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ ${g_x, g_y}$. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–∞—Å–∫–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –≤–∏–¥–µ —Ü–≤–µ—Ç–æ–≤–æ–π —Å—Ö–µ–º—ã HSV (Magnitude + angle).

–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏–º–µ—é—Ç —Å–≤–µ—Ä—Ç–æ—á–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–µ—Ç–∏ —Ç–∏–ø–∞ U-Net –∏–ª–∏ FPN(feature pyramid network). –í —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Å–µ—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –ø–∏—Ä–∞–º–∏–¥–∞, –≤ –∫–æ—Ç–æ—Ä–æ–π —á–µ—Ä–µ–¥—É—é—Ç—Å—è —Å–ª–æ–∏ `Conv 3x3`, `MaxPool 2x2`, –ò –ø–∏—Ä–∞–º–∏–¥–∞ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –≤ –∫–æ—Ç–æ—Ä–æ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `UpSample` –∏ `Conv 2x2`. –ü–æ–¥–æ–±–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω–∞ –∏–∑ —Å—Ç—Ä–æ–≥–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø—É—Ç–µ–º —Å–∏–Ω—Ç–µ–∑–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤. 

–ü—Ä–∏–Ω—Ü–∏–ø—ã —Å–∏–Ω—Ç–µ–∑–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –¥–≤—É–º–µ—Ä–Ω—ã—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –≤ [–∫—É—Ä—Å–µ DSP](#) - —Ü–∏—Ñ—Ä–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞—à–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ - —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –Ω–∞ —Å–µ–∫—Ü–∏–∏ –≤—Ç–æ—Ä–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ –∏ –Ω–∞ –±–∞–∑–∏—Å–Ω—ã–µ –ø–æ–ª–∏–Ω–æ–º—ã.

–ü—É—Ç—å –¥–ª—è –æ—Å–≤–æ–µ–Ω–∏—è –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –≤ —Å–≤–æ–∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞—Ö, –≤ –º–æ–µ–º –≤–∏–¥–µ–Ω–∏–∏, –ª–µ–∂–∏—Ç —á–µ—Ä–µ–∑ —Ä–∞–∑–±–æ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ ONNX –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —ç–∫—Å–ø–æ—Ä—Ç–∞ –º–æ–¥–µ–ª–µ–π YOLO –≤ ONNX. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Edge- —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö –Ω–∞ –±–∞–∑–µ RockChip [—Å–µ—Ç—è—Ö RKNN](https://docs.ultralytics.com/ru/integrations/rockchip-rknn/). 

–ú–æ–¥–µ–ª–∏ YOLO –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–æ–±—É—á–µ–Ω—ã –∏ –≤—Å—Ç—Ä–æ–µ–Ω—ã –≤ –ø—Ä–æ–≥—Ä–∞–º–º—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RKNN Toolkit.
```sh
yolo export model=yolo11n.pt format=rknn name=rk3588 #
```

–ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –¥–ª—è feature extraction —è –±—ã –Ω–∞—á–∏–Ω–∞–ª —Å –∏–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π. 
1. –°–≤–µ—Ä—Ç–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ HOG - –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–º–∏ –∏ –í-—Å–ø–ª–∞–π–Ω–∞–º–∏. –î–≤—É–º–µ–Ω—Ä–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–≥–æ (bi-—Å–ø–ª–∞–π–Ω—ã) –ø–æ –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤—É. –ü—Ä–æ—Å—Ç–µ–π—à–∏–π –ø—Ä–∏–º–µ—Ä [–±–∏–ª–∏–Ω–µ–π–Ω–æ–µ –∏ –±–∏–∫—É–±–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ](bicubic.c). 
2. –°–≤–µ—Ä—Ç–∫–∏ –≤ –ø–∏—Ä–∞–º–∏–¥–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª—É—á–µ–Ω—ã –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏ –∏ –¥–æ–æ–±—É—á–µ–Ω—ã. 
3. –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ KSN/KAN –∏ —Å–æ–≤–º–µ—â–∞—Ç—å –≤ —Å–µ–±–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ñ–∏–∑–∏–∫–∏, –≤ –ø—Ä–æ—Å—Ç–µ–π—à–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –ø–æ–¥–æ–±–Ω–æ–π GRU –∏–ª–∏ State-Space –º–æ–¥–µ–ª–∏ (SSM). –ü–µ—Ä–µ—Ö–æ–¥ –æ—Ç —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫ State-Space –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ time-mix –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞. 

## –ü—Ä–∏–Ω—Ü–∏–ø—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

The goal of text embedding models is to convert variable length text into a fixed vector, encoding the text semantics into a multidimensional vector in such a way that semantically close texts are close in the vector space, while dissimilar texts have a low similarity.

* [[2502.20204](https://arxiv.org/pdf/2502.20204)] Granite Embedding Models, IBM team, 2025
* [[2506.18902](https://arxiv.org/pdf/2506.18902)] jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval

–ü—Ä–∏–º–µ—Ä. –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π LLM –¥–ª—è –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –¥–ª—è –∑–∞–¥–∞—á–∏ Embedding –∏ Reranking. –û—Ç–ø—Ä–∞–≤–Ω–∞—è —Ç–æ—á–∫–∞ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ Embedding –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –Ω–∞ –ø–æ—Ç–æ–∫–µ. –ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –æ—Ç–∫—Ä—ã—Ç—ã –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

* [[2505.09388](https://arxiv.org/pdf/2505.09388)] Qwen3 Technical Report
* [[2506.05176](https://arxiv.org/pdf/2506.05176)] Qwen3 Embedding: Advancing Text Embedding and
Reranking Through Foundation Models

–ë–∞–∑–æ–≤–∞—è –∏–¥–µ—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è LLM-—Å–µ—Ç–µ–π –∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Å–≤–æ–¥—è—Ç—Å—è –∫ –º–Ω–æ–≥–æ–∫–∞—Å–∫–∞–¥–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —Ñ—É–Ω–∫—Ü–∏–∏ softmax, –∫–æ—Ç–æ—Ä–∞—è —è–≤–ª—è—è—Å—å —á–∞—Å—Ç–Ω—ã–º —Å–ª—É—á–∞–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏, similarity, –≤—ã–ø–æ–ª–Ω—è—é—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å (semantic similarity) –≤–µ–∫—Ç–æ—Ä–æ–≤ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (K) –∏ –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ (Q) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –∫–∞–∫ –≤–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –≤—ã–±–æ—Ä–∞ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ –∑–Ω–∞—á–µ–Ω–∏–π V. –≠—Ç–æ –∏ –µ—Å—Ç—å –ø—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã –∞—Å—Å–æ—Ü–∏–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏, –º–µ—Ö–∞–Ω–∏–∑–º Attention.

Embedding –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø–æ–ª—É—á–∞—é—Ç—Å—è –≤–µ–∫—Ç–æ—Ä–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –¥–ª–∏–Ω—ã –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏. 

–ó–∞–¥–∞—á–∞ Reranking –º–æ–¥–µ–ª–∏ - –≤—ã–¥–µ–ª–∏—Ç—å –∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ - –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ –∑–∞–ø—Ä–æ—Å—É. 

–í—ã—Ö–æ–¥ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π Embedding –º–æ–¥–µ–ª–∏ –º–æ–∂–Ω–æ –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥—Ä—É–≥–æ–π –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Ç—Ä–∏—Ü (Embedding matrix). –¢–∞–∫ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Ç—Ä–∏—Ü—ã Embedding —Å–ª–æ—è —Å–µ—Ç–∏ –∏ —Å–ª–æ–≤–∞—Ä—è. 

–í–µ–∫—Ç–æ—Ä–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω—ã–µ –≤ –µ–¥–∏–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –º–æ–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è (_inner product_) –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –Ω–∞ –µ–≥–æ –±–∞–∑–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏, –ø—Ä–æ–µ–∫—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –¥—Ä—É–≥–æ–π. –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫ –≤ –µ–≤–∫–ª–∏–¥–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —á–µ—Ä–µ–∑ –æ–ø–µ—Ä–∞—Ü–∏—é _dot product_.

–ú–æ–∂–µ—Ç –±—ã—Ç—å –∏–Ω–æ–π —Å–ø–æ—Å–æ–± —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –≤–∫–ª—é—á–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ) –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫—É –≤–µ—Å–æ–≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –†—è–¥ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –≤–∫–ª—é—á–∞—é—Ç —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–≤—É—Ö –∏ –±–æ–ª–µ–µ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–µ—Ç–æ–¥–∞ mix (–ª–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è):
```math
\text{mix}(A, B, \mu) = \mu \cdot A + (1-\mu )\cdot B
```
–°–∫–∞–ª—è—Ä–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä ($\mu$) –∏–ª–∏ –≤–µ–∫—Ç–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ $\mu$ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–µ—Å –∫–∞–∂–¥–æ–≥–æ –∏–∑ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤. –í–µ—Å –º–æ–∂–µ—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0,1] –∏ —Ç—Ä–µ–±—É–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ - –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –≤—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ.
```math
\mu = \sigma(W \cdot x + b)
```
–ë–∞–∑–æ–≤—ã–µ –∏–¥–µ–∏: 
1. –ï—Å–ª–∏ –≤—Ö–æ–¥–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —É–∂–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∞ –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∞—Ñ—Ñ–∏–Ω–Ω–æ–µ (–æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω–æ–µ) –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ, —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å. –¢–∞–∫–æ–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–µ—Ç–µ–π KAN. –°–µ—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏—è—Ö mix –æ–∫–∞–∂–µ—Ç—Å—è —Å–µ—Ç—å—é KAN. –î–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å –æ–ø–µ—Ä–∞—Ü–∏—é —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è. –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –æ–±–ª–∞–¥–∞–µ—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º, —Ç–æ –≤ —Å–æ—Å—Ç–∞–≤ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –º–æ–∂–µ—Ç –≤—Ö–æ–¥–∏—Ç—å –æ–∫–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–≤–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã). –ü—Ä–∏–º–µ—Ä–æ–º —è–≤–ª—è–µ—Ç—Å—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ (—Å–µ—Ç–∏ KAN) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–æ–ª–∏–Ω–æ–º–æ–≤ –ß–µ–±—ã—à–µ–≤–∞ –∏ –Ø–∫–æ–±–∏.
2. –û–ø–µ—Ä–∞—Ü–∏—è `softmax` –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
–í–º–µ—Å—Ç–æ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–æ–µ —è–≤–ª—è–µ—Ç—Å—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è _cross entropy loss_. –ü—Ä–∏ –∫–∞–∫–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö –æ–ø–µ—Ä–∞—Ü–∏—é —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å? –û—á–µ–≤–∏–¥–Ω–æ —á—Ç–æ –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∞–≤–∏–ª–∞ –ª–æ–≥–∞—Ä–∏—Ñ–º–æ–≤ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∏ –ª–æ–≥–∞—Ä–∏—Ñ–º–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏—è. 
3. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ - –ì–∏–ª—å–±–µ—Ä—Ç–æ–≤–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ. 
4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–µ—Ä–µ—Ö–æ–¥–∞ –º–µ–∂–¥—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –∏ —Ç–µ—Ä–Ω–∞—Ä–Ω–æ–π –ª–æ–≥–∏–∫–æ–π. –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏. 

–ú–µ—Ç–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ä–µ–∫—É—Ä—Å–∏—é, –∏ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º —É—á–∏—Ç—ã–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —à–∞–≥–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ —Å–ª—É—á–∞–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ä–∞–∑–Ω—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è - —Ä–∞–∑–Ω–∏—Ü–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ (time-mix). –†–µ–∫—É—Ä—Å–∏—è —Ç–∞–∫–æ–≥–æ —Ä–æ–¥–∞ –ø–æ—Ä–æ–∂–¥–∞–µ—Ç –º–∞—Ç—Ä–∏—á–Ω—É—é —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—É. –ú–∞—Ç—Ä–∏—á–Ω–∞—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç–∞ –≤ –º–æ–¥–µ–ª—è—Ö –æ—Ç—Å—ã–ª–∞–µ—Ç –∫ —Ñ—É–Ω–∫—Ü–∏–∏ _softmax_. 

–¢–µ—Ö–Ω–∏–∫–∞ time-mix –æ—Ç—Å—ã–ª–∞–µ—Ç –∫ –º–æ–¥–µ–ª—è–º —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (RNN) –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ –æ–ø–∏—Å—ã–≤–∞—Ç—å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏. –í —Ç–µ—Ä–º–∏–Ω–∞—Ö time-mix –º–æ–∂–Ω–æ –≤—ã—Ä–∞–∂–∞—Ç—å —É—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–∏–Ω–∞–º–∏–∫–∏ –≤ –æ–±–æ–±—â–µ–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö –∏ –≤ State-space (SSM). –≠—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –≤ –æ–±–æ–±—â–µ–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö.

–°–ª–µ–¥—É–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–µ—Ç–∏ FFN –≤ –≤–∏–¥–µ —Å–ø–ª–∞–π–Ω–æ–≤–æ–π —Å–µ—Ç–∏ KAN. –°–ø–ª–∞–π–Ω–æ–≤—ã–µ —Å–µ—Ç–∏ –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤–∞-–ê—Ä–Ω–æ–ª—å–¥–∞ KSN –ª—É—á—à–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –°–µ—Ç–∏ KAN –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏ mix –∏ –Ω–µ —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –ø–æ—Å–∫–æ–ª—å–∫—É —Å–≤–æ–π—Å—Ç–≤–æ –ø–æ–ª–∏–Ω–æ–º–æ–≤ –ë–µ—Ä–Ω—à—Ç–µ–π–Ω–∞ (B-—Å–ø–ª–∞–π–Ω–æ–≤) —Ç–∞–∫–æ–≤–æ. –¶–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –Ω–∞ —Å–ø–ª–∞–π–Ω–∞—Ö. –í —Ç–æ–∂–µ –≤—Ä–µ–º—è –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã –∏ –Ω–∞ —Å–≤–µ—Ä—Ç–∫–∞—Ö. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∏–Ω—Ç–µ—Ä–µ—Å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π CNN –∏ MLP –≤ –≤–∏–¥–µ —Å–ø–ª–∞–π–Ω–æ–≤—ã—Ö —Å–µ—Ç–µ–π KAN. 

* [[2309.04664](https://arxiv.org/pdf/1606.08415)] GAUSSIAN ERROR LINEAR UNITS (GELU)\
    -- We could use the CDF of $N (¬µ, œÉ^2)$ and have $¬µ$ and $œÉ$ be learnable parameters, but throughout this work we simply let $¬µ = 0$ and $œÉ = 1$. 
    $$\text{GELU}(x) = xP(X ‚â§ x) = x\Phi(x)$$ 

**Gaussian distribution, Normal distribution**
In probability theory and statistics, a Normal distribution or Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. 
```math
N(x)= \frac {1}{\sqrt {2\pi \sigma ^{2}}} e^{-{\cfrac {(x-\mu )^{2}}{2\sigma ^{2}}}}
```
The parameter ‚Å†$\mu$ is the mean or expectation of the distribution (and also its median and mode), while the parameter $\sigma^{2}$ is the variance. 

–ó–∞–º–µ—Ç–∏–º, —á—Ç–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–º –±—É–¥–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω—ã –ø—Ä–∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–º —á–∏—Å–ª–µ –æ–ø—ã—Ç–æ–≤. –§–æ—Ä–º—É–ª–∞ –ø–æ–ª—É—á–∞–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–µ–¥–µ–ª—å–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ (–ü—Ä–µ–¥–µ–ª—å–Ω–∞—è —Ç–µ–æ—Ä–µ–º–∞ –ú—É–∞–≤—Ä–∞-–õ–∞–ø–ª–∞—Å–∞). –ü—Ä–∏ –∫–æ–Ω–µ—á–Ω–æ–º —á–∏—Å–ª–µ –æ–ø—ã—Ç–æ–≤ –∏–º–µ–µ—Ç –º–µ—Å—Ç–æ –±–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –∏—Å–ø—ã—Ç–∞–Ω–∏–π –ë–µ—Ä–Ω—É–ª–ª–∏. –ü–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å —Ñ–æ—Ä–º—É–ª–æ–π –±–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–≤–æ–¥—è—Ç—Å—è –ø–æ–ª–∏–Ω–æ–º—ã –ë–µ—Ä–Ω—à—Ç–µ–π–Ω–∞, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º–æ–∂–Ω–æ —Å—Ç—Ä–æ–∏—Ç—å KSN (—Å–ø–ª–∞–π–Ω–æ–≤—ã–µ —Å–µ—Ç–∏ –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤–∞). –§–æ—Ä–º—É–ª–∞ –ë–µ—Ä–Ω—É–ª–ª–∏, –∫–∞–∫ –∏ –ø–æ–ª–∏–Ω–æ–º—ã –ë–µ—Ä–Ω—à—Ç–µ–π–Ω–∞ –≤—ã–≤–æ–¥—è—Ç—Å—è —á–µ—Ä–µ–∑ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ, —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–±—ã—Ç–∏—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∏—Å–ø—ã—Ç–∞–Ω–∏–∏.

**Cumulative distribution function**

The Cumulative Distribution Function (CDF) of the standard normal distribution
```math
\Phi (x)={\frac {1}{\sqrt {2\pi }}}\int _{-\infty }^{x}e^{-t^{2}/2}~dt
```

**Error function**

The related error function $\text{erf}(x)$ gives the probability of a random variable, with normal distribution of mean 0 and variance 1/2 falling in the range ‚Å†$[-x,x]$‚Å†.
```math
\text{erf} (x)={\frac {1}{\sqrt {\pi }}}\int _{-x}^{x}e^{-t^{2}}~dt
```

* [[1807.03748](https://arxiv.org/pdf/1807.03748)] Representation Learning with
Contrastive Predictive Coding

* [[2211.14438](https://arxiv.org/pdf/2211.14438)] BERN-NN: Tight Bound Propagation For Neural Networks Using Bernstein Polynomial Interval Arithmetic

* [[2309.04664](https://arxiv.org/pdf/2309.04664)] Compact: Approximating Complex Activation Functions for Secure Computation

* [[2505.24293](https://arxiv.org/pdf/2505.24293)] Large Language Models are Locally Linear Mappings
* [[2406.03865](https://arxiv.org/pdf/2406.03865)] Semantic Similarity Score for Measuring Visual Similarity at Semantic Level

1. –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (Cosine Similarity):

–ú–µ—Ç–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤—É—Ö embedding –≤–µ–∫—Ç–æ—Ä–æ–≤ (–≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤). –í—ã—á–∏—Å–ª—è–µ—Ç (–ø—Ä–æ–µ–∫—Ü–∏—é) –∫–æ—Å–∏–Ω—É—Å _—É–≥–ª–∞_ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –≤ n-–º–µ—Ä–Ω–æ–º –µ–≤–∫–ª–∏–¥–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –Ω–æ—Ä–º–∏—Ä—É—è –∏—Ö –¥–ª–∏–Ω—É. –ó–Ω–∞—á–µ–Ω–∏–µ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –æ—Ç -1 (–ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ) –¥–æ 1 (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ).
–§–æ—Ä–º—É–ª–∞:
$$\text{Cosine Similarity} = \frac{\langle A , B\rangle}{\|A\|_2 \|B\|_2} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2} \cdot \sqrt{\sum_{i=1}^n B_i^2}}$$
–≥–¥–µ $A$ –∏ $B$ ‚Äî –¥–≤–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–≤–µ–∫—Ç–æ—Ä–∞), $A_i$ –∏ $B_i$ ‚Äî –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, $\langle \cdot,\cdot\rangle$ ‚Äî —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ, $\|A\|_2$ ‚Äî 2-–Ω–æ—Ä–º–∞ (–¥–ª–∏–Ω–∞) –≤–µ–∫—Ç–æ—Ä–∞. 

2. –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (Dot Product):

–ú–µ—Ç–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤—É—Ö embedding –≤–µ–∫—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—á–∏—Å–ª—è–µ—Ç —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. –ó–Ω–∞—á–µ–Ω–∏–µ –≤–∞—Ä—å–∏—Ä—É–µ—Ç—Å—è –æ—Ç -1 (–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ) –¥–æ 1 (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ).

–ï—Å–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã (2-–Ω–æ—Ä–º–∞), —Ç–æ —Ñ–æ—Ä–º—É–ª–∞ —É–ø—Ä–æ—â–∞–µ—Ç—Å—è –¥–æ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤:
$$\text{Dot Product} = \langle A, B \rangle = \sum_{i=1}^n A_i B_i$$

3. –ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (Euclidean Distance):

–ò–∑–º–µ—Ä—è–µ—Ç –ø—Ä—è–º–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–æ—á–∫–∞–º–∏ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ (–¥–ª–∏–Ω—É –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–∏). –ß–µ–º –º–µ–Ω—å—à–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, —Ç–µ–º –±–ª–∏–∂–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.

$$\text{Euclidean Distance} = \sqrt{\sum_{i=1}^n (A_i - B_i)^2}$$

4. –ú–∞–Ω—Ö–µ—Ç—Ç–µ–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (Manhattan Distance)

–°—É–º–º–∏—Ä—É–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Ä–∞–∑–Ω–∏—Ü—ã (SAD) –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤. –¢–∞–∫–∂–µ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è L1-—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º.
–§–æ—Ä–º—É–ª–∞:
$$\text{Manhattan Distance} = \sum_{i=1}^n |A_i - B_i|$$

–ü–æ—á–µ–º—É –ú–∞–Ω—Ö–µ—Ç—Ç–µ–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ? –°–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ –∑–∞–ø–ª–∞—Ç–∏—Ç—å –∑–∞ —Ç–∞–∫—Å–∏, —á—Ç–æ–±—ã –ø–µ—Ä–µ–π—Ç–∏ –æ—Ç —Ç–æ—á–∫–∏ A –¥–æ —Ç–æ—á–∫–∏ B —Å —É—á–µ—Ç–æ–º —Ç–æ–≥–æ —á—Ç–æ –≤—Å–µ —É–ª–∏—Ü—ã –ø–µ—Ä–µ—Å–µ–∫–∞—é—Ç—Å—è –ø–æ–¥ –ø—Ä—è–º—ã–º —É–≥–ª–æ–º. –¢–∞–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª–∏—Ü —Å–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ –¥–ª—è –ú–∞–Ω—Ö–µ—Ç—Ç–µ–Ω–∞ –∏ –ë–∞—Ä—Å–µ–ª–æ–Ω—ã. 

*MSE* is the simplest and most commonly used image similarity metric. Given two images of the same size, $ùëã$ and $ùëå$, where the pixel values are denoted as ùë•ùëñ and $ùë¶_ùëñ$ respectively
and the total number of pixels is ùëÅ, the MSE can be expressed as:
```math
MSE(ùëã, ùëå) ={1 \over ùëÅ}\sum_{i=1}^N (ùë•_ùëñ ‚àí ùë¶_ùëñ)^2
```

*PSNR* is to convert *MSE* into the commonly used decibel form in signal processing. From a digital perspective, it has more distinguishing power compared to *MSE*. 
```math
PSNR(ùëã, ùëå) = 10 \lg \frac{L^2}{MSE(ùëã, ùëå)}
```
–≥–¥–µ $L$ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π), $L=255$ –¥–ª—è 8-–±–∏—Ç–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. 

5. –ò–Ω–¥–µ–∫—Å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ Structure-level Metrics

–°—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ - —ç—Ç–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—Å–æ–∫, –ø—Ä–∏—á–µ–º –º–∞—Å–∫–∏ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –∏–∑ –ø–∏–∫—Å–µ–ª—å–Ω—ã–π —Å–≤–æ–π—Å—Ç–≤ —Ç–æ–≥–æ –∂–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è. –ü—Ä–∏ –Ω–∞–ª–æ–∂–µ–Ω–∏–∏ –º–∞—Å–æ–∫ –¥–µ–π—Å—Ç–≤—É–µ—Ç –ø—Ä–∞–≤–∏–ª–æ —É–º–Ω–æ–∂–µ–Ω–∏—è –∫–∞–∫ –æ–ø–µ—Ä–∞—Ü–∏—è –ò –≤ –Ω–µ—á–µ—Ç–∫–æ–π –ª–æ–≥–∏–∫–µ. 

* [[wang03b](https://www.cns.nyu.edu/pub/eero/wang03b.pdf)]: MULTI-SCALE STRUCTURAL SIMILARITY FOR IMAGE QUALITY ASSESSMENT. Zhou Wang, Eero P. Simoncelli and Alan C. Bovik (2003)

–ò–Ω–¥–µ–∫—Å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ (SSIM –æ—Ç –∞–Ω–≥–ª. structure similarity) —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–∏–º –∏–∑ –º–µ—Ç–æ–¥–æ–≤ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –¥–≤—É–º—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ [[2406.03865](https://arxiv.org/pdf/2406.03865)].
*SSIM* –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–∞ –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –æ–∫–Ω–∞. –†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –¥–≤—É–º—è –æ–∫–Ω–∞–º–∏ 
$x$ –∏ $y$ –∏–º–µ—é—â–∏–º–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä $N√óN$:

```math
{\text{SSIM}}(x,y)={\frac {(2\mu _{x}\mu _{y}+c_{1})(2\sigma _{xy}+c_{2})}{(\mu _{x}^{2}+\mu _{y}^{2}+c_{1})(\sigma _{x}^{2}+\sigma _{y}^{2}+c_{2})}},
```
–≥–¥–µ $\mu_{x}$ ‚Äî —Å—Ä–µ–¥–Ω–µ–µ $x$, $\mu_{y}$ ‚Äî —Å—Ä–µ–¥–Ω–µ–µ $y$, 
$\sigma_{x}^{2}$ ‚Äî –¥–∏—Å–ø–µ—Ä—Å–∏—è $x$,
$\sigma_{y}^{2}$ ‚Äî –¥–∏—Å–ø–µ—Ä—Å–∏—è $y$,
$\sigma_{xy}$ ‚Äî –∫–æ–≤–∞—Ä–∏–∞—Ü–∏—è $x$ –∏ $y$, \
$c_{1}=(k_{1}L)^{2}$, 
$c_{2}=(k_{2}L)^{2}$,
$c_{3}=—Å_{2}/2$, ‚Äî –¥–≤–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã:
$L$ ‚Äî –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω –ø–∏–∫—Å–µ–ª–µ–π ($2^{\text{(bits)}}-1$), \
$k_{1}=0{,}01$ –∏ $k_{2}=0{,}03$ ‚Äî —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—â–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã, –ø–æ–¥–æ–±—Ä–∞–Ω—ã —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å –ø—Ä–∏ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–µ.

–í –±–æ–ª–µ–µ –æ–±—â–µ–º –≤–∏–¥–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –∫—Ä–∏—Ç–µ—Ä–∏–π —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
```math
\text{SSIM}(ùë•, ùë¶) = ùëô(ùë•, ùë¶)^ùõº ùëê(ùë•, ùë¶)^ùõΩ ùë†(ùë•, ùë¶)^\gamma
```
–≤–∫–ª—é—á–∞–µ—Ç $l(x,y)$ - luminance similarity, 
$c(x,y)$ - contrast  similarity, 
$s(x,y)$ - structure similarity
```math
\begin{aligned}
l(ùë•, ùë¶) &= \frac{2ùúá_ùë• ùúá_ùë¶ + c_1}{ùúá_x^2 + ùúá_y^2 + c_1} \\
c(ùë•, ùë¶) &= \frac{2ùúé_ùë• ùúé_ùë¶ + c_2}{ùúé_x^2 + ùúé_y^2 + c_2} \\
s(ùë•, ùë¶) &= \frac{  ùúé_{ùë•ùë¶} + c_3}{ùúé_ùë• ùúé_ùë¶ + c_3}
\end{aligned}
```
–ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã $c_1, c_2, c_3$, –¥–æ–±–∞–≤–ª–µ–Ω—ã —á—Ç–æ–±—ã —Å–Ω–∏–∑–∏—Ç—å –æ—à–∏–±–∫—É —Å–≤—è–∑–∞–Ω–Ω—É—é —Å –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–µ–π –∏ –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å.

```math
\mu_x = {1 \over n} \sum^n_{i=1}x_i~, \quad \sigma_x^2 = { {1 \over n} \sum^n_{i=1}(x_i ‚àí ¬µ)^2}
```
–ö–æ–≤–∞—Ä–∏–∞—Ü–∏—è $\sigma_{xy}$ –≤–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫
```math
\sigma_{xy} = { {1 \over n} \sum^n_{i=1}(x_i ‚àí ¬µ_x)(y_i - \mu_y)}
```
–ü–æ —Å—É—Ç–∏ —Ç–µ—Ä–º $s(x,y)$ —è–≤–ª—è–µ—Ç—Å—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏–ª–∏ "–∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç—å—é" –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, —Å—á–∏—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –ø—Ä–æ–µ–∫—Ü–∏—è –≤ –µ–≤–∫–ª–∏–¥–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ:
```math
s(x,y) = \frac{\sigma_{xy}}{\sigma_x \sigma_y} = \frac{x^\mathsf{T} y}{\|x\|_2 \cdot \|y\|_2}
```

–ù–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–±–∏—Ç–æ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤. –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ (MSSIM) index to evaluate the overall image quality:
```math
MSSIM(X, Y) = {1 \over M} \sum^M_{j=1} SSIM(x_j , y_j),
```

–î–∞–ª–µ–µ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–π—Ç–∏ –∫ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã—Ä–∞–∂–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ –∫–∞—Å–∫–∞–¥ —Ñ–∏–ª—å—Ç—Ä–æ–≤ (MS-SSIM, multiscale -Structural Similarity). –ö–∞—Å–∫–∞–¥ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –º–æ–∂–Ω–æ –≤—ã—Ä–∞–∑–∏—Ç—å —á–µ—Ä–µ–∑ –æ–ø–µ—Ä–∞—Ü–∏—é —Å–≤–µ—Ä—Ç–∫–∏. –°–ª–µ–¥—É–µ—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É CNN (—Å–≤–µ—Ä—Ç–æ—á–Ω–æ–π) —Å–µ—Ç–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Feature Pyramid. 

–§–∏–Ω–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –º–Ω–æ–≥–æ–∫–∞—Å–∫–∞–¥–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è MS-SSIM –º–æ–∂–Ω–æ —Ç—Ä–∞–∫—Ç–æ–≤–∞—Ç—å –∫–∞–∫ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ (s) —Å —É—á–µ—Ç–æ–º –º–∞—Å–∫–∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏(—Å). –ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É (–º–æ–∂–µ—Ç) –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–∞—Å–∫–∞ luminance-similarity. 
```math
\text{MS-SSIM}(ùë•, ùë¶) = ùëô(ùë•, ùë¶)^ùõº \prod^N_{k=1}{ùëê(ùë•_k, ùë¶_k)^{\beta_k} ùë†(ùë•_k, ùë¶_k)^{\gamma_k}}
```
–Ø –Ω–µ —Å—Ç–æ—Ä–æ–Ω–Ω–∏–∫ —Å—Ç–µ–ø–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –û–¥–Ω–∞–∫–æ, –µ—Å–ª–∏ –ø–µ—Ä–≤–∏—á–Ω–æ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–æ–≥–∞—Ä–∏—Ñ–º–∞ –æ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏, —Ç–∞–∫–æ–π –∫—Ä–∏—Ç–µ—Ä–∏–π –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ —Å—É–º–º—É —Å –≤–µ—Å–æ–≤—ã–º–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º–∏ –∏ –æ–ø—Ä–∞–≤–¥—ã–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π. 

* [[1910.07467](https://arxiv.org/pdf/1910.07467)] Root Mean Square Layer Normalization

–í –æ—Å–Ω–æ–≤–µ –ª–µ–∂–∏—Ç –¥–∏—Å–∫—É—Å—Å–∏—è –æ –º–µ—Ç–æ–¥–∞—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –≤–ª–∏—è–Ω–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç.

**–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è** (LayerNorm) –ø—Ä–∏–∑–≤–∞–Ω–∞ —É–ª—É—á—à–∏—Ç—å —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ü—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ –º–æ–¥–µ–ª—å –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –∏–¥–µ—é
```math
\bar{a}_i = \frac{a_i - \mu}{\sigma}g_i~,\quad y_i = f(\bar{a}_i + b_i) 
```
–≥–¥–µ $\bar{a}_i$ - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, $g_i$ - gain, $b_i$ - bias, $f(\cdot)$ - –Ω–µ–∫–æ—Ç–æ—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –í —Å—Ç–∞—Ç—å–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è —á—Ç–æ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —Å—Ç–æ–ª—å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å, –∫–∞–∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∑–∞–º–µ–Ω—è—Ç—å –º–µ—Ç–æ–¥ LayerNorm –Ω–∞ RMSNorm.  
```math
\bar{a}_i = \frac{a_i}{RMS(a)}g_i~,\quad {RMS}(a) = \sqrt{ {1 \over n} \sum^n_{i=1} a_i^2}
```
* [[2104.14294](https://arxiv.org/pdf/2104.14294)] Emerging Properties in Self-Supervised Vision Transformers
* [[2111.07832](https://arxiv.org/pdf/2111.07832)] IBOT: IMAGE BERT PRE-TRAINING WITH ONLINE TOKENIZER

–ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º iBot –∏ DINO –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ–Ω—è—Ç–∏–µ Cross-Entropy Loss –∏–ª–∏ –≤–∞—Ä–∏–∞–Ω—Ç knowledge distillation loss
```
Alg. Input:
gs, gt ; // student and teacher network
 C, C0 ; // center on [CLS] token and patch tokens
œÑs, œÑt ; // temperature on [CLS] token for student and teacher network
œÑs0,œÑt0; // temperature on patch tokens for student and teacher network
   l   ; // momentum rate for network
 m, m0 ; // momentum rates for center on [CLS] token and patch tokens

``` 
$u_{[CLS]}$, $u_{patch}$ = g(u, return all tok=true) ; // [n, K], [n, S^2, K]
–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º computation graph.

```
The function H(s, t, c, œÑs, œÑt) takes five inputs:
    s: Student logits (predictions from a student model).
    t: Teacher logits (predictions from a teacher model).
    c: A centering parameter (likely a scalar or vector).
    œÑs: Temperature parameter for the student softmax (scalar).
    œÑt: Temperature parameter for the teacher softmax (scalar).
```

```py
## Algorithm 1 DINO PyTorch pseudocode w/o multi-crop.
# gs, gt: student and teacher networks
# C: center (K)
# tps, tpt: student and teacher temperatures
# l, m: network and center momentum rates
gt.params = gs.params

for x in loader: # load a minibatch x with n samples
    x1, x2 = augment(x), augment(x) # random views
    s1, s2 = gs(x1), gs(x2) # student output n-by-K
    t1, t2 = gt(x1), gt(x2) # teacher output n-by-K
    loss = H(t1, s2)/2 + H(t2, s1)/2
    loss.backward() # back-propagate
    # student, teacher and center updates
    update(gs) # SGD
    gt.params = l*gt.params + (1-l)*gs.params
    C = m*C + (1-m)*cat([t1, t2]).mean(dim=0)

def H(t, s):
    t = t.detach() # stop gradient
    s = softmax(s / tps, dim=1)
    t = softmax((t - C) / tpt, dim=1) # center + sharpen
    return - (t * log(s)).sum(dim=1).mean()
```
The output of the teacher network is centered with a mean computed over the batch.
Each networks outputs a $K$ dimensional feature that is normalized
with a temperature *softmax* over the feature dimension. Their
similarity is then measured with a _cross-entropy loss_. We apply a
stop-gradient operator on the teacher to propagate gradients
only through the student. The teacher parameters are updated with
an _exponential moving average_ (EMA) of the student parameters.

**Knowledge Distillation**

[[1503.02531](https://arxiv.org/pdf/1503.02531)]:
Neural networks typically produce class probabilities by using a *softmax* output layer that converts the logit, $z_i$, computed for each class into a probability, $q_i$, by comparing $z_i$ with the other logits.
```math
q_i = \frac{\exp(z_i/\tau)}{\sum_{j} \exp(z_j/\tau)}
```
where $\tau$ is a temperature that is normally set to $1$.

–ü–∞—Ä–∞–¥–∏–≥–º–∞ –æ–±—É—á–µ–Ω–∏—è —Å–µ—Ç–∏ —Å–≤–æ–¥–∏—Ç—Å—è –∫ –ø–æ–∏—Å–∫—É –º–∏–Ω–∏–º—É–º–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏, $H(a,b) = -a\cdot \log(b)$
```math
\min_{Œ∏_s} H(P_t(x), P_s(x)),
```
–î–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å–µ—Ç–∏ $g$ —Å –Ω–∞–±–æ—Ä–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ $\theta_s$–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è `softmax`.
```math
P_s(x)_{i} = \frac{\exp(z_i/\tau_s - z_{max})}{\sum^K_{k=1} \exp(z_{k}/\tau_s - z_{max})}~,\quad \mathbf{z} = g[Œ∏s](\mathbf{x})
```
—á—Ç–æ–±—ã –∑–Ω–∞—á–µ–Ω–∏—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ç—ã –Ω–µ –∑–∞—à–∫–∞–ª–∏–≤–∞–ª–∏, –Ω–µ –≤—ã–∑—ã–≤–∞–ª–∏ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ, –≤ —Ñ—É–Ω–∫—Ü–∏—é –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è —Å–º–µ—â–µ–Ω–∏–µ `z_max`. –°–º–µ—â–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞–Ω–Ω–æ–π –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π –∏–ª–∏ –≤—ã—á–∏—Å–ª—è—Ç—å—Å—è –∏–Ω–∞—á–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä –∫–∞–∫ —Å—Ä–µ–¥–Ω–µ–µ. –í –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –ø–ª–∞–Ω–µ —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–º–µ—â–µ–Ω–∏—è.

* [[2304.07288](https://arxiv.org/pdf/2304.07288)] Cross-Entropy Loss Functions:
Theoretical Analysis and Applications

–û–±–æ–±—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –≤–≤–æ–¥–∏—Ç—Å—è —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è $\Phi_2(.)$ - –Ω–µ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–∞—è, $\Phi_1(.)$ –Ω–µ —É–±—ã–≤–∞—é—â–∞—è. 
```math
\ell_{}(x,y) = \Phi_1 \left( \sum^N_{y\neq y'} \Phi_2(h(x,y)-h(x,y')) \right)
```
```math
\Phi_1^{œÑ}(u) =
\begin{cases}
\frac{1}{1‚àíœÑ}
((1 + u)^{1‚àíœÑ} ‚àí 1)~, \quad œÑ ‚â• 0, œÑ ‚â† 1 \\
\log(1 + u)~,\quad œÑ = 1.
\end{cases}
```
–ü—Ä–∏–Ω–∏–º–∞–µ–º $\Phi_2(u) = \exp(‚àíu)$. –§—É–Ω–∫—Ü–∏—è $\Phi^{œÑ}_1(u)$ —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–µ—Ç —Å–ª–µ–¥—É—é—â–µ–º—É —É—Ä–∞–≤–Ω–µ–Ω–∏—é
```math
{‚àÇŒ¶^{œÑ} \over ‚àÇu} (u) =\frac{1}{(1 + u)^œÑ}~, \quad Œ¶^{œÑ}(0) = 0~.
```

For a single sample $ i $ (row in the batch):

Student softmax:
$$s_{i,j} = \frac{\exp(s_{i,j} / \tau_s)}{\sum_{k=1}^C \exp(s_{i,k} / \tau_s)}, \quad j = 1, \dots, C$$
$ s_i $ is a probability distribution over $ C $ classes.

Teacher softmax:
$$t_{i,j} = \frac{\exp((t_{i,j} - c_j) / \tau_t)}{\sum_{k=1}^C \exp((t_{i,k} - c_k) / \tau_t)}, \quad j = 1, \dots, C$$
$ t_i $ is a probability distribution over $ C $ classes.
Loss:
$$H_i = -\sum_{j=1}^C t_{i,j} \cdot \log(s_{i,j})$$
This is the cross-entropy loss for sample $ i $. For the batch, the output is a vector $ [H_1, H_2, \dots, H_N] $. 


* [[2104.08821](https://arxiv.org/pdf/2104.08821)] SimCSE: Simple Contrastive Learning of Sentence Embeddings

–°–ª–µ–¥—É–µ—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –º–µ—Ç–æ–¥ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –æ—à–∏–±–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, —ç—Ç–∏ –∂–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤. –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–Ω—Ü–∏–ø - contrastive learning.

Contrastive learning –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –±–ª–∏–∑–∫–∏—Ö –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º 

**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.**\
It assumes a set of paired examples $D = \{(x_i, x_i^{+})\}^m_{i=1}$, where $x_i$ and $x^{+}_i$ are semantically related. We follow the contrastive framework in Chen et al. (2020) and take a cross-entropy objective with in-batch negatives (Chen et al., 2017; Henderson et al., 2017): 
let $h_i$ and $h^{+}_i$ denote the representations of $x_i$ and $x^{+}_i$, the training objective
for $(x_i, x^{+}_i)$ with a mini-batch of $N$ pairs is:
```math
\mathcal{L}_i = ‚àí \log \frac{e^{\mathrm{sim}(h_i,h^{+}_i)/\tau}}{
\sum^{N}_{j=1} e^{\mathrm{sim}(h_i, h^{+}_j)/\tau}}~, 
```
where $\tau$ is a temperature hyperparameter and $\mathrm{sim}(h_1, h_2)$ is the _cosine similarity_ 
```math
\mathrm{sim}(h_1, h_2) = \frac{\langle h_1, h_2 \rangle}{\|h_1\|¬∑\|h_2\|}~.
```
–ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —ç—Ç–æ–≥–æ –ø—Ä–∏–Ω—Ü–∏–ø–∞, –≤—Å–µ —Ä–∞–≤–Ω–æ —Å–ª–µ–¥—É–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏ –∑–∞ —Å—á–µ—Ç —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è, –≤—ã—á–∏—Ç–∞–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ –≤ —Å–ª—É—á–∞–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ—è `LayerNorm` (—Å–º. [Centered kernel functions](#)). –î–∞–ª—å–Ω–µ–π—à–∏–π –ø—É—Ç—å –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏ - –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Å–µ–≥–º–µ–Ω—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–∏–∂–µ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è embedding –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Å–µ—Ç–µ–π, –ø—Ä–∏–Ω—Ü–∏–ø –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ DINO.

–ü—Ä–∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—à–∏–±–∫–∏ –ø–æ–¥–æ–±–Ω–æ cross-entropy loss –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –≤–∏–¥–∞
```math
\mathcal{L}_{tot} = \alpha_1\mathcal{L}_1 + \alpha_2\mathcal{L}_2 + ... + \alpha_{N} \mathcal{L}_N~,
```
–≥–¥–µ $\alpha_i$ - –≤–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã, —É–¥–æ–≤–ª—è–µ—Ç–≤–æ—Ä—è—é—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é $\sum_{i=1}^N \alpha_i = 1$.

–î–ª—è –ø–æ–¥–±–æ—Ä–∞ –≤–µ—Å–æ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ `mix(L_1, mix(L_2, ..., b2),b1)`:
```math
\mathcal{L}_{tot} = (1-\beta_1) \cdot \mathcal{L}_1 + \beta_1 \cdot ((1-\beta_2) \cdot \mathcal{L}_2 + \beta_2 \cdot (...))
```
–≥–¥–µ $\beta_i$ - –≤–µ—Å–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é $\beta_i \in [0,1]$.

## Visual Embeddings

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ SigLIP:**

–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏, –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ SigLIP —á–∞—â–µ –≤—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (cosine similarity). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ –±–ª–∏–∑–∫–∏ —Ç–µ–∫—Å—Ç –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–∏–ª–∏ —Ç–µ–∫—Å—Ç –∏ —Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ) –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.

> *–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ* ‚Äî —ç—Ç–æ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ, –≤ –∫–æ—Ç–æ—Ä–æ–º –æ–±—ä–µ–∫—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–≤–∞, —Ñ—Ä–∞–∑—ã, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –¥—Ä—É–≥–∏–µ –¥–∞–Ω–Ω—ã–µ) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ –≤–∏–¥–µ —á–∏—Å–ª–æ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –∏–ª–∏ —É–≥–æ–ª –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –æ—Ç—Ä–∞–∂–∞–µ—Ç –∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å –∏–ª–∏ —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Å–º—ã—Å–ª—É.

Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of Large Language Models (LLMs) by integrating them with external knowledge sources. Instead of relying solely on their pre-trained data, RAG allows LLMs to retrieve relevant information from specified documents, databases, or the web before generating a response.

–¢–µ—Ö–Ω–∏–∫–∞ RAG –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è embedding –≤–µ–∫—Ç–æ—Ä–æ–≤ –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—ã–±—Ä–∞—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫ –∑–∞–ø—Ä–æ—Å—É –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å –∏ –≤–µ–∫—Ç–æ—Ä–∞ –æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞. –¢–∞–∫ –Ω–∞–ø—Ä–∏–º–µ—Ä –º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤—ã–±–æ—Ä–∫—É –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∑–∞–ø—Ä–æ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. 

–ú–æ–¥–µ–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ —Å—Ç—Ä–æ—è—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è-–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è. –≠—Ç–∞–ø –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –ø—Ä–æ–º–ø—Ç –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. 

BERT (Bidirectional encoder representations from transformers)

In summary, BERT is an encoder-only transformer model consisting of 4 main parts:

* Tokenizer: chops up texts into sequences of integers.
* Embedding: the module that converts discrete tokens into vectors.
* Encoder: a stack of transformer blocks with self-attention.
* Task head: when encoder is finished with the representations, this task-specific head handles them for token generation or classification tasks.

https://datahacker.rs/005-how-to-create-a-panorama-image-using-opencv-with-python/#Detecting-distinctive-keypoints-

* CLIP (Contrastive Language-Image Pretraining): Trained on image-text pairs, CLIP maps both modalities into a shared embedding space, enabling cross-modal tasks.
* DINO and SimCLR: Self-supervised contrastive learning models that generate robust visual embeddings without labeled data.

* [[DeViSE](https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/41473.pdf)] DeViSE: A Deep Visual-Semantic Embedding Model
* [[2303.15343](https://arxiv.org/pdf/2303.15343)] Sigmoid Loss for Language Image Pre-Training
* [[2304.07193](https://arxiv.org/pdf/2304.07193)] DINOv2: Learning Robust Visual Features without Supervision
* [[2405.20204](https://arxiv.org/pdf/2405.20204)] JINA CLIP: Your CLIP Model Is Also Your Text Retriever
-- —Ä–∞–∑–≤–∏—Ç–∏–µ –∏–¥–µ–π DeViSE
* [[2406.03865](https://arxiv.org/pdf/2406.03865)] Semantic Similarity Score for Measuring Visual Similarity at Semantic Level
* [[2406.18587](https://arxiv.org/pdf/2406.18587)] Nomic Embed Vision: Expanding the Latent Space
* [[2502.14786](https://arxiv.org/pdf/2502.14786)] SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Feaures
* [[2503.08723](https://arxiv.org/pdf/2503.08723)] Is CLIP ideal? No. Can we fix it? Yes!
-- –≤ —Ä–∞–±–æ—Ç–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –º–µ—Ç–æ–¥ Dense Cosine Similarity Maps (DCSMs)

–ù–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —ç—Ç–∞–ø–µ —Ä–∞–∑–≤–∏—Ç–∏—è –º–µ—Ç–æ–¥–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —è–≤–ª—è–µ—Ç—Å—è *ClipScore* –∏–ª–∏ –º–µ—Ç–æ–¥—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –Ω–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–µ. 

* [[2406.03865](https://arxiv.org/pdf/2406.03865)]: Semantic Similarity Score for Measuring Visual Similarity at Semantic Level, 2024


*ClipScore* is based on the multimodal model `CLIP` [17], which aligns images and natural language descriptions, enabling it to ‚Äùunderstand‚Äù images at a higher level. As a result, it is the most robust to changes weakly correlated with image
semantics. Therefore, it has been widely used in research fields related to visual semantic communication [[2406.03865](https://arxiv.org/pdf/2406.03865)].
```math
d_{CLIP} (ùëã, ùëå) = 1 ‚àí \frac{\langle ùëí(ùëã) , ùëí(ùëå) \rangle}{\|ùëí(ùëã)\|_2 \|ùëí(ùëå)\|_2}
```
Where $ùëí(¬∑)$ is the image encoder (output) of the CLIP model, and ClipScore is the *cosine similarity* between the image encodings.

* [[2103.10697](https://arxiv.org/pdf/2103.10697)] ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases
* [[2105.01601](https://arxiv.org/pdf/2105.01601)] MLP-Mixer: An all-MLP Architecture for Vision
* [[2108.08810](https://arxiv.org/pdf/2108.08810)] Do Vision Transformers See Like Convolutional Neural Networks?

    —Å–º. Representation Similarity and CKA (Centered Kernel Alignment) 
* [[1203.0550](https://arxiv.org/pdf/1203.0550)] Algorithms for Learning Kernels Based on Centered Alignment\
-- —ç—Ç–∞ —Ä–∞–±–æ—Ç–∞ —Ö–æ—Ä–æ—à–∞ –≤ –ø–ª–∞–Ω–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–Ω–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –í—Å–µ –º–µ—Ç–æ–¥—ã —Å–≤–æ–¥—è—Ç—Å—è –∫ —Å–∫–∞–ª—è—Ä–Ω–æ–º—É –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—é (–∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏). –ú–µ—Ç–æ–¥—ã —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è—é—Ç—Å—è –Ω–∞ –º–∞—Ç—Ä–∏—á–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ - kernels. –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤—ã—à–∞–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –§—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–∞–∫ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è, —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω—ã.

**2.1 Centered kernel functions**

Let $D$ be the distribution according to which training and test points are drawn. A feature mapping $\Phi: X ‚Üí H$ is centered by subtracting from it its expectation, that is forming it by $Œ¶‚àí\mathsf{E}_x[Œ¶]$, where
$\mathsf{E}_x$ denotes the expected value of $\Phi$ when $x$ is drawn according to the distribution $D$. Centering a positive definite symmetric (PDS) kernel function $K: X √ó X ‚Üí \mathbb{R}$ consists of centering any feature mapping $\Phi$ associated to $K$. Thus, the centered kernel $K_c$ associated to $K$ is defined for all $x, x‚Ä≤ ‚àà X$ by 
$$K_c(x, x‚Ä≤) = (Œ¶(x) ‚àí \mathsf{E}_x[Œ¶])^‚ä§(Œ¶(x‚Ä≤) ‚àí \mathsf{E}_{x‚Ä≤}[Œ¶])$$

–Ø –ø—Ä–æ–≤–µ–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å —Ñ—É–Ω–∫—Ü–∏–µ–π cosine similarity -- —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–ª–æ—Ö–æ. –ò –≤ —Å–ª–µ–¥—É—é—â–µ–º –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é `softmax` –≤ –∫–∞—á–µ—Å—Ç–≤–µ –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏ $\Phi$ –∏ –≤–º–µ—Å—Ç–µ —Å —Ç–µ–º —Å—á–∏—Ç–∞—Ç—å –æ–∂–∏–¥–∞–µ–º–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —Å–µ—Ä–∏–∏ –∫–∞–¥—Ä–æ–≤ –∏–ª–∏ –Ω–∞ –ø–æ—Ç–æ–∫–µ, –∫–∞–∫ —ç—Ç–æ –¥–∞–µ—Ç—Å—è –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å. –ò —Ñ—É–Ω–∫—Ü–∏—é –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å—á–∏—Ç–∞—Ç—å –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –Ω–∏–∂–µ.

–í —Å—Ç–∞—Ç—å–µ –¥–∞–µ—Ç—Å—è –¥–≤–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —è–¥–µ—Ä (kernels) –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ –≤ –º–∞—Ç—Ä–∏—á–Ω–æ–π —Ñ–æ—Ä–º–µ
```math
A = \frac{\mathsf{E}[KK']}{\sqrt{\mathsf{E}[K^2]\cdot \mathsf{E}[K'^2]}}, \quad
A_b = \frac{{\langle K, K' \rangle}_{F}}{\|K\|_F \|K'\|_F}
```
–í –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–æ—Ä–º–∞ –º–∞—Ç—Ä–∏—Ü—ã –§—Ä–æ–±–µ–Ω–∏—É—Å–∞ –∏ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –§—Ä–æ–±–µ–Ω–∏—É—Å–∞ (—Å–ª–µ–¥ –º–∞—Ç—Ä–∏—Ü—ã) —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–π —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è. –ú–∞—Ç—Ä–∏—á–Ω–∞—è –Ω–æ—Ä–º–∞ –§—Ä–æ–±–µ–Ω–∏—É—Å–∞ –∏–Ω–¥—É—Ü–∏—Ä–æ–≤–∞–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –§—Ä–æ–±–µ–Ω–∏—É—Å–∞.\
Let $‚ü®¬∑, ¬∑‚ü©_F$ denote the Frobenius inner product and $\| ¬∑ \|_F$ the Frobenius norm defined by
```math
\forall A, B \in \mathbb{R}^{n√ón}, \quad ‚ü®A, B‚ü©_F = \text{Tr}[A^‚ä§ B], \quad \|A\|_F = \sqrt{‚ü®A, A‚ü©_F}~.
```
* [[2311.15419](https://arxiv.org/pdf/2311.15419)] –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –±—É–¥–µ—Ç –≤—ã—Ä–∞–∂–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü:
```math
\forall A, B \in \mathbb{C}^{n√ón}, \quad ‚ü®A, B‚ü©_{\mathsf{F}} = \text{Tr}[A^{\mathsf{H}} B], \quad \|A\|_{\mathsf{F}} = \sqrt{‚ü®A, A‚ü©_{\mathsf{F}}}~.
```
–ù–æ—Ä–º–∞ –∏ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –§—Ä–æ–±–µ–Ω–∏—É—Å–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–∞–∫–∂–µ –∫–∞–∫ –µ—Å–ª–∏ –±—ã –º–∞—Ç—Ä–∏—Ü–∞ –±—ã–ª–∞ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ –≤–∏–¥–µ –µ–¥–∏–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞-—Å—Ç–æ–ª–±—Ü–∞ –∏–∑ —Å—Ç–æ–ª–±—Ü–æ–≤ –∏—Å—Ö–æ–¥–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω—É–∂–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –º–∞—Ç—Ä–∏—Ü—ã, –ø–æ—Å–ª–µ —á–µ–≥–æ —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –§—Ä–æ–±–µ–Ω–∏—É—Å–∞ —Å–≤–æ–¥–∏—Ç—Å—è –∫ –æ–ø–µ—Ä–∞—Ü–∏–∏ _dot product_. 

```math
\langle \mathbf {A} ,\mathbf {B} \rangle _{\mathsf{F} }={\overline {\mathrm {vec} (\mathbf {A} )}^{\mathsf{T}}}\mathrm {vec} (\mathbf {B} )\,.
```

–í—ã—Ä–∞–∂–µ–Ω–∏—è –æ—Ç–Ω–æ—Å—è—Ç—Å—è –∫ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ (–Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ) –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –º–∞—Ç—Ä–∏—Ü–∞–º.
An $n\times n$ symmetric real matrix $M$ is said to be positive-semidefinite (PSD) or non-negative-definite if $\mathbf {x} ^{\mathsf {T}}M\mathbf {x} \geq 0$ for all 
$\forall \mathbf {x} \in \mathbb{R} ^{n}$. Formally,
```math
M{\text{ positive semi-definite}}\quad \iff \quad \mathbf {x} ^{\mathsf {T}}M\mathbf {x} \geq 0{\text{ for all }}\mathbf {x} \in \mathbb {R} ^{n}
```

* [[1905.00414](https://arxiv.org/pdf/1905.00414)] Similarity of Neural Network Representations Revisited

* [[1912.11370](https://arxiv.org/pdf/1912.11370)] Big Transfer (BiT): General Visual Representation Learning
* [[2010.11929](https://arxiv.org/pdf/2010.11929)] AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE

–ú–µ–π–Ω—Å—Ç—Ä–∏–º —Ä–∞–±–æ—Ç –ø–æ –≤–∏–∑—É–∞–ª—å–Ω—ã–º —Å–µ—Ç—è–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –ø–µ—Ä–µ—Ö–æ–¥ –æ—Ç —Å–≤–µ—Ç–æ—á–Ω—ã—Ö —Å–µ—Ç–µ–π —Ç–∞–∫–∏—Ö –∫–∞–∫ ResNet –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ Visual Transformers. –ü—Ä–∏ —ç—Ç–æ–º –≤—Å–µ –∫—Ä—É–ø–Ω—ã–µ –∏–≥—Ä–æ–∫–∏ —Ç–∞–∫–∏–µ –∫–∞–∫ Google –∏ Meta –ø—Ä–µ–∂–¥–µ –≤—Å–µ–≥–æ —Ä–∞–∑–≤–∏–≤–∞–ª–∏ –º–æ–¥–µ–ª–∏ ResNet –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –∏—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π ViT. –°–µ—Ç–∏ ViT –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ª—É—á—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–µ–º CNN. 

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –≥–∏–±—Ä–∏–¥–Ω–æ–π, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–∞–ª–æ–≤ _feature map_ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CNN —Å–µ—Ç–µ–π.

–ú—ã —Å–∫–ª–æ–Ω–Ω—ã –ø—Ä–∏ —ç—Ç–æ–º —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∑–∞–º–µ–Ω–µ–Ω—É —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã CNN –Ω–∞ —Å–ø–ª–∞–π–Ω–æ–≤—ã–µ —Å–µ—Ç–∏ KSN, –∞ —ç–ª–µ–º–µ–Ω—Ç —Å–µ—Ç–∏ MLP –Ω–∞ KAN.

**Visual Transformer** (ViT) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π —Ç—É –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Self-Attention Transformers c –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π GELU –≤ –±–ª–æ–∫–µ MLP. 

–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π|—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã D. –í –¥–∞–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤–∞–∂–Ω–æ, —á—Ç–æ –∏–∑ —Å–µ–±—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≤–µ–∫—Ç–æ—Ä –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ —Å–ª—É–∂–∞—Ç embedding –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏. –ù–∞ –≤—Ö–æ–¥–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–µ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã ("–ø–∞—Ç—á–∏"), –∫–æ—Ç–æ—Ä—ã–µ —Ñ–æ—Ä–º–∏—Ä—É—é—Ç—Å—è –ø—É—Ç–µ–º —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ä–∞–∑–º–µ—Ä–æ–º $P\times P$. –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–º–µ–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ $H\times W \times C$ (–≤—ã—Å–æ—Ç–∞, —à–∏—Ä–∏–Ω–∞, –∫–∞–Ω–∞–ª—ã —Ü–≤–µ—Ç–Ω–æ—Å—Ç–∏), —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –∏ —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã $N \times (P^2 \cdot C)$ (—Å–º. –æ–ø–µ—Ä–∞—Ü–∏—é `im2col`).

Transformer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ D –≤–æ –≤—Å–µ—Ö —Å–≤–æ–∏—Ö —Å–ª–æ—è—Ö, –ø–æ—ç—Ç–æ–º—É –º—ã
—Å–≥–ª–∞–¥–∏—Ç—å —É—á–∞—Å—Ç–∫–∏ –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –∏—Ö —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏ D —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–∞–µ–º–æ–π –ª–∏–Ω–µ–π–Ω–æ–π –ø—Ä–æ–µ–∫—Ü–∏–∏ (—É—Ä–∞–≤–Ω–µ–Ω–∏–µ 1). –ú—ã —Å—Å—ã–ª–∞–µ–º—Å—è –Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —ç—Ç–æ–π –ø—Ä–æ–µ–∫—Ü–∏–∏ –≤ –≤–∏–¥–µ —Å–µ—Ä–∏–∏ –ø–∞—Ç—á–µ–π (embedding patch).
```math
z_0 = [x_{class}; x^1_p E; x^2_p E; ¬∑ ¬∑ ¬∑ ; x^N_p E] + E_{pos}~, \quad E ‚àà \mathbb{R}^{(P^2\cdot C)\times D},~~ E_{pos} ‚àà \mathbb{R}^{(N+1)√óD}
```
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–ª–æ–µ–≤ ViT –≤–∫–ª—é—á–∞–µ—Ç: MSA - Multi-head Self-Attention, MLP - multilayer perceptron —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–≤–∞ —É—Ä–æ–≤–Ω—è —Å —Ñ—É–Ω–∫—Ü–∏–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ GELU:
```math
\begin{aligned}
z_\ell'&= \text{MSA}(LN(z_{\ell‚àí1})) + z_{\ell‚àí1}~,\quad \ell = 1 . . . L \\
z_\ell &= \text{MLP}(LN(z_\ell')) + z_\ell'~,\quad \ell = 1 . . . L  \\
y      &= \text{LN} (z_L) 
\end{aligned}
```
*Hybrid Architecture*. As an alternative to raw image patches, the input sequence can be formed
from feature maps of a CNN. In this hybrid model, the patch embedding
projection $E$ (Eq. 1) is applied to patches extracted from a CNN feature map.

*Model Variants*. –í–∞—Ä–∏–∞–Ω—Ç—ã –º–æ–¥–µ–ª–∏ —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –∏–∑ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π: —Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞ $(16\times 16)$, —á–∏—Å–ª–æ —Å–ª–æ–µ–≤ –º–æ–¥–µ–ª–∏ (L), —Ä–∞–∑–º–µ—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –≤—Ö–æ–¥–µ $D = HW/P^2$ , —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å MLP —Å–ª–æ—è $d_{ff}$, —á–∏—Å–ª–æ –∫–∞–Ω–∞–ª–æ–≤ —Ü–≤–µ—Ç–Ω–æ—Å—Ç–∏ (–°).
–ß–∏—Å–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ —Å—á–∏—Ç–∞–µ—Ç—Å—è, –∫–∞–∫ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ $L \cdot D \cdot d_{ff} \cdot C$, 


–ú–µ—Ç–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è *ViTscope* –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –¥–≤—É—Ö –Ω–∞–±–æ—Ä–æ–≤ *feature vectors*.
```math
R = {1 \over N} \sum\limits_{i=1}^N \left( \max\limits_{j} x_i^T y_j\right)~,\quad
P = {1 \over M} \sum\limits_{j=1}^M \left( \max\limits_{i} x_i^T y_j\right)
```
–î–∞–ª–µ–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω—ã—Ö –≤–µ–ª–∏—á–∏–Ω $(1/R + 1/P)/2  = 1/V$:
```math
ViTscope = 2 \frac{R\cdot P}{R+P}
```
-- –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç–æ–º—É –∫–∞–∫ —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –ø—Ä–æ–≤–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≤–æ–¥–Ω–∏–∫–æ–≤. 
{–ú–Ω–µ —Ö–æ—á–µ—Ç—Å—è —Å—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –º–µ—Ç–æ–¥–æ–º "contrastive".} 


* [[1908.10396](https://arxiv.org/pdf/1908.10396)] Accelerating Large-Scale Inference with Anisotropic Vector Quantization

## Semantic Segmentation

    *SAM* - Segment Anything 
    *SGG* - Scene Graph Generation
    *SeSS* - Semantic Similarity Score
    *CLIP* - Contrastive Language-Image Pre-training 

–ü–æ–ª–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ SeSS. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å SAM –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏. SAM —Ä–∞–∑–±–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –Ω–∞–±–æ—Ä –º–∞—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤. –≠—Ç–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑–∞—Ç–µ–º –≤–≤–æ–¥–∏—Ç—Å—è –≤ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∞ —Å—Ü–µ–Ω—ã SGG, –∫–æ—Ç–æ—Ä–∞—è –≤—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –≥—Ä–∞—Ñ –æ–±—ä–µ–∫—Ç–æ–≤-–æ—Ç–Ω–æ—à–µ–Ω–∏–π. –ì—Ä–∞—Ñ—ã –æ–±—ä–µ–∫—Ç–æ–≤-–æ—Ç–Ω–æ—à–µ–Ω–∏–π –¥–≤—É—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –º–æ–¥–µ–ª—å CLIP, –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≥—Ä–∞—Ñ–æ–≤ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–Ω–µ—á–Ω–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å—É –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∫–∞–∫ —ç—Ç–æ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—é—Ç –ª—é–¥–∏.

–ú—ã –ø–ª–∞–Ω–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á—Ç–æ-—Ç–æ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–µ

* [[1807.10221](https://arxiv.org/pdf/1807.10221)] Unified Perceptual Parsing for Scene Understanding
* [[2111.09883](https://arxiv.org/pdf/2111.09883)] Swin Transformer V2: Scaling Up Capacity and Resolution
* [[2106.13797](https://arxiv.org/pdf/2106.13797)] PVT v2: Improved Baselines with Pyramid Vision Transformer
* [[2302.06378](https://arxiv.org/pdf/2302.06378)] Semantic Image Segmentation: Two Decades of Research
* [[2304.02643](https://arxiv.org/pdf/2304.02643)] Segment Anything
* [[2304.03284](https://arxiv.org/pdf/2304.03284)] SegGPT: Segmenting Everything In Context
* [[2305.08196](https://arxiv.org/pdf/2305.08196)] A comprehensive survey on segment anything model for vision and beyond
* [[2408.00714](https://arxiv.org/pdf/2408.00714)] Sam 2: Segment anything in images and videos, 2024.
* [[2506.18807](https://arxiv.org/pdf/2506.18807)] PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision Applications
* (https://docs.ultralytics.com/models/sam/) –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

<!-- * (https://habr.com/ru/articles/801885/) KNN –º–µ—Ç–æ–¥ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π {–∑–∞–º–µ–Ω–∏—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é}
-->
* (https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)

* (https://segmentation-models.readthedocs.io/en/latest/tutorial.html)


* [[2003.12039](https://arxiv.org/pdf/2003.12039)] RAFT: Recurrent All-Pairs Field Transforms for
Optical Flow 


## Simple Recurrent Unit

{–ú—ã –æ–±—Ä–∞—â–∞–µ–º—Å—è –∫ –º–æ–¥–µ–ª—è–º —Ä–µ–∫–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö —Å–µ—Ç–µ–π —Ç–∏–ø–∞ GRU –∏ SRU –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤. –†–∞—Å–∫—Ä—ã—Ç—å —Å–≤—è–∑—å —Å KAN}

* [[1709.02755](https://arxiv.org/pdf/1709.02755)] Simple Recurrent Units for Highly Parallelizable Recurrence

> We present and explain the design of Simple Recurrent Unit (SRU) in this section. A single layer
of SRU involves the following computation:
```math
\begin{aligned}
f_t &= œÉ (W_f x_t + v_f \odot c_{t‚àí1} + b_f ) \\
c_t &= f_t \odot c_{t‚àí1} + (1 ‚àí f_t) \odot (W x_t) \\
r_t &= œÉ (W_r x_t + v_r \odot c_{t‚àí1} + b_r) \\
h_t &= r_t \odot c_t + (1 ‚àí r_t) \odot x_t
\end{aligned}
```
–¢—É—Ç –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∫—É—Ä—Å–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–ø–∏—Å–∞—Ç—å State Space Model –° –¥–≤—É–º—è –≥–µ–π—Ç–∞–º–∏ (—É–ø—Ä–∞–≤–ª—è—é—â–∏–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏): foget –∏ reset. —Å–∏–≥–Ω–∞–ª foget –ø–æ —Å—É—Ç–∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å—Å—è –º–µ–∂–¥—É –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ —Å–æ –≤—Ö–æ–¥–∞ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏, —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ Enable –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞. –°–∏–≥–Ω–∞–ª reset –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–±—Ä–æ—Å–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∫ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∏–ª–∏ –æ–±–æ–π—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞. 

–ü—Ä–∏ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–π —ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ø–∞—Ä–∞–¥–∏–≥–º–∞, —á—Ç–æ –ª—é–±–∞—è —Ü–∏—Ñ—Ä–æ–≤–∞—è —Å—Ö–µ–º–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã—Ä–∞–∂–µ–Ω–∞, –∫–∞–∫ –∫–∞—Å–∫–∞–¥ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ —Å –∫–æ–º–±–∏–Ω–∞—Ç–æ—Ä–Ω–æ–π –ª–æ–≥–∏–∫–æ–π (FFN) –∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é (—Ä–µ–∫—É—Ä—Å–∏–µ–π). –¢—Ä–∏–≥–≥–µ—Ä - —ç—Ç–æ —ç–ª–µ–º–µ–Ω—Ç —Å–Ω–∞–±–∂–µ–Ω–Ω—ã–π –≤—Ö–æ–¥–∞–º–∏ set –∏ reset. –ê–Ω–∞–ª–æ–≥–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–º—É –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤—É, –∫–æ–≥–¥–∞ —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–≥–Ω–∞–ª—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —á–∏—Å–ª–∞–º–∏ [0,1], –∞ –∫–æ–º–±–∏–Ω–∞—Ç–æ—Ä–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—É—é –ª–æ–≥–∏–∫—É.

–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ $r_t$ –∏ $f_t$ –≤ —Ñ–æ—Ä–º–µ 
```math
r_t = \sigma(W_r x_t + b_r)
```
–±–µ—Å—Å–ø–æ—Ä–Ω–æ. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ—Ä–º - –ª–æ–≥–∏–∫–∞ —Å–±—Ä–æ—Å–∞ –º–æ–∂–µ—Ç –∑–∞–≤–∏—Å–µ—Ç—å –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã —Ç—Ä–µ–±—É–µ—Ç –≤–≤–µ—Å—Ç–∏ –≤—Ç–æ—Ä–æ–π —Å–∏–≥–Ω–∞–ª —Å–±—Ä–æ—Å–∞, –∫–æ—Ç–æ—Ä—ã–π –∫–∞–∫–æ–π-—Ç–æ —Ñ—É–Ω–∫—Ü–∏–µ–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–≤—è–∑–∞–Ω —Å –ø–µ—Ä–≤—ã–º. 
```math
r_t = \sigma(W_r [x_t,c_{t-1}] + b_r)
```

* [[1611.01576](https://arxiv.org/pdf/1611.01576)] QUASI-RECURRENT NEURAL NETWORKS
* [[1705.07393](https://arxiv.org/pdf/1705.07393)] Recurrent Additive Networks

## –õ–∏–Ω–µ–π–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏

–í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ .. –º–Ω–µ –∫–∞–∂–µ—Ç—Å—è —É–º–µ—Å—Ç–Ω—ã–º –ø—Ä–æ–≤–µ—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π —Ç–∏–ø–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤ –≤–∏–¥–µ RNN (—Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π) –∏ –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥ –∫ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º —É—Ä–∞–≤–Ω–µ–Ω–∏—è–º. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è, –Ω–æ —ç—Ç–æ –Ω–µ –∑–Ω–∞—á–∏—Ç, —á—Ç–æ —ç—Ç–æ –≤–µ—Ä–Ω—ã–π –ø—É—Ç—å —Ä–∞–∑–≤–∏—Ç–∏—è. –ù—É–∂–Ω–∞ –ø—Ä—è–º–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π. –î–æ–ª–∂–Ω–æ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–µ –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ—Å–Ω–æ–≤–∞–º. –í –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ (—Ç–µ–æ—Ä–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, —Ç–µ–æ—Ä–∏–∏ —Ñ—É–Ω–∫—Ü–∏–∏) –æ—Å–Ω–æ–≤–æ–π —è–≤–ª—è–µ—Ç—Å—è —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø–æ –±–∞–∑–∏—Å—É —Ñ—É–Ω–∫—Ü–∏–π. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø–æ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–º —Ñ—É–Ω–∫—Ü–∏—è–º. –í —Ç–µ–æ—Ä–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —ç—Ç–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø–æ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º —Å–æ–±—ã—Ç–∏—è–º. –í —á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø–æ –±–∞–∑–∏—Å–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–∞–º –≤ –µ–≤–∫–ª–∏–¥–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ. –í –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –º–æ–∂–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å –ø—Ä–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –ø–æ –±–∞–∑–∏—Å–Ω—ã–º –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–∞–º.
–†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –ø–æ –±–∞–∑–∏—Å—É –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤. –°–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ n-–º–µ—Ä–Ω–æ–º –µ–≤–∫–ª–∏–¥–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ - –æ–ø–µ—Ä–∞—Ü–∏—è dot product.

–ë–ª–∏–∂–∞–π—à–∏–º –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ "–≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ—Å–Ω–æ–≤–∞–º" —è–≤–ª—è–µ—Ç—Å—è –ª–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏ —Å–µ—Ç–∏ —Ç–∏–ø–∞ RWKV, Mamba (SSM, State-Space Models).
* (https://en.wikipedia.org/wiki/State-space_representation) 

* [[2111.00396](https://arxiv.org/pdf/2111.00396)] Efficiently Modeling Long Sequences with Structured State Spaces
* [[2305.13048](https://arxiv.org/pdf/2305.13048)] RWKV: Reinventing RNNs for the Transformer Era
* [[2312.00752](https://arxiv.org/pdf/2312.00752)] Mamba: Linear-Time Sequence Modeling with Selective State Spaces
* [[2401.04081](https://arxiv.org/pdf/2401.04081)] MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts
* [[2403.19887](https://arxiv.org/pdf/2403.19887)] Jamba: A Hybrid Transformer-Mamba Language Model
* [[2405.21060](https://arxiv.org/pdf/2405.21060)] Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
* [[2505.18975](https://arxiv.org/pdf/2505.18975)] FastMamba: A High-Speed and Efficient Mamba
Accelerator on FPGA with Accurate Quantization

* (https://habr.com/ru/articles/786278/) Mamba. –û—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞ (–æ–±–∑–æ—Ä)

* [[RWKV-6](https://openreview.net/pdf?id=soz1SEiPeq)] Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence
* [[2404.05892](https://arxiv.org/pdf/2404.05892)] Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence
* [[2404.19756](https://arxiv.org/pdf/2404.19756)] KAN: Kolmogorov‚ÄìArnold Networks
* [[2503.14456](https://arxiv.org/pdf/2503.14456)] RWKV-7 "Goose" with Expressive Dynamic State Evolution

–í —Å–µ—Ç—è—Ö RWKV –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –±–ª–æ–∫–∞ Attention –∫ –≤–∏–¥—É —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –ü–æ —Å—É—Ç–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–µ–∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–µ–º—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –¥–≤–∞ –±–ª–æ–∫–∞: $FFN_{ReLU2}$ –∏ RWKV- –æ–ø–µ—Ä–∞—Ç–æ—Ä —Å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π —Å–≤—è–∑—å—é. –†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Å–≤—è–∑—å –≤ —á–µ–º-—Ç–æ —Å—Ö–æ–∂–∞ —Å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏ —Å —ç–ª–µ–º–µ–Ω—Ç–æ–º GRU (Gated Recurrent Unit), SRU (Simple Recurrent Unit).

–Ø –ø—Ä–µ–¥–ª–∞–≥–∞—é —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –±–ª–æ–∫ FFN –∫–∞–∫ –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω —Å –ø–æ–Ω–∏–∂–µ–Ω–∏–µ–º —Ä–∞–∑—Ä—è–¥–Ω–æ—Å—Ç–∏ –≤–ø–ª–æ—Ç—å –¥–æ —Ç–µ—Ä–Ω–∞—Ä–Ω–æ–π –ª–æ–≥–∏–∫–∏ –∏ —Å –ø–æ–¥–º–µ–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –õ—é–±–∞—è –ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–∞–∫ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π AND, OR, NOT. –†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –¥–∞–µ—Ç—Å—è –≤ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ –î–ù–§ –∏–ª–∏ –ê–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–æ–π –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ. –¢–∞–∫–∞—è —Ñ–æ—Ä–º–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–ª–æ–∂–µ–Ω–∞ –Ω–∞ –º–∞—Å–∫–∏ - –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –≤ —Ç–µ—Ä–Ω–∞—Ä–Ω–æ–π –ª–æ–≥–∏–∫–µ. —Å–º. [–¢–µ—Ä–Ω–∞—Ä–Ω–∞—è –ª–æ–≥–∏–∫–∞](#). –õ—é–±–∞—è –ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –≤ —Ç–µ—Ä–Ω–∞—Ä–Ω–æ–π –ª–æ–≥–∏–∫–µ. –ö—Ä–æ–º–µ —Ç–æ–≥–æ, –∫–ª–∞—Å—Å FFN –¥–æ–ª–∂–µ–Ω –≤—ã—Ä–∞–∂–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –Ω–µ—á–µ—Ç–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –ª–æ–≥–∏–∫–µ. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –ª–æ–≥–∏–∫–µ - —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 0 –¥–æ 1.
–î—Ä—É–≥–æ–µ –∑–Ω–∞—á–∏–º–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ MLP - —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–π –ª–æ–≥–∏–∫–∏ –≤ –±–∞–∑–∏—Å–µ –ø–æ–ª–∏–Ω–æ–º–æ–≤ –ë–µ—Ä–Ω—à—Ç–µ–π–Ω–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–∏–¥–µ B-—Å–ø–ª–∞–π–Ω–æ–≤. –°–≤–æ–π—Å—Ç–≤–æ –±–∞–∑–∏—Å–Ω—ã—Ö –ø–æ–ª–∏–Ω–æ–º–æ–≤ - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –∏ —Ç–µ–æ—Ä–∏–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π [KAN]. –ë–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–π—Å—Ç–≤–∞–º –ø–æ–ª–∏–Ω–æ–º–æ–≤ –ë–µ—Ä–Ω—à—Ç–µ–π–Ω–∞, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ –≤–≤–æ–¥–∏—Ç—å –Ω–∞—Å—ã—â–µ–Ω–∏–µ). –í –æ–±—â–µ–º —Å–ª—É—á–∞–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ª—é–±–æ–π –Ω–∞–±–æ—Ä –±–∞–∑–∏—Å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –µ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è –ø–æ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–º –ø–æ–ª–∏–Ω–æ–º–∞–º –ß–µ–±—ã—à–µ–≤–∞ –∏ –Ø–∫–æ–±–∏.

–†–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—é –ø–æ–¥–ª–µ–∂–∏—Ç —Å—Ö–µ–º–∞ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–π —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –±–ª–æ–∫–∏ Kolmogorov‚ÄìArnold Networks (KAN) –∏ –±–ª–æ–∫–∞ –æ–±—ã–∫–Ω–æ–≤–µ–Ω–Ω—ã—Ö –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ –º–∞—Ç—Ä–∏—á–Ω–æ–π —Ñ–æ—Ä–º–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–π –∫ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (neural ODEs). –í –∫—É–∫–æ–π —Ç–æ –º–µ—Ä–µ —ç—Ç–∏ –ø—Ä–∏–Ω—Ü–∏–ø—ã –≤–æ—Å—Ö–æ–¥—è—Ç –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ—Å–Ω–æ–≤–∞–º –∑–∞–ª–æ–∂–µ–Ω–Ω—ã–º –≤ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ –ö–æ–ª–º–æ–≥–æ—Ä–æ–≤–∞ –∏ –ê—Ä–Ω–æ–ª—å–¥–∞. –Ø –Ω–µ —è–≤–ª—è—é—Å—å —Å—Ç–æ—Ä–æ–Ω–Ω–∏–∫–æ–º, —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é –∫–∞–∫ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –ê—Ä–Ω–æ–ª—å–¥–∞, –∫–æ—Ç–æ—Ä–æ–µ –≤ –æ–±—â–µ–º –≤–∏–¥–µ —Å–≤–æ–¥–∏—Ç—Å—è –∫ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ—Å–Ω–æ–≤–∞–º –∫–≤–∞–Ω—Ç–æ–≤–æ–π —Ñ–∏–∑–∏–∫–∏ –∏ —Ç–µ–æ—Ä–∏–∏ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤. –í —Ç–æ–∂–µ –≤—Ä–µ–º—è –≤–∞–∂–Ω–æ –≤ –æ–±—â–µ–º –≤–∏–¥–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –º–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏, –∫–∞–∫ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –∏ –∞–Ω—Ç–∏—Å–∏–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, –≥–∞–º–∏–ª—å—Ç–æ–Ω–æ–≤—ã –º–∞—Ç—Ä–∏—Ü—ã. –î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º —Å–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –¥–∏–Ω–∞–º–∏–∫–∏ –¥–≤–∏–∂–µ–Ω–∏—è –≤ –≥–∞–º–∏–ª—å—Ç–æ–Ω–æ–≤–æ–π –¥–∏–Ω–∞–º–∏–∫–µ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—Ä–∞–≤–Ω–µ–Ω–∏–π –≤ –º–∞—Ç—Ä–∏—á–Ω–æ–π —Ñ–æ—Ä–º–µ. 

* [[1806.07366](https://arxiv.org/pdf/1806.07366)] Neural Ordinary Differential Equations
* [[2506.16392](https://arxiv.org/pdf/2506.16392)] State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification
-- –°–ª–µ–¥—É–µ—Ç –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å. 
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ SS-KAN –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ –¥–≤–∏–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –≤–∏–¥–µ–æ, –≥–¥–µ –º–æ–¥–µ–ª—å –∞–ø–ø—Ä–æ–∫—Å–∏–º–∏—Ä—É–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏.

* (https://github.com/SynodicMonth/ChebyKAN/)

* [[1907.06732](https://arxiv.org/pdf/1907.06732)] PAD√â ACTIVATION UNITS: END-TO-END LEARNING
OF FLEXIBLE ACTIVATION FUNCTIONS IN DEEP NETWORKS\
 -- –í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ –≤–∏–¥–µ —Ä–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π. –û—Ç —Å–µ–±—è —Ö–æ—á–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è —Å—Ç–µ–ø–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –ø–æ –ø–æ–ª–∏–Ω–æ–º–∞–º –ë–µ—Ä–Ω—à—Ç–µ–π–Ω–∞ –∏ –ø–æ–ª–∏–Ω–æ–º–∞–º –¢–µ–π–ª–æ—Ä–∞.

```math
F(x) = \frac{P(x)}{Q(x)}
= \frac{ \sum_{j=0}^m a_jx^j }{1 + \sum_{k=1}^n b_kx^k }
```
* [[1704.07483](https://arxiv.org/pdf/1704.07483)] Continuously Differentiable Exponential Linear Units (ELU, CELU)
* [[2410.10084](https://arxiv.org/pdf/2410.10084)] POINTNET WITH KAN VERSUS
POINT NET WITH MLP FOR 3D CLASSIFICATION AND SEGMENTATION OF POINT SETS
* [[2505.22686](https://arxiv.org/pdf/2505.22686)] Localized Weather Prediction Using
Kolmogorov-Arnold Network-Based Models and Deep RNNs
* [[2506.06644](https://arxiv.org/pdf/2506.06644)] Spark Transformer: Reactivating Sparsity in FFN and Attention
* [[2506.14802](https://arxiv.org/pdf/2506.14802)] SS-MAMBA: SEMANTIC-SPLINE SELECTIVE STATE-SPACE MODEL
* [[2506.18339](https://arxiv.org/pdf/2506.18339)] Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and Symbolic Discovery of Nonlinear Dynamics

* (https://arxiv.org/pdf/1502.03167) - –æ–ø–∏—Å–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∏ –±–ª–æ–∫–æ–≤
* [[2006.16236](https://arxiv.org/pdf/2006.16236)] Transformers are RNNs
* [[2409.18747](https://arxiv.org/pdf/2409.18747)] COTTENTION: LINEAR TRANSFORMERS WITH COSINE ATTENTION

```math
ATTN_l(x) = V' = \text{softmax }\left(\frac{Q\cdot K^{T}}{\sqrt{D}}\right) V
```
Equation 2 implements a specific form of self-attention called *softmax* attention where the _similarity score_ is the exponential of the dot product between a Query and a Key.
Given that subscripting a matrix with i returns the i-th row as a vector, we can write a generalized attention equation for any similarity function as follows,
```math
V'_i = \frac{\sum_{j=1}^N {\text{sim }(Q_i, K_j) V_j}}{\sum_{j=1}^N {\text{sim }(Q_i, K_j)}}
```
–≠—Ç–∏ –¥–≤–∞ —É—Ä–∞–≤–Ω–µ–Ω–∏—è —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã –ø—Ä–∏ –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–µ 
$$\text{sim }(q, k) = \exp \left(\frac{q^T k}{\sqrt{D}}\right)$$


* [[2506.06941](https://arxiv.org/pdf/2506.06941)] The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity

**KSN/KAN:**\
–í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–∞—É—á–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ —Å–µ—Ç–∏ KAN –ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä—É—é—Ç—Å—è, –∫–∞–∫ –∑–∞–º–µ–Ω–∞ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã—Ö MLP —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–µ—Ö–∞–Ω–∏–∑–º–∞ Attention. MLP –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–µ–≤ –ª–∏–Ω–µ–π–Ω—ã—Ö –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –∏ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, —Ç–∞–∫–æ–π –∫–∞–∫ `ReLU` –∏–ª–∏ `GELU`. MLP –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞–Ω—ã, –∞ —Ç–∞–∫–∂–µ –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –í –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö –º–æ–¥–µ–ª–∏ KAN –º–æ–≥—É—Ç –±—ã—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞–Ω—ã —á–µ—Ä–µ–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –∏ –∫–æ–Ω–µ—á–Ω—ã–µ —Ä–∞–∑–Ω–æ—Å—Ç–∏, –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –ø–æ–≤–µ—Ä—Ö KAN –∏–º–µ–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø–æ–ª–∏–Ω–æ–º–æ–≤ –ë–µ—Ä–Ω—à—Ç–µ–π–Ω–∞ –∏–ª–∏ –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –ø—Ä–∏–≤–æ–¥–∏—Ç –º–æ–¥–µ–ª—å –≤ –ì–∏–ª—å–±–µ—Ä—Ç–æ–≤–æ –∏–ª–∏ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Clamp –∏ Softmax –∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. 

–°–ø–ª–∞–π–Ω–æ–≤—ã–µ —Å–µ—Ç–∏ (KSN) –∏ KAN –∏—Å–ø–æ–ª—å–∑—É—é—Ç B-—Å–ø–ª–∞–π–Ω—ã –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∏—Ö –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–º–∏ –∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –¥–ª—è –∑–∞–¥–∞—á –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π [Igelnik03][2404.19756]. –¶–∏—Ñ—Ä–æ–≤—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã B-—Å–ø–ª–∞–π–Ω–∞–º–∏ –∏ –∏–º–µ—é—Ç —Ñ–∏–∑–∏—á–µ—Å–∫—É—é –∏ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫—É—é –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é.

* [K57] A. N. Kolmogorov, ‚ÄúOn the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition, ‚Äù Dokl. Akad. Nauk SSSR, pp. 953‚Äì956, vol. 114, 1957.
* [[Igelnik03](https://ieeexplore.ieee.org/document/1215392)] B. Igelnik, at al. Kolmogorov‚Äôs Spline Network, IEEE Transactions on Neural Networks ( Volume: 14, Issue: 4, July 2003)
* [[2410.04096](https://arxiv.org/pdf/2410.04096)] Sinc Kolmogorov-Arnold Network and Its Applications on Physics-informed Neural Networks

-- –ù–µ –º–æ–≥—É —Å–æ–≥–ª–∞—Å–∏—Ç—å—Å—è, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω–æ–≥–æ —Ä–æ–¥–∞ –ø–æ–ª–∏–Ω–æ–º–æ–≤ –∫—Ä–æ–º–µ –æ—Ä—Ç–æ–≥–æ–Ω–ª—å–Ω—ã—Ö –ø–æ–ª–∏–Ω–æ–º–æ–≤ –∏ –±–∞–∑–∏—Å–Ω—ã—Ö –ø–æ–ª–∏–Ω–æ–º–æ–≤, –Ω–∞ –º–æ–π –≤–∑–≥–ª—è–¥, —É–≤–æ–¥–∏—Ç –≤ —Å—Ç–æ—Ä–æ–Ω—É –æ—Ç —Ñ–∏–∑–∏–∫–∏. –ú–æ–∂–Ω–æ —Å–æ–≥–ª–∞—Å–∏—Ç—å—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ä—Ç–æ–≥–æ–Ω–∞–ª—å–Ω—ã–µ –ø–æ–ª–∏–Ω–æ–º—ã –≠—Ä–º–∏—Ç–∞, –ß–µ–±—ã—à–µ–≤–∞, –Ø–∫–æ–±–∏ –≤ –∑–∞–¥–∞—á–∞—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∏ —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º—ã—Ö PINN (Physic-informed Neural Network), —Ç–∞–∫–∏—Ö –∫–∞–∫ –æ–ø–∏—Å–∞–Ω–∏–µ –≤–æ–ª–Ω –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏–∫–∏ —á–∞—Å—Ç–∏—Ü. –í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –Ω–∞–¥–æ –∫–∞–∫-—Ç–æ –æ–±–æ–∑–Ω–∞—á–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏–∑–∏–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞–∑–∏—Å–Ω—ã—Ö –ø–æ–ª–∏–Ω–æ–º–æ–≤ –∏ –∫–æ–Ω–µ—á–Ω—ã—Ö —Ä–∞–∑–Ω–æ—Å—Ç–µ–π –ø–æ–¥–æ–±–Ω–æ —Ñ–∏–ª—å—Ç—Ä–∞–º —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –∏–º–ø—É–ª—å—Å–Ω–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–æ–π. –í –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ KAN —è –Ω–µ –º–æ–≥—É —Å–æ–≥–ª–∞—Å–∏—Ç—å—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SILU –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –ü–æ —Å—É—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é –ö–êN. –í—Å—è —Å–µ—Ç—å –¥–æ–ª–∂–Ω–∞ —Å—Ç—Ä–æ–∏—Ç—Å—è –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö KAN. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ö —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π-–∏–º–ø—É–ª—å—Å–Ω–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–æ–π –∏ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π –º–æ–¥–µ–ª–∏ –≤ Z-–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –≤–æ–∑–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É SSM (State-Space Models) –∏ —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ (RNN —Ç–∞–∫–∏–µ –∫–∞–∫ GRU –∏ STU).

