# Разложение матриц и решение СЛАУ #

**СЛАУ** - системы линейных алгебраических уравнений

* [Основные определения](#основные-определения)
* [LU разложение матриц](#разложение-матриц-и-решение-слау)
* [LL Разложение Холецкого](#разложение-холецкого)
* [Модифицированное разложение Холецкого](#модифицированное-разложение-холецкого)
* [QR - разложение матриц](#qr-разложение-матрицы)
* [LQ - разложение матриц](#lq-разложение-матрицы)
* [SVD Сингулярное разложение](#)
* [Разложение Шура](#разложение-шура)
* [Разложение Хессенберга](#разложение-хессенберга-для-вещественных-матриц)
* [Спектральное разложение](#)
* [Каноническая Жорданова форма](#)

* [Вращения Гивенса и отражения Хаусхолдера](#)

* [Ортогонализация. Процесс Грама-Шмидта](#процесс-грама-шмидта)
* [Диагонализация. Спектральное разложение](#спектральное-разложение-матрицы)
* [Разложения Хессенберга](#разложение-хессенберга-для-вещественных-матриц)
* [Бидиагонализация](#бидиагонализация)
* [Балансировка матриц](#балансировка)


* [Решение СЛАУ, поиск обратной матрицы](#)
* [Метод наименьших квадратов](#)
* [Задача на собственные числа](#)
* [Классы симметричных, анти-симметричных, эрмитовых матриц](#)
* [Класс гамильтоновых матриц](#)
* [Класс спектрально обратимых матриц](#)
* [Сжатие матриц](#)

* [Абстрактная линейная алгебра](#абстрактная-линейная-алгебра)
* [Базовые операции и обозначения](#базовые-операции-и-обозначения)
* [BLAS - Базовый набор методов для линейной алгебры](#базовый-набор-методов-для-линейной-алгебры)

*Вместо введения*
Для меня линейная алгебра с ее численными методами - инструмент. 
Я хотел бы иметь сборник освоенных алгоритмов, или готовую библиотеку методов масштабируемую по производительности и пригодную для расчетов физических задач и задач оптимизации больших матриц. 
Но глядя в исходники готовых библиотек, я все равно ищу что-то свое в этом. 
Сейчас мне важно сделать свой инструментарий эффективным. 
Я хочу научиться раскладывать большие матрицы, 
я не вижу такого инструмента, который меня полностью устраивает. 
Практически вся линейная алгебра может быть описана, как синтезируемые
методы по шаблону. Есть традиционные способы описания этих шаблонов 
принятые в библиотеках BLAS/LAPACK. Современное требование, 
которое диктуется развитием технологий ИИ - 
* возможность описания операторов в терминах BLAS и перенос вычислений на высокопроизводительные архитектуры
* Возможность замены базового класса для использования базовых типов пониженной размерности, т
.к. Float16 и BFloat16, а также использование упакованных форматов для хранения матриц.
* Использование блочных рекурсивных алгоритмов.
* Оценка и компенсация ошибки округления. Квантизация матриц. 

Вычислительные сети используют прежде всего ряд операций: умножение матрицы на вектор, матрицы на матрицу. 
Помимо этого используются операции нормировки слоя, операции скалярного произведения векторов, 
и все это на масштабах порядка десятка тысяч элементов.
При описании операторов используются функции от матриц и векторов.
Операторы должны быть синтезируемые под аппаратуру и заданный метод квантизации. 
Самое важное свойство, которое мы хотели бы добавить
к инструментам - возможность эквивалентного разложения матриц и вынос ортогональных матриц из выражений. 
Это требует методов разложения матриц: ортогонализация, диагонализация, балансировка, преобразования подобия; 
QR разложения, LDL - разложения Холе́цкого, SVD разложения, LS- метод наименьших квадратов, решение СЛАУ. В курсе 
следует уделить внимание методам классификации матриц и оптимизации методов для 
решения физических задач теоретической механики, гамильтоновой динамики, квантовой механики.

Системы линейных алгебраических уравнений возникают при численном решении систем дифференциальных уравнений в конечных разностях. В этом случае используются методы численного решения для сильно разряженных и ленточных матриц. 

В приложении Аффинных (обратимых) преобразований необходимо находить обратную матрицу и находить разложения для перехода в базис ортогональных собственных векторов. В этой связи вводятся понятия ортогональных матриц, ортонормированный базис, собственных векторов и собственных значений. 

Матрицы или линейные формы образуются на множестве булевом $\mathbb{B}: \{0,1\}$, вещественных $\mathbb{R}$ или множестве комплексных чисел $\mathbb{C}$. 
Как правило, методы решения СЛАУ реализованные на множестве комплексных чисел легко переносятся на множество вещественных чисел, но наоборот - теряются операции сопряжения. 
Матрицы применяются в сочетании с понятием вектор, а вектора возникают при разложении по базису. Переход между базисами (системами координат или наборами ортогональных функций) - это все матрицы. 

Линейная алгебра - это алгебра абстрактная, которая оперирует множеством матриц и операциями: сложения, умножения и умножения матрицы на скаляр. Операция умножения матриц не являются коммутативной в общем случае. По отношению к операции умножения существуют обратные матрицы, произведение матрицы на обратную дает инвариант - единичную диагональную матрицу. Абстрактная линейная алгебра может быть построена на любом базовом классе удовлетворяющем ряду аксиом. 

Линейная алгебра может быть приложением аналитической геометрии и теории линейных операторов. Теория линейных операторов начинается с теории функций [1] и продолжается в [2]. В [3] хорошо описан переход от линейных операторов к линейной алгебре. В [4] методы линейной алгебры доведены до алгоритмов, что позволяет разобраться с современными библиотеками BLAS/LAPACK и научиться оптимизировать алгоритмы под практические задачи, такие как Orthogonal Low Rank Optimization для нейронных сетей. 

* [2] Садовничий В.А. Теория операторов. 2023
* [3] В.А. Ильин, Г.Д. Ким. Линейная алгебра и аналитическая геометрия. 3-е изд. 2024
* [4] Golub & van Loan. Matrix Computations. 4rd ed. 2013
* [5] Гантмахер Ф.Р. Теория матриц. 5-е изд. 2010 Пер. с англ. ISBN 978-5-9221-0524-8
* [6] Хорн Р., Джонсон Чарльз. Матричный анализ. Пер. с англ.
* [7] Уоткинс Д.С. Основы матричных вычислений. Пер. - 2006
      Watkins D.S. Fundamentals of Matrix Computations. 3rd ed, 2010.
* [] Strang, G. (1993), Introduction to Linear Algebra

Линейную алгебру надо изучать на примерах (в приложениях) и реализовывать численные методы. 
Без практики и прикладных задач - это кажется бессмысленным. 


Решение СЛАУ требует большого числа операций. Решение может быть получено за один проход по строкам матрицы в особых случаях - матрицы треугольные. Если на диагонали матрицы единицы, то исключается операция деления. По этой причине разложение матриц на произведение диагональных и треугольных матриц - это способ снизить сложность расчетов. Для сильно разреженных матриц применяется перестановка строк, чтобы матрицы стали диагональные. Перестановка строк может привести матрицу к блочно-диагональному виду - это тоже удобный метод снижения сложности расчетов, который позволяет выполнять расчеты блоками меньшей размерности. 

## Основные определения

**Диагональная матрица** — квадратная матрица, все элементы которой, стоящие вне главной диагонали, равны нулю.

**Трехдиагональная матрица** - важный класс матриц,
для которой ненулевыми являются элементы на диагонали, над диагональю и под диагональю.

Следует обратить внимание на матрицы квази-диагональные, для которых на диагонали располагаются элементы 1х1 и 2x2 (соответствующие двум комплексно-сопряженным значениям). Путем вращений можно такую матрицу преобразовать к виду бидиагональной матрицы с ненулевыми элементами на диагонали и над диагональю. (См. [3] §93 Вещественный аналог Жордановой формы).

**Хессенберговская матрица**

Верхняя матрица Хессенберга — это квадратная матрица 
$H\in \mathbb{C}^{n\times n}$, у которой все элементы лежащие ниже первой поддиагонали равны нулю, то есть 
$h_{ij}=0\ \forall i>j+1$.

Аналогично определяется нижняя матрица Хессенберга, как квадратная матрица, при транспонировании которой получается верхняя матрица Хессенберга, для которой элементы выше наддиагонали равны нулю $h_{ij}=0\ \forall j>i+1$. 

Матрицы Хессенберга могут использоваться в процессе нахождения собственных значений.

**Треуго́льная ма́трица** — квадратная матрица, у которой все элементы, стоящие ниже (или выше) главной диагонали, равны нулю.

**Унитреугольная матрица** - треугольная матрица (верхняя или нижняя), в которой все элементы на главной диагонали равны единице.

**Унитарная матрица** - квадратная матрица, в общем случае состоящая из
комплексных чисел, произведение которой на эрмитово-сопряженную матрицу
равно единичной матрице, т.е.:
$$U^{\mathsf{H}} U = U U^{\mathsf{H}} = I$$

**Ортогональная матрица** - унитарная матрица, но только с вещественными
числами, для которой справедливо:
$$A^{\mathsf{T}} A = A A^{\mathsf{T}} = I~,$$
и как следствие: $A^{\mathsf{T}} = A^{−1}$

**Эрмитова матрица** - это квадратная самосопряженная матрица, в общем случае состоящая из 
комплексных чисел, которая равна транспонированной комплексно-сопряженной матрице, т.е.:
$$A = (\bar{A})^{\mathsf{T}} = A^{\mathsf{H}},$$
$A^\ast$ или $A^{\mathsf{H}}$-- это и транспонирование и поэлементное комплексное сопряжение.

**Анти-эрмитова матрица** (skew-Hermitian) для которой выполняется тождество
$$A = -A^{\mathsf{H}},$$

**Симметричная матрица**
$$A = A^{\mathsf{T}},$$

**Анти-симметричная матрица** (skew-Symmetric) 
$$A = -A^{\mathsf{T}}.$$ 
Свойство антисимметричной матрицы - нули на главной диагонали.

*Зная, куда заводит использование комплексных чисел, можно сказать, что эрмитова матрица состоит из вещественной и мнимой части. Вещественная матрица подчиняется свойствам симметричной матрицы, а мнимая - анти-симметричной.*

**Нормальная матрица** - это квадратная матрица A, в общем случае состоящая из
комплексных чисел, для которой справедливо следующее соотношение:
$A^\ast A = A A^\ast$,
где $A^\ast$ - сопряженно-транспонированная матрица, а для матрицы с вещественными числами:
$A^{\mathsf{T}} A = A A^{\mathsf{T}}$
Основные свойства нормальной матрицы:
* нормальная матрица диагонализируема через унитарные матрицы, т.е. представима в виде: $A = U D V$, где D - диагональная матрица.
* нормальная матрица с действительными собственными значениями является эрмитовой матрицей.

**Подобная матрица** Вещественные матрицы $A,B \in \mathbb{R}^{n \times n}$ называются подобными, если существует невырожденная матрица $Q$ такая что 
$$A = Q^{-1} B Q$$

У подобных матриц совпадают многие характеристики, а именно:
* ранг;
* определитель;
* след;
* собственные значения (но собственные векторы могут не совпадать);
* характеристический многочлен;
* Жорданова форма с точностью до перестановки клеток.

Любая матрица $A$ подобна $A^{\mathsf{T}}$.

Характеристические многочлены подобных матриц совпадают. 

Характеристическим называется многочлен $f(\lambda) = |A - \lambda I|$. 
Следствие: у подобных матриц совпадают: следы и определители.

* см. [Преобразования подобия](#преобразования-подобия)

**Галмильтонова матрица**

В математике гамильтонова матрица — это матрица $H \in \mathbb{R}^{2n \times 2n}$ четной размерности $2n\times 2n$, 
такая что $JH$ - симметрична, где $J$ — кососимметричная матрица
```math
J=
\begin{bmatrix}
0_{n}&I_{n}\\
-I_{n}&0_{n}
\end{bmatrix}
```
и $I_n$ — $n \times n$ единичная матрица. 
$H$ является гамильтоновой тогда и только тогда $(JH)^{\mathsf{T}} = JH$, симметрична.

* Следует отметить возможность представления класса гамильтоновых матриц в классе комплексных матриц. 

* Следует обратить внимание на возможность подобным образом, через матрицы перестановок, определить 
кватернионы, гиперкомплексные числа, матрицы Паули и группы Ли. 

* Следует рассмотреть представление дифференциальных операторов в матричной форме.
Также вводится матричная операция - коммутатор дифференциальных операторов.

Класс гамильтоновых матриц возникает при решении или распознавании физических систем, которые описываются уравнениями гамильтоновй динамики.

Класс гамильтоновых матриц 

```math
H=
\begin{bmatrix}
-B^{\mathsf{T}} & C\\
A & B
\end{bmatrix}
```
где $A,B,C \in \mathbb{R}^{n\times n}$ - вещественные матрицы размерностью $n$, $A=A^{\mathsf{T}}$, $C=C^{\mathsf{T}}$ - симметричные матрицы.

Предположим, что матрица $A$ размером $2n \times 2n$ записана в виде блочной матрицы
```math
H={\begin{bmatrix}a&b\\c&d\end{bmatrix}}
```
где $a$, $b$, $c$ и $d$ — матрицы $n \times n$. Тогда условие, что $H$ является гамильтоновой матрицей, 
эквивалентно требованию, чтобы матрицы $b$ и $c$ были симметричными, а $a + d^T = 0$

*Теорема*. Матрица $H$ размерность $2n$ гамильтонова тогда и только тогда $JHJ^{\mathsf{T}} = -H^{\mathsf{T}}$

При анализе структуры матрицы следует рассматривать обобщение - более широкий класс
матриц четной размерности, которые получаются из гамильтоновых матриц 
путем наложения произвольных ортогональных преобразований. 

Это свойство гарантирует наличие симметричного спектрального разложения у матриц этого класса.

> Замечательное свойство гамильтоновых матриц состоит в том, что у каждой из них множество собственных значений симметрично
относительно нуля. Свойство сохраняется для некоторого более широкого класса матриц $G$ четной размерности. 
Таким свойством обладают все матрицы полученные из гамильтоновой матрицы G посредством преобразования $UHU^T = G$, где $U$ - матрица ортогонального преобразования.


*Теорема #2*. Множество вещественных $2n×2n$-матриц $I$, удовлетворяющих условиям
$I^T = −I$, $I^2 = −1$, состоит из матриц, ортогонально эквивалентных матрице J, то есть
$I = UJU^T$.

Смежный класс матриц  - симплектические матрицы $S^{\mathsf{T}} J S = J$.
Все возможные симплектические матрицы одинаковой размерности образуют симплекстическое многообразие.
Все симплектические матрицы изоморфны. 

*Мое отношение к этой науке - она лишняя, если бы мы перешли в комплексные числа, то не надо было бы создавать 
новый класс - симплекстическое пространство. Но так же можно сказать и про комплексное пространство, 
вместо комплексного пространства и ортогональных свойств комплексных эрмитовых матриц, 
мы переходим к рассмотрению симплектических свойств сопряженных пар в вещественных матриц удвоенной размерности.*

*Хотелось бы чтобы математика приняла одно из направлений а другое отбросили.*

Первичным является свойства сопряженных пар и способ построения комплексного пространства 
(математического класса), которое вводится через скалярное произведение с сопряженным свойством. 
Это свойство в совершенно идентичной аксиоматике вводится как свойство сиплектического пространства
-- косое скалярное произведение. 

Эрмитова форма вводится через набор аксиом комплексного скалярного произведения.
$f(x,y)={\overline {f(y,x)}}\ \ \forall x,y\in V$.

Таким образом, полный набор условий, определяющих эрмитову форму, состоит в следующем:
* $f(x_{1}+x_{2},y)=f(x_{1},y)+f(x_{2},y)\ \ \forall x_{i},y\in V$,
* $f(\alpha x,y)=\alpha f(x,y)\ \ \forall x,y\in V,\ \alpha \in \mathbb{C}$.
* $f(x,y_{1}+y_{2})=f(x,y_{1})+f(x,y_{2})\ \ \forall x,y_{i}\in V,$
* $f(x,\alpha y)={\overline {\alpha }}f(x,y)\quad\forall x,y\in V,~\alpha \in \mathbb {C} ,$
* $f(x,y)=\overline {f(y,x)}\quad \forall x,y\in V.$

Симплектическая форма вводится через набор аксиом косо-скалярного произведения. Которое ведет себя как мнимая часть комплексного скалярного произведения. 

Симплектическая форма вводится как 
$$f(x,y)=-f(y,x)\quad\forall x,y\in V$$
соответствует анти-симметричным вещественным матрицам (skew-symmetric). Эрмитовы матрицы можно было бы определить как две матрицы: симметричную вещественную и анти-симметричную мнимую. 

Исходно симплектическая геометрия возникла из гамильтонова формализма в классической механике, когда фазовое пространство для классической системы ~~неожиданно~~ оказалось симплектическим многообразием. Это случилось благодаря формулировкам Арнольда, но я не уверен что это хороший путь для изучения, потому что тоже самое можно формулировать в комплексном пространстве. 

Есть два пути в науке которые вызывают такое отторжение. Гамильтон для описания своих измышлений ввел комплексные и гиперкомплексные числа (кватернионы). Кватернионы оказались удобным инструментом вывода уравнений динамики и в частности позволили описать электродинамику Максвелла. Позже математики и физики выбрали другой путь для построения теории, и почему-то не используют кватернионы, как оказалось пространство кватернионов можно построить на группе вращений Ли и на матрицах Паули, а обобщать кватернионы дальше невозможно. А уравнения квантовой механики выражать в комплексном пространстве собственных функций научились сразу, без оговорок о симплектическом многообразии фазового пространства гамильтонианов, оно сразу стало гильбертовым пространством со своим комплексным скалярным произведением. Наверное, первичным было влияние семинаров Гильберта и новая математика, которой все увлекались в начале XX-века. Глядя на симплекстические многообразия для гамильтонианов, я вижу комплексно сопряженные пары и вижу свойства гильбертова пространства. Все чего не хватает в аксиоматике симплесктических пространств - нормировка обобщенных переменных фазового пространства гамильтоновой динамики, что является аксиомой класса гильбертова пространства.

Для решения гамильтоновых систем в действительных числах используются симплектические аналоги преобразования вращения Гивенса и отражения Хаусхолдера. Повторюсь, не вижу разницы в описании симплекстических преобразований подобия и преозбразований подобия в комплексном пространстве, определения очень схожи.

см.[4] Golub & van Loan, 7.8  Hamiltonian and Product Eigenvalue Problems

Для любой положительно определённой вещественной симплектической матрицы $S$ существует U в U(2n,R), такая, что
```math
S=U^{\mathsf{T}}DU\quad {\text{где}}\quad D=\mathrm{diag} (\lambda _{1},\ldots ,\lambda _{n},\lambda _{1}^{-1},\ldots ,\lambda _{n}^{-1})~,
```
где диагональные элементы D являются собственными значениями S. Полное разложение будет включать диагонализацию и сортировку столбцов матрицы $U$.

Для матриц существует QR разложение, с симметрично расположенными диагональными элементами. 

Любая вещественная симплектическая матрица может быть разложена в произведение трёх матриц:
```math
S=O{\begin{bmatrix}D&0\\0&D^{-1}\end{bmatrix}}O'~,
```
таких что $O$ и $O'$ симплектические и ортогональные, а $D$ — положительно определённая диагональная матрица. Это разложение тесно связано с cингулярным разложением матрицы и известно как разложение Эйлера или Блоха-Мессии.

**Переход к комплексным матрицам**. Можно вместо матрицы $M \in \mathbb{R}^{2n \times 2n}$ рассматривать комплексные матрицы $\mathbb{C}^{n \times n}$ со свойством эрмитова споряжения $M^{\ast} \Omega M = \Omega$.

Альтернативой матрице $J$, приведённой выше, является блочно-диагональная матрица
```math
\Omega ={\begin{bmatrix}{\begin{matrix}0&1\\
-1&0\end{matrix}} & & 0\\
&\ddots &\\0&&{\begin{matrix}0&1\\-1&0\end{matrix}}\end{bmatrix}},
```
которая отличается от $J$ перестановкой базисных векторов.

**Матричная экспонента**
$e^A$ - это матричная функция от квадратной матрицы, которая определяется степенным рядом следующим образом:
```math
e^A =\sum\limits_{k=0}^{\infty} \frac{1}{k!} A^k
```
Матричная экспонента обладает рядом свойств:
* $e^0 = I$
* $e^{A^{\mathsf{T}}} = (e^A)^{\mathsf{T}}$, $e^{A^{\ast}} = (e^A)^{\ast}$
* $e^{V B V^{−1}} = V e^B V^{-1}$, если $V$ - невырожденная

Заметим, любую аналитическую функцию, которую можно представить в виде разложения в степенной ряд можно сделать матричной. 

*{предположеие}* Разложение аналитических матричных функций
* $f(V B V^{−1}) = V f(B) V^{-1}$, где $V$ - невырожденная (ортогональная) матрица. Это свойство следует обобщить на произвольную аналитическую функцию на единчином интервале и доказать как теорему. Идея доказательства в том, чтобы 

Базовое утверждение: если $A=V D V^{-1}$ диагонализируема, то $A^k = V D^k V^{−1} = V D^k V^{-1}$.

Аналитичность скалярной функции на интервале определяется, как возможность разложения фукнции в степенной ряд. Сходимость бывает абсолютная или по норме. Аналогично можно опеределить разложение векторных и матричных функций со сходимостью по норме. Первичным является определение производной векторной(матричной) функции. 

**Матричное дифференцирование**
*{отнести к методам машинного обучения}*

Свойства векторного(матричного) дифференцирования
* Линейность
* Производная от произведения
* Производная от скалярного поризведения
* Производная сложной функции, см. Лейбница

**Матричные возмущения**
Теория возмущений оперирует понятием разложение в ряд Тейлора. Ряд тейлора можно примнить для коммутативных и анти-коммутативных алгебр. Можно обобщить на случай алгебр с операцией коммутации, если определена операция - коммутатор. Приложением матричных функций являются численные вариационные методы и методы дискретные, которые требуют повышения точности (см. производная в конечных приращениях, остаточный член в форме Лагранжа). Для этого производная обобщаяется до производной векторной/матричной функции.

*{дописать}*
 
*Предлагается в курсе реализовать численный метод возведение матрицы в произвольную степень с логарифмической сложностью. Реализовать метод возведения в степень через диагонализацию матрицы, на примере LDL^T разложения Холецкого.*

**Плотная** и **разряженная** матрица (Dense matrix, Sparse matrix).

**Ранг** матрицы.  (full rank, low-rank, rank-deficient, rank revealing decomposition)

*Рангом* системы строк (столбцов) матрицы $A$ с $m$ строками и $n$ столбцами называется максимальное число линейно независимых строк (столбцов). Несколько строк (столбцов) называются линейно независимыми, если ни одна из них не выражается линейно через другие. Ранг системы строк всегда равен рангу системы столбцов, и это число называется рангом матрицы.

{перенести ниже}

В ряде приложений разложение матриц приследует цель - снижение вычислительной сложнотси. Для этого разложение может быть выполнено на матрицы обладающие минимальным рангом. 
Eсли известно, что матрица $A\in\mathbb{R}^{m\times n}$ имеет неполный ранг, то существует такое $r = \mathrm{rank}(A) < \min(m,n)$. Иногда $r$ известен, тогда разложение с полным рангом $A = GH$, где $G\in\mathbb{R}^{m \times r}$ и $H\in\mathbb{R}^{r \times n}$, обе матрицы имеют ранг $r$. Однако часто ранг $r$ неизвестен. Более того, вместо точного ранга $r$, часто бывает $A$ близка к матрице с рангом $r$ из-за ошибок (округление, фазовые ошибки в преобразованиях подобия и шумы), возникающих по разным причинам. Для выявления ранга требуется выполнить разложение выявляющее ранг (rank-revealing).

Наилучшим инструментом является метод SVD - сингулярное разложение, разложение по собственным числам.
```math
A = U\Sigma V^T, \quad \Sigma = \mathrm{diag}(\sigma_1,\dots, \sigma_p)\in\mathbb{R}^{m\times n}~,
```
где $p = \min(m,n), \sigma_1\ge \sigma_2\ge \cdots \ge \sigma_p \ge 0$, а матрицы $U\in\mathbb{R}^{m\times m}$ и $V\in\mathbb{R}^{n \times n}$ ортогональны.

Существуют более экономичные методы разложения с выявлением ранга матрицы, такие как: RR-QR, RR-LU, COD, CS разложения, UTV разложения.

* [4] Golub & van Loan. 5.4 Other Orthogonal Factorizations
* <https://nhigham.com/2021/05/19/what-is-a-rank-revealing-factorization/>
* <https://nhigham.com/2020/10/27/what-is-the-cs-decomposition/>
* [4] Golub & van Loan. 2.5.4 The CS Decomposition

**Норма вектра** в ${K}^n$ над полем $K$ это такая функция $f:{K}^n \to {K}$, которая удовлетворяет свойствам:
* $f(x) \geq 0$, $x \in {K}^n$, причем $f(x)=0$ только при $x=0$;
* $f(x+y) \geq f(x) + f(y), \quad x,y \in {K}^n$;
* $f(\alpha x) = |\alpha| f(x), \quad \alpha\in {K},x \in {K}^n$.

В теории функции определяется p-норма [1]:
```math
\Vert x \Vert_p = (|x_1|^p + ... + |x_n|^p)^{\frac{1}{p}}~, \quad p \geq 1.
```
По отношению к норме определяется понятие _единичный вектор_, $\Vert x \Vert_p =1$. 
В основном используются нормы 1 - абсолютная, 2-квадратичная, и $\infty$- норма.
```math
\begin{aligned}
&\Vert x \Vert_1 = |x_1| + ... + |x_n|\\
&\Vert x \Vert_2 = (|x_1|^2 + ... + |x_n|^2)^{\frac{1}{2}}\\
&\Vert x \Vert_{\infty} = \max\limits_{1\leq i \leq n} |x_i|
\end{aligned}
```


В $n$-мерном евклидовом (векторном) пространстве $\mathbb{R}^n$ используется квадратичная 2-норма, определенная через скалярное произведение:

$$\Vert x \Vert_\mathsf{E} = \sqrt{\langle x,x \rangle} = \sqrt{x^{\mathsf{T}}x}$$

**Норма матрицы** вводится согласованно с определением нормы вектора. Норма рождается вместе с понятием нормуируемого линейного пространства. Например, линейное пространство всех матриц ${m \times n}$ на множестве $\mathbb{R}^{m \times n}$. Также вводится пространство линейных операторов, через определение скалярного произведения и нормы, порожденной скалярным произведением. 

*Норма $L_{p,q}$* определяется через две нормы векторные [3, Гл. XIX]. Матрицу можно представить, как вектор-столбец состоящий из векторов-строк. Определение обобщенной нормы матрицы строится на двух p-нормах:
```math
{\displaystyle \Vert A\Vert _{p,q}=\left(\sum _{j=1}^{n}\left(\sum _{i=1}^{m}|a_{ij}|^{p}\right)^{q/p}\right)^{1/q}}, \quad p,q \geq 1
```

*Норма Фробениуса* (норма евклидова пространства) вводится аналогично 2-норме вектора, как сумма квадратов всех элементов матрицы или аналогично норме $L_{2,2}$. 
```math
{\displaystyle \Vert A\Vert _{\mathsf{F}}=\Vert A\Vert _{2,2}=\left(\sum _{j=1}^{n}\sum _{i=1}^{m}|a_{ij}|^2\right)^{1/2}}
```
Свойства евклидовой нормы:
* $\Vert Ax \Vert_{F}^{2} \leq \Vert A\Vert_{F}^{2}\Vert x\Vert_2^{2}$
* $\Vert AB \Vert_{F}^{2} \leq \Vert A\Vert_{F}^{2}\Vert B\Vert_{F}^{2}$
* $\Vert A\Vert_{F}^{2}=\mathop{\rm{tr}} A^{\ast}A=\mathop {\rm{tr}} AA^{\ast}$, где 
$\mathop {\rm{tr}} A$ — след матрицы, ${\displaystyle A^{\ast}}$ — эрмитово-сопряжённая матрица.
* $\Vert A\Vert_{F}^{2}=\sigma _{1}^{2}+\sigma _{2}^{2}+\dots +\sigma _{n}^{2}$, где 
$\sigma _{1},\sigma _{2},\dots ,\sigma _{n}$ — сингулярные числа матрицы $A$.
* $\Vert A\Vert_{F}$ не изменяется при умножении матрицы $A$ слева или справа на ортогональные (унитарные) матрицы.

*Норма (матрицы) линейного оператора* (Операторная норма) . Норма линейного оператора $\mathcal{A}:K^n \mapsto K^m$ строится как согласованное определение из двух векторных норм, заданных в линейных нормируемых простарнствах $K^n$ и $K^m$. 

**Число обусловленности** 

Для квадратной матрицы число обусловленности $k_2(A) = \Vert A\Vert_2 \cdot \Vert A^{-1}\Vert_2$ выражается через квадратичную норму матрицы. Если число обусловленности мало, то говорят, матрица хорошо-обусловленна (well-conditioned). Если велико, то плохо обусловлена (ill-conditioned). Для ортогональной матрицы $Q$, $κ_2(Q) = \Vert Q \Vert _2 \cdot \Vert Q^T \Vert _2 = 1$ (perfect conditioned).

**Положительно определенная матрица**

**Спектрально обратимая матрица**
*Определение* 
Пусть G – невырожденная четномерная матрица размерности $2n$, 
$\mathop{\rm{det}} G≠0$ простой структуры (см. [Гант], гл. 3, §8, стр.85), не имеющая кратных точек спектра  $λ_1,...,λ_{2n}$, $λ_i ≠ λ_j$, если i≠j, ij =1:2n.
Матрицу G назовем спектрально обратимой, еcли ...

## LU- разложение матриц и решение систем СЛАУ методом LU ##

Методы вычисления LU-разложения:
- метод Гаусса;\
(низкая точность для разреженных матриц, большой объем вычислений)
- алгоритм Дулиттла;\
(L - унитреугольная, U - верхняя треугольная)
- алгоритм Кроута.\
(L - нижняя треугольная, U - унитреугольная)

*LUP*-разложение (LUP-декомпозиция) — представление данной матрицы 
𝐴 в виде произведения $P A = L U$ где матрица 
𝐿 является нижнетреугольной с единицами на главной диагонали, 
𝑈 — верхнетреугольная общего вида, а 𝑃 — т. н. матрица перестановок, 
получаемая из единичной матрицы путём перестановки строк или столбцов. 

*LUP*-разложение это LU-разложения матрицы A с частичным выбором ведущего
элемента (перестановка по строкам), которое записывается следующим образом:
$$P A = L U ,$$
а в случае перестановок по столбцам LUP-разложение записывается в виде:
$$A Q = L U ,$$
а, в наиболее общем виде:
$$P A Q = L U ,$$
где:\
$P$ - матрица перестановок по строкам;\
$Q$ - матрица перестановок по столбцам;\
$L$ - нижняя унитреугольная матрица;\
$U$ - верхняя треугольная матрица.

*LDU*-разложение матрицы A - это ее представление в виде произведения трех
матриц:
$$A = L D U ,$$
где:\
$D$ - диагональная матрица;\
$L$ - нижняя унитреугольная матрица;\
$U$ - верхняя унитреугольная матрица.

*LL* - разложение (Разложение Холецкого)
LL-разложение положительно определенной матрицы A - это ее представление в
виде произведений двух матриц:
$$A = L L^\ast \quad A = U^\ast U$$
где:\
$L$ - нижняя треугольная матрица; \
$U$ - верхняя треугольная матрица.

Для вещественных матриц операция сопряжения заменяется на транспонирование: $L^\ast = L^\mathsf{T}$ и $U^\ast = U^\mathsf{T}$.

## Спектральная теорема

Любая эрмитова матрица по теореме о спектральном разложении может быть представлена, как вещественная диагональная матрица 
$D$, переведённая в другую систему координат (то есть $M=U^{-1}D U$, где $U$ — унитарная матрица, строками которой являются ортонормальные собственные векторы $M$, образующие базис). По этому определению $M$ — положительно определённая матрица, если все элементы главной диагонали $D$  положительны (собственные значения $M$  положительны). То есть в базисе, состоящем из собственных векторов $M$, действие $M$ на вектор $z\in \mathbb{C}^{n}$ равносильно покомпонентному умножению $z$ на положительный вектор.

## Разложение Холецкого

*Разложе́ние Холе́цкого* (метод квадратного корня) — представление симметричной положительно определённой матрицы $A$ в виде 
$A=L L^{\mathsf{T}}$, где $L$ — нижняя треугольная матрица со строго положительными элементами на диагонали. Иногда разложение записывается в эквивалентной форме: $A=U^{T}U$, где $U=L^{T}$ — верхняя треугольная матрица. Разложение Холецкого всегда существует и единственно для любой симметричной положительно определённой матрицы.

Существует обобщение разложения на случай комплекснозначных матриц. Если $A$ — положительно определённая эрмитова матрица, то существует разложение $A=LL^{\ast}$, где $L$ — нижняя треугольная матрица с положительными действительными элементами на диагонали, а $L^{\ast}$ — эрмитово-сопряжённая к ней матрица.

*Очевидно, должно существовать LL-разложение и для анти-симметричных матриц.*

Разложение названо в честь французского математика польского происхождения Андре-Луи Холески (1875—1918).

Это разложение может применяться для решения системы линейных уравнений $Ax=b$, если матрица 
$A$ симметрична и положительно определена. Такие матрицы часто возникают, например, при использовании метода наименьших квадратов и численном решении дифференциальных уравнений.

*Theorem (Cholesky Decomposition)*. A symmetric, positive definite matrix $A$ can be factorized into a product $A = L L^{\mathsf{⊤}}$, where $L$ is a lowertriangular matrix with positive diagonal elements.
L is called the Cholesky factor of A, and L is unique.

*Definition (Diagonalizable)*. A matrix $A \in \mathbb{R}^{n\times n}$ is diagonalizable
if it is similar to a diagonal matrix, i.e., if there exists an invertible matrix
$P \in \mathbb{R}^{n\times n}$ such that $D = P^{−1}A P$ .

*Theorem*. A symmetric matrix $S \in \mathbb{R}^{n\times n}$ can always be diagonalized.

Существует несколько алгоритмов получения разложения Холецкого:
- классический алгоритм Холецкого;
(в его основе метод исключения Гаусса при приведении исходной матрицы к нижнетреугольному виду)
- алгоритм Холецкого-Банахевича;\
(алгоритм вычисления матрицы L начиная с первой строки и далее итерационно по каждой строке)
- алгоритм Холецкого-Кроута;\
(алгоритм вычисления матрицы U начиная с первого столбца и далее итерационно по каждому столбцу)

Алгоритм Кроута в обобщенном виде является алгоритмом вычисления LU-разложения,
где $L$ - нижняя треугольная, а $U$ - унитарная верхняя треугольная.


## Разложение Холецкого с перестановками

Симметричная положительно определённая квадратная матрица $A$ имеет альтернативное разложение Холецкого на произведение нижней единичной треугольной матрицы $L$, диагональной матрицы $D$ и $L^{\mathsf{T}}$. Для положительно определённых матриц это эквивалентно рассмотренному выше $LDL^{\mathsf{T}}$ разложению Холецкого. Для плохо обусловленных матриц можно использовать стратегию поворота, чтобы предотвратить рост элементов $D$ и $L$ до слишком больших значений, а также обеспечить $D_1 \ge D_2 \ge \cdots \ge D_n > 0$, где $D_i$ — диагональные элементы $D$. Окончательное разложение определяется как
```math
P A P^{\mathsf{T}} = L D L^{\mathsf{T}}
```
где $P$ - матрица перестановок.

## Модифицированное разложение Холецкого

Модифицированное разложение Холецкого подходит для решения систем $A x = b$, где $A$ — симметричная знакоопределённая матрица. Такие матрицы возникают в алгоритмах нелинейной оптимизации. Стандартное разложение Холецкого требует положительно определённой матрицы и в этом случае не сработает. Вместо того чтобы прибегать к таким методам, как QR или SVD, которые не учитывают симметрию матрицы, мы можем внести небольшое изменение в матрицу $A$ чтобы сделать её положительно определённой, а затем использовать разложение Холецкого для изменённой матрицы. Полученное разложение удовлетворяет
```math
P (A + E) P^{\mathsf{T}} = L D L^{\mathsf{T}}
```
где $P$ — матрица перестановок, $E$ — диагональная матрица возмущений, $L$ — единичная нижняя треугольная матрица, $D$ — диагональная матрица. Если $A$ является положительно определённой, то матрица возмущений E будет равна нулю, и этот метод будет эквивалентен алгоритму Холецкого с поворотом. Для неопределённых матриц матрица возмущений E вычисляется таким образом, чтобы $A + E$ была положительно определённой и хорошо обусловленной.


### Трехдиагональная декомпозиция вещественных симметричных матриц

Симметричная матрица $A$ может быть разложена на множители преобразованиями подобия в виде,
```math
A = Q T Q^{\mathsf{T}}
```
где $Q$ — ортогональная матрица, а $T$ — симметричная трёхдиагональная матрица.

*{Трех-диагональные матрицы возникают при решении систем дифференциальных уравнений в конечных разностях}*

### Трехдиагональное разложение эрмитовых матриц

Эрмитова матрица $A$ может быть разложена на множители с помощью преобразований подобия в вид,
```math
A = U T U^{\mathsf{T}}
```
где $U$ — унитарная матрица, а $T$ — вещественная симметричная трёхдиагональная матрица.

## Разложение вещественных матриц по Xессенбергу

Произвольная вещественная матрица $A$ может быть разложена с помощью ортогональных преобразований подобия в форму
```math
A = U H U^{\mathsf{T}}
```

где $U$ является ортогональной, а $H$ является верхней матрицей Xессенберга, то есть имеет нули ниже первой поддиагонали. Приведение к Хессенбергу является первым шагом в разложении Шура для несимметричной задачи о собственных значениях, но применяется и в других областях.

## Хессенберг-Треугольная декомпозиция вещественных матриц
Общая пара вещественных матриц $(A, B)$ в обобщенной задаче на собственные значения $Av = \lambda Bv$ может быть разложена с помощью ортогональных преобразований подобия в форму
```math
A = U H V^{\mathsf{T}} \\ 
B = U R V^{\mathsf{T}}
```

где $U$ и $V$ являются ортогональными, $H$ является верхней матрицей Хессенберга, а $R$ является верхней треугольной матрицей. Приведение к треугольному виду Xессенберга является первым шагом в обобщённом разложении Шура для обобщённой задачи на собственные значения.

## QR разложение матрицы

*QR* - разложение матрицы — представление матрицы в виде произведения унитарной 
(или ортогональной матрицы) и верхне-треугольной матрицы. QR-разложение является 
основой одного из методов поиска собственных векторов и чисел матрицы — QR-алгоритма. 

Произвольная прямоугольная матрица A размером $M\times N$ имеет QR разложение, где, $Q^\ast Q = I$ - ортогональная (унитарная) матрица $M \times M$, 

## LQ разложение матрицы
LQ разложение эквивалентно QR разложению транспонированной матрицы.

Произвольная прямоугольная матрица A размером $N\times M$ имеет LQ разложение, где, $Q^\ast Q = I$ - ортогональная (унитарная) матрица $M \times M$, $L$ - нижняя треугольная матрица.

*Заметим, комбинация методов QR-LQ позволяет выполнить диагонализацию матрицы. см. QR-алгоритм поиска собственных значений и векторов.* 

## Сингулярное разложение

The singular value decomposition (SVD) of a matrix is a central matrix
decomposition method in linear algebra. It has been referred to as the
“fundamental theorem of linear algebra” (Strang, 1993) because it can be
applied to all matrices, not only to square matrices, and it always exists.

**Theorem (SVD Theorem)**. Let $A ∈ R^{m×n}$ be a rectangular matrix of
rank $r ∈ [0, \min(m, n)]$. The SVD of A is a decomposition of the form
$A = U\Sigma V$\
with an orthogonal matrix $U ∈ \mathbb{R}^{m×m}$ with column vectors $u_i, i = 1, ... , m$,\
 and an orthogonal matrix $V ∈ \mathbb{R}^{n×n}$ with column vectors $v_j, j = 1, ... , n$.\
Moreover, $\Sigma$ is an $m × n$ matrix with $\Sigma_{ii} = σ_i \geqslant 0$ and $\Sigma_{ij} = 0, ~~i \neq j$.

* [1] Kalman, Dan. 1996. A Singularly Valuable Decomposition: The SVD of a Matrix. College Mathematics Journal, 27(1), 2–23.
* [2] Roy, Anindya, and Banerjee, Sudipto. 2014. Linear Algebra and Matrix Analysis for
Statistics. Chapman and Hall/CRC.

## Разложение Шура

Для вещественной матрицы $A=VSV^{\mathsf{T}}$,
где $V$ — ортогональная матрица, а $S$ — квазитреугольная. Последняя называется вещественной формой Шура. Блоки на диагонали 
$S$ либо размера 1×1 (действительные собственные значения) или 2×2 (образованные парой комплексно-сопряжённых собственных чисел).

Для комплесной матрицы $A=UTU^{\mathsf{H}}$, где $U$ — унитарная, $U^{H}$ — её эрмитово-сопряжённая, а $T$ — верхняя треугольная матрица, называемая комплексной формой Шура, которая содержит собственные значения $A$ на диагонали.

## Спектральное разложение матрицы

Спектральное разложение матрицы A - это ее представление в виде произведения трех матриц:
$$A = V \Lambda V^{−1},$$
где:\
$V$ - матрица с собственными векторами матрицы $A$;\
$\Lambda$ - диагональная матрица с собственными значениями $\lambda(A)$.
Только матрицы, имеющие полный набор собственных значений, могут быть представлены в виде спектрального разложения.

Каждая _нормальная матрица_ $A$ (такая что $AA^\ast = A^\ast A$) имеет спектральное разложение. К номрмальным относятся все унитарные, эрмитовы, анти-эрмитовы; вещественные отртогональные, симметричные и антисимметричные.
Любая симметричная матрица может быть разложена $A=V\Lambda V^{\mathsf{T}}$, где $V$ и $\Lambda$ вещественные матрицы.

Заметим интересное свойство разложения, которое может быть использовано при оптимизации матричных операций. Если $x_t = A^t x_0$, то $x_t = V\Lambda^t V^{-1} x_0$. Причем степенная функция от диагональной матрицы означает возвещение каждого элемента в степень независимо от других. 

## Каноническая форма Жордана

Каноническая форма Жордана - это представление квадратной матрицы A в виде:
$$A = C J C^{−1},$$
где:\
$C$ - матрица перехода к новому базису;\
$J$ - матрица Жордана.

По сути, это разложение - обобщение спектрального разложения на случай кратных собственных значений.
Если кратность всех собственных значений матрицы A равна единице, то матрица
J - диагональная. В противном случае матрица J является блок-диагональной и
состоит из Жордановых блоков.

**Theorem 7.1.9 (Jordan Decomposition)**. Если $A \in \mathbb{C}^{n \times n}$ существует не сингулярная матрица $X \in \mathbb{C}^{n \times n}$ такая что $X^{-1}A X = \mathrm{diag}(J_1,...,J_q)$

```math
J_i = \begin{bmatrix}
\lambda_i & 1 & \cdots & \cdots & 0 \\
0 & \lambda_i & \ddots & & \vdots \\
\vdots &  \ddots & \ddots & \ddots & \vdots \\
\vdots &  &\ddots & \lambda_i & 1 \\
0 & \cdots & \cdots & 0 &\lambda_i\\
\end{bmatrix} \in \mathbb{C}^{n_i \times n_i}
```
$J_i$ называется Жордановой клеткой.

* [3] Гл.XV Структура линейного оператора в комплексном пространстве
* [3] Гл.XV §93 Вещественный аналог жордановой формы

## Здача поиска собственных значений
$Av = \lambda v$

Задача может быть решена через факторизацию полиномов. 

Характеристическим многочленом матрицы $A \in K^{n \times n}$ называется функция
$f(\lambda) = \mathrm{det}(A - \lambda I)$, где $\lambda \in K$.

* *[3] Гл.XV §86 Характеристический многочлен

## Обобщенная задача поиска собственных значений
$Av = \lambda Bv$

Для решения такой задачи используются специальные методы разложения матриц, такие как обобщенное разложение Шура (см. QZ разложение).

## Метод наименьших квадратов

**Least Squares Problem**

Рассмотрим случай Full-Rаnk когда число уравнений больше числа неизвестных. Из всех вариантов нормы выбираем квадратичную норму, поскольку она дифференцируема (в отличие от абсолютной нормы) и позволяет посчитать градиент. 

$$\min\limits_{x\in \mathbb{R^n}} \Vert Ax-b\Vert _2$$

* При выборе функции $\phi(x) = \Vert Ax-b\Vert_2^2$ можно использовать метод графдиентного спуска для поиска минимума $\nabla \phi(x) = 0$.
* Другое важно свойство квадратичной нормы - она не меняется при ортогональных преобразованиях. Может быть использована эквивалентная задач поиска минимума нормы $\Vert(Q^{\mathsf{T}}A)x - Q^{\mathsf{T}}b\Vert_2$


**Algorithm 5.3.1 (Normal Equations)**
Метод решения эквивалентной задачи минимизации нормы $\Vert (A^{\mathsf{T}}A)x - A^{\mathsf{T}}b\Vert_2$ с использованием разложения Холецкого для симетричных матриц.

**Algorithm 5.3.2 (Householder LS Solution)**
Метод решения задачи минимизации нормы $\Vert Ax - b\Vert _2$ с использованием QR разложения.

## Ортогонализация

Обычно мы работаем в евклидовом n-мерном пространстве $\mathbb{R}^n$. Или комплексном (унитарном) n-мерном пространстве с определенной нормой через скалярное произведение векторов. Операцию скалярного произведения векторов (dot product) обозначаем $v\odot w = v^{\mathsf{T}} w = \langle v, w \rangle$. 
В англоязычной литературе различают внутреннее (inner product $v^\mathsf{T} w$, порождает скаляр) и внешнее произведение векторов (outer product $v w^\mathsf{T}$, порождает матрицу). Знак $\odot$ означает поэлементное умножение векторов или матриц (тензоров), применяется в публикациях по нейросетям, соответствует произведению Адамара.  

Скалярное произведение вводится, как сумма

$$\langle v, w \rangle = \sum\limits_{k=1}^{n} v_k w_k$$

Аналогично вводится скалярное произведение в комплексном пространстве $\mathbb{C}^n$. 

$$\langle \mathbf{v}, \mathbf{w} \rangle = \sum\limits_{k=1}^{n} {v_k} \overline{w_k}$$


Два вектора $v,w \in V(\mathbb{R}^n)$ называются _ортогональными_, если их скалярное произведение $\langle v, w \rangle = 0$. 

Скалярное произведение определяет норму вектора $\Vert v \Vert  = \sqrt{\langle v, v\rangle}$

*Определение* Базис $u_1, ..., u_n$ в векторном пространстве называется _ортогональным_ если все элементы базиса взаимно ортогональны, $u_i \odot u_j=0$ , для всех $i\neq j$.
Базис называется _ортонормированным_, если все вектора базиса - единичной длины $\Vert u_i \Vert=1$.

Ортогональный базис $v_1, ..., v_n$ в векторном пространстве, нормируется $u_i = \cfrac{v_i}{\Vert v_i\Vert}$ дает ортонормированный базис.

Единичный базис. Любой ортонормированный базис изоморфно может бтыь представлен через единичный базис и матрицу поворота - ортогональную матрицу. 

Квадратная матрица $Q$ называется ортогональной, если $Q^{\mathsf{T}} Q = I$. Свойство ортогональных матриц $Q^{-1} = Q^{\mathsf{T}}$

Вещественная матрица $Q$ ортогональна тогда и только тогда её колонки представляют собой ортонормированный базис. 

Теорема. Пусть A - вещественная симметричная матрица. Тогда существует такая ортогональная матрица Q
$$A = Q \Lambda Q^{-1} = Q \Lambda Q^{\mathsf{T}}$$
где  $\Lambda$ - диагональная матрица с собственными значениями $(\lambda_1, ..., \lambda_{n} )$ на диагонали, а колонки матрицы Q представляют собой ортонормированные собственные вектора. 

Определение линейная независимость, линейная кобинация. 

Косинус угла между векторами в n-мерном евклидовом пространстве можно определить через скалярное произведение нормированных векторов.
$$a_i = \cfrac{\langle w, v_i \rangle}{\Vert  w_i \Vert  \Vert v_i \Vert}$$

Процесс ортогонализации можно представить, как последовательное вычитание проекции одного ввектора на другой, пока все вектора не станут ортогональными. 


### Процесс Грама-Шмидта

Процесс Грама-Шмидта — это метод вычисления ортогональной матрицы Q, которая состоит из ортогональных или независимых единичных векторов и занимает такое же пространство, что и матрица X.

1. Выбрать вектор столбец $$u_1 = x_1,\quad x_1 \neq 0$$
2. Рассчитать для всех столбцов $$u_k = x_k - \sum_{j=1}^{k-1} \frac{\langle x_k, u_j \rangle}{\Vert u_j \Vert^2} u_j$$
3. Для всех векторов выполнить нормализацию $$Q_k = \cfrac{u_k}{\Vert u_k \Vert}$$

По каждому столбцу считаем проекции ортогональных векторов на данный вектор столбец и вычитаем эти проекции. Проекции считаются с использованием операции скалярного произведения векторов.

proj - считает ортогональный вектор $u_k$ к другим, найденным на предыдущем шаге.
```math
QR = X, \quad
R = Q^{-1} X = Q^{\mathsf{T}} X
```
Матрица R находится из свойства ортогональных матриц $Q^{-1} = Q^{\mathsf{T}}$.

<https://www-users.cse.umn.edu/~olver/aims_/qr.pdf>

На практике следует использовать модифицированный алгоритм Грама ― Шмидта (MGS), 
поскольку классический алгоритм обладает плохой численной устойчивостью [3].

*У нас в реализации используется модифицированный алгоритм MGS*

## Преобразования подобия

Альтернативные алгоритмы для вычисления QR-разложения основаны на отражениях Хаусхолдера и вращениях Гивенса[4].

Ключом для понимания сути этих методов является аналог преобразования вращения и преобразования отражения в Евклидовой геометрии. 
Путем вращения системы вокруг базисных векторов можно получить преобразование из одной системы координат в другую. 
Любое вращение можно представить, как поворот вокруг вектора нормального к плоскости вращения. Отражение двух векторов одинаковой длины можно определить плоскостью. Обобщение на случай n-мерного пространства, дает преобразование Гивенса и Хаусхолдера.
Каждое такое преобразование можно представить, как ортогональную матрицу.

*Для полноты картины нужна отсылка к группам вращений, к алгебрам Ли.*

Преобразование вращения на заданный угол можно представить, как левое и правое произведение на матрицу вращения $A = P B P^{-1}$, где $B$ - матрица в новых координатах, а $P$ - матрица вращения. Любую ортогональную матрицу можно представить, как последовательность вращений вокруг базисных ортогональных векторов. Частным случаям вращения является матрица перестановок строк. Также преобразование системы координат можно построть на отражениях. 

Квадратные матрицы $A$ и $B$ одинакового порядка называются подобными, если существует такая невырожденная матрица $P$ того же порядка, что $B=P^{-1}AP$.

> Подобные матрицы получаются при задании одного и того же линейного преобразования матрицей в разных координатных системах; при этом матрица Р является матрицей перехода от одной системы координат к другой.

> Если две матрицы подобны, то говорят, что одна из матриц может быть получена _преобразованием подобия_ из другой. Если при этом одна из матриц диагональная, то про вторую матрицу говорят, что она может быть диагонализована.

* [8] Н.Н. Калиткин. Численные методы 2-e изд. Гл.4 §2 Эрмитовы матрицы. Метод отражений. Прямой метод вращений.
* [4] "Golub & Van Loan. Matrix Computations 4-th (2013). 5.1. Householder and Givens Transformations"

### Вращение Гивенса 

Матрица отличается от единичной матрицы только подматрицей $2 \times 2$
```math
M(\phi )={\begin{bmatrix}\cos {\phi }&-\sin {\phi }\\
\sin {\phi }&\cos {\phi }\end{bmatrix}}
```

расположенной на строках и столбцах с номерами $k$ и $l$, является ортогональной.

Если дан вектор 
$a=[a_{1} \ldots a_{n}]^{T}\in \mathbb{R}^{n}$, $s=\sqrt{a_{k}^{2}+a_{l}^{2}} \neq 0$, то выбрав

$$\cos{\phi}=\frac{ a_{k}}{\sqrt{a_{k}^{2}+a_{l}^{2}}}$$

$$\sin{\phi}=\frac{-a_{l}}{\sqrt{a_{k}^{2}+a_{l}^{2}}}$$

можно обнулить $l$-ую компоненту вектора $a$:

```math
\begin{bmatrix}
\cos {\phi }&-\sin {\phi }\\
\sin {\phi }& \cos {\phi }
\end{bmatrix}
\begin{bmatrix}
a_{k}\\
a_{l}
\end{bmatrix}
=
\begin{bmatrix}{\sqrt {a_{k}^{2}+a_{l}^{2}}}\\
0
\end{bmatrix}
```

С помощью поворотов Гивенса можно вычислять QR-разложение матриц и приводить эрмитовы матрицы к диагональной форме, а матрицы общего вида к трёхдиагональной, треугольной или хессенберговской форме.

Также того же самого можно добиться при помощи преобразований отражения Хаусхолдера. Матрица отражения 2x2 (на плосткости) 
```math
\begin{bmatrix}
\cos {\phi }& \sin {\phi }\\
\sin {\phi }&-\cos {\phi }
\end{bmatrix}
```

Исторически метод вращения был предложен Якоби, (см. [вращение на единичный кватернион](QUAT.md)), работа 1846 года. Для кватернионов поворот оперделеятеся через матрицу вращений (Гивенса) на половинный угол. Метод вращения Якоби может использоваться для приведения матрицы к трехдиагональному виду, и для поиска собственных значений. Также следует рассмотреть применимость метода для гамильтоновых матриц, эрмитовых, трехдиагональных, симметричных и анти-симметричных. 

Любую вещественную матрицу можно представить, как сумму симметричной и антисимметричной матриц. Соответственно, любую комплексную матрицу можно предсавить, как сумму эрмитовой и анти-эрмитовой матриц. Антиэрмитову(антисимметричную) матрицу $A$ можно представить, как $A=jB$, где $B$ - эрмитова матрица, $j$ - мнимая единица или соответствующая ей симплектическая единичная матрица.

$A\mapsto Q_{k\ell}^{T}AQ_{k\ell}=A'$.
```math
\begin{bmatrix}
a_{kk} & a_{k\ell}\\
a_{\ell k} & a_{\ell \ell}
\end{bmatrix} \to
\begin{bmatrix}
a'_{kk} & 0\\
0 & a'_{\ell\ell}
\end{bmatrix}
```

Пусть $A$ — симметричная матрица, а $G=G(i,j,\theta )$ — матрица вращения (Гивенса). Тогда $A'=G^{\mathsf{T}}AG$
```math
\begin{bmatrix} c&s\\-s&c\end{bmatrix}^{\mathsf{T}} 
\begin{bmatrix}
a_{kk} & a_{k\ell}\\
a_{\ell k} & a_{\ell \ell}
\end{bmatrix} 
\begin{bmatrix} c&s\\-s&c\end{bmatrix} =
\begin{bmatrix}
a'_{kk} & 0\\
0 & a'_{\ell\ell}
\end{bmatrix}
```


см. Golub & van Loan[4] §8.5 Jacobi Methods

При вращениях норма матрицы не меняется. Критерием сходимости итерационного метода является уменьшение нормы внедиагональных элементов матрицы (увеличение суммы квадратов диагональных элементов матрицы). 

см. Frobenius norm

### Вращение Гивенса комплексных матриц

Обобщим определение вращения Гивенса, как матрицу, которая приводит два элемента
```math
R(c,s) = 
\begin{bmatrix}
c  & s \\
-\bar{s} & \bar{c}
\end{bmatrix} \begin{bmatrix} u \\ v\end{bmatrix} = 
\begin{bmatrix} r \\ 0\end{bmatrix}
```
функция вычисляет три величины $c,s$ и $r$ - норма.
Норма определяется через скалярное произведение и операцию сопрояжения
$\Vert v\Vert^2 = v^{\mathsf{H}}v$.

Условие ортогональности 
$R(c,s) R(c,s)^\ast = I_n$

Выполняется тождество
$$|c|^2 + |s|^2 = 1$$

Но сами значения задаются неодназначно
```math
\begin{aligned}
с &= \omega \cdot \frac{\bar{u}}{\sqrt{|u|^2 + |v|^2}}\\
s &= \omega \cdot \frac{\bar{v}}{\sqrt{|u|^2 + |v|^2}}\\
r &= \omega \cdot \sqrt{|u|^2 + |v|^2}
\end{aligned}
```
где $\omega$ произвольный комплексный вектор единичной длины.

### Использование поворотов Гивенса для трёхдиагонализации

Последовательно вращая ($G_{kl}AG_{kl}^{\mathsf{T}}$) плоскости (2, 3), (2, 4), ... , (2, n) (при этом зануляя элементы 
$a_{31},a_{41},...,a_{n1}$), затем последовательно вращая плоскости (3, 4), (3, 5), ... , (3, n) (при этом зануляя элементы 
$a_{42},a_{52},...,a_{n2}$) и т.д. можно привести эрмитову (симметричную) матрицу к трёхдиагональной форме, а произвольную матрицу к хессенберговой форме.

*Аналогично может быть использован метод вращения Якоби.*

### Преобразование отражения Хаусхолдера

*Определение* Пуcть гиперплоскость описывается единичным вектором $u$, который ей ортогонален, тогда 
$$H_{u}(x)=x-2\langle x,u\rangle u$$
называется оператором Хаусхолдера.

В геометрическом смысле, между двумя векторами $x, y$ с одинаковой нормой всегда можно найти гиперплоскость и построить матрицу отражения $y = Px$. Матрица будет ортогональной и симметричной. Проекция вектора на нормаль $u$ будет определяться скалярным произведением, проекция вектора на плоскость останется неизменной. Чтобы получить отраженный вектор надо вычесть две проекции на нормаль.

$$y = x - 2 u\langle u, x \rangle$$

Компоненты вектора вычисляются как 
$$y_i = x_i - 2 u_i  \sum\limits_{j=1}^N \bar{u}_j x_j$$

Матрица отражения Хаусхолдера имеет вид:
$$H = I - 2uu^{\mathsf{H}}$$

Свойства Матрицы Хаусхолдера:
* является эрмитовой: $H=H^{\ast}$;
* является унитарной: $H^{\ast}H=I$;
* является инволюцией: $H^2 = I$;
* имеет одно собственное значение, равное $−1$, которое соответствует собственному вектору $u$,
* Определитель матрицы Хаусхолдера равен $-1$.

Ненулевой вектор $v \in \mathbb{R}^n$ образует матрицу $P \in \mathbb{R}^{m \times n}$
```math
P = I_n - \beta v v^{\mathsf{T}}, \quad \beta = \frac{2}{v^{\mathsf{T}} v}
```
-- матрица отражения Хаусхолдера

На множестве комплексных чисел $v \in \mathbb{С}^n$ образует матрицу $P \in \mathbb{С}^{m \times n}$
```math
P = I_n - \beta v v^{\mathsf{H}}, \quad \beta = \frac{2}{v^{\mathsf{H}} v}
```



Операции с матрицами отражения включают 
* `_hauseholder` - расчет скалярного произведения $v^{\mathsf{T}} v$ и вектора $v$.
* правое наложение $AP = A (I − βvv^T) = A − (Av)(βv)^{\mathsf{T}}$.
* левое  наложение $PA = (I − βvv^T)A = A − (βv)(v^{\mathsf{T}} A)$

cм. Golub & van Loan[4].  Algorithm 5.1.1 (Householder Vector)

{дописать}

Матрицу Q можно представить как произведение матриц отражения $Q = H_n...H_2H_1$

*{перенести к методам HH}*
Существует несколько вариантов умножения. 
слева-направо и справа-налево. Оба метода применимы для восстановления ортогональной матрицы в методе QR разложения Хаусхолдера. При рзаложении матрица A заменяетcя на матрицу R (верхняя треугольная), а нулевые элементы под главной диагональю заменяются на вектора $v(j:m)$.
```c
Q = I_m
for j=1:n
   Q = Q·Q_j
```
```c
Q = I_m
for j=n:-1:1
   Q = Q_j·Q
```
см. [4] Golub & van Loan. (5.1.5) 
```py
Q = I_m[:, 1:k]
for j = n:-1:1 :
    Q[j:m, j:k] = Q[j:m, j:k] − (β_j v[j:m])·(v[j:m]ᵀ Q[j:m, j:k])
```

### QR разложение

Существует множество методов для решения задач разложения

см. Golub & van Loan[4]. 
* Algorithm 3.2.1 (Outer Product LU)
* Algorithm 3.2.2 (Gaxpy LU) 
* Algorithm 3.2.3 (Recursive Block LU) 
* Algorithm 3.2.4 (Nonrecursive Block LU)

* Algorithm 3.4.1 (Outer Product LU with Partial Pivoting)
* Algorithm 3.4.2 (Gaxpy LU with Partial Pivoting)
* Algorithm 3.4.3 (Outer Product LU with Complete Pivoting)

* Algorithm 4.1.1 (LDLᵀ) 
* Algorithm 4.2.1 (Gaxpy Cholesky) 
* Algorithm 4.2.2 (Outer Product LDLᵀ with Pivoting) 
* Algorithm 4.2.3 (Recursive Block Cholesky)
* Algorithm 4.2.4 (Nonrecursive Block Cholesky)

* Algorithm 4.3.4 (Hessenberg LU)
* Algorithm 4.3.5 (Band Cholesky)

* Algorithm 5.2.1 (Householder QR) 
* Algorithm 5.2.2 (Block Householder QR)
* Algorithm 5.2.3 (Recursive Block QR)
* Algorithm 5.2.4 (Givens QR) 
* Algorithm 5.2.5 (Hessenberg QR)
* Algorithm 5.2.6 (Modified  Gram-Schmidt)
* Algorithm 5.2.7 (Classical Gram-Schmidt)
* Algorithm 5.3.1 (Normal Equations) 
* Algorithm 5.3.2 (Householder LS Solution)
* Algorithm 5.4.1 (Householder QR With Column Pivoting)
* Algorithm 5.4.2 (Householder Bidiagonalization) 

* Algorithm 7.4.2 (Householder Reduction to Hessenberg Form)

Методы QR- разложения вводятся для квадратных матриц. Но в более общем виде применимы для разложения комплексных матриц $m×n$, где $m ≥ n$, как произведение кадратной $m×m$ унитарной матрицы Q и прямоугольной $m×n$ верхне-треугольной матрицы $R$, у которой нижние ($m−n$) строк содержат нули. Это свойство применяется для построения блочных методов разложения с разбиением (partition) R, или разбиениением обоих матриц R и Q на блоки:

```math
A=QR=Q{\begin{bmatrix}R_{1}\\
0
\end{bmatrix}}=
\begin{bmatrix}Q_{1}&Q_{2}
\end{bmatrix}
\begin{bmatrix}R_{1}\\
0\end{bmatrix} = Q_{1}R_{1}~,
```

where $R_1$ is an $n×n$ upper triangular matrix, $0$ is an $(m − n)×n$ zero matrix, $Q_1$ is $m×n$, $Q_2$ is $m×(m − n)$, and $Q_1$ and $Q_2$ both have orthogonal columns.

Методы разложения выражаются через базовоый набор операций линейной алгебры. Сначала мы рассмотрим базовые операции и их записи в языках программирования, потом вернемся к описанию алгоритмов разложения. Иерархия методово линейной алгебры включает базовые математические операции (level 1) над векторами, базовые операции над матрицами (level 2) и операции с матрицами (level 3). Методы блочные и рекурсивные, методы разложения такие как LU, QR относятся к "level 3".

## Базовые операции и обозначения

Базовые операции над элементами матриц:
сложение, вычитание, умножение.
Кроме того, используются поэлементное умножение `.*` и деление `./`.

Используются операции разбинения матрицы на подматрицы и выделение 
строк, подстрок, колонок из матрицы. Для этого используются нотация выделения диапазонов.

Приведем примеры выделения строк и стольбцов в алгоритмах


**Операция внешнего произведения**

Given a pair of matrices $\mathbf{A}$ of size 
$m\times p$ and $\mathbf{B}$ of size $p\times n$, consider the matrix product 
$\mathbf{C} =\mathbf{A} \mathbf{B}$ defined as usual as a matrix of size 
$m\times n$.

Now let $\mathbf{a}_{k}^{\text{col}}$ be the $k$-th column vector of $\mathbf{A}$ and 
let $\mathbf {b} _{k}^{\text{row}}$ be the $k$-th row vector of $\mathbf{B}$. 
Then $\mathbf{C}$ can be expressed as a sum of column-by-row outer products:
```math
\mathbf{C} =\mathbf{A} \mathbf {B} =\sum _{k=1}^{p}{A}_{ik}\,{B}_{kj}=
\begin{bmatrix}&&\\
\mathbf{a}_{1}^{\text{col}}&\cdots &\mathbf{a}_{p}^{\text{col}}\\&&\end{bmatrix}
\begin{bmatrix}&\mathbf {b} _{1}^{\text{row}}&\\&\vdots &\\&\mathbf {b} _{p}^{\text{row}}&\end{bmatrix}=
\sum_{k=1}^{p}\mathbf{a}_{k}^{\text{col}}\mathbf{b}_{k}^{\text{row}}
```
This expression has duality with the more common one as a matrix built with row-by-column inner product entries (or dot product): 
```math
C_{ij}= \langle \mathbf {a} _{i}^{\text{row}},~\mathbf {b} _{j}^{\text{col}} \rangle
```

Запись векторной операции *outer product*
$$(a\otimes b)_{i,j} = a_i\cdot b_j$$
```py
for i = 1:m :
   for j = 1:n :
      A[i,j] = A[i,j] + x[i]·y[j]
```
Эту же операцию можно записать следующим образом с выделением 
строки из матрицы, по столбцам
```python
for i = 1:m :
   A[i, :] = A[i, :] + x[i]·yᵀ
```
или по строкам
```python
for j = 1:n :
   A[:, j] = A[:, j] + y[j]·x
```
Задание диапазона `:` - означает по всему диапзону `1:m`. В математической записи нумерация ведется от единицы, включая `m`.
Задание диапазона `j:` - означает поддиапзон `j:m`. Такая нотация совместима с синтаксисом языка Pyton, что делает его весьма удобным для описания матричных операций.

Выделяются базовые абстрактные методы, которые хорошо изложены
см. Golub & van Loan[4]. 
* Algorithm 1.1.1 (Dot Product) $\alpha = x^{\mathsf{T}} y$
* Algorithm 1.1.2 (Saxpy) $y = y + \alpha x$
* Algorithm 1.1.3 (Row-Oriented Gaxpy) 
* Algorithm 1.1.4 (Column-Oriented Gaxpy)
* Algorithm 1.1.5 (Matrix Multiplication) $C = C + AB$
* Algorithm 1.1.6 (Dot Product Matrix Multiplication)
* Algorithm 1.1.7 (Saxpy Matrix Multiplication)
* Algorithm 1.1.8 (Outer Product Matrix Multiplication)

* Algorithm 1.2.1 (Triangular Matrix Multiplication) 
* Algorithm 1.2.2 (Band Storage Gaxpy)
* Algorithm 1.2.3 (Symmetric Storage Gaxpy)

Данные абстрактные методы синтезируются по шаблону.

Кроме того, рассмотрены блочные методы, 
которые хорошо согласуются с задачей распределения 
задания на вычислительные узлы. 

**Алгоритм 1.1 (Dot Product)** $c = x^{\mathsf{T}} y$, где $x, y \in \mathbb{R}^n$
```py
c = 0
for i=1:n :
   c = c + x[i]·y[i]
```

**Алгоритм 1.2 (Saxpy)**
```py
for i=1:n :
   y[i] = y[i] + a·x[i]
```

**Алгоритм 1.3 (Row-Oriented Gaxpy)**
```py
for i=1:m :
   for j=1:n :
      y[i] = y[i] + A[i,j]·x[j]
```

**Алгоритм 1.4 (Column-Oriented Gaxpy)**
```py
for j=1:n :
   for i=1:m :
      y[i] = y[i] + A[i,j]·x[j]
```

**Алгоритм (Outer Product Update)**
```py
for i=1:m :
   A[i,:] = A[i,:] + x[i]·yᵀ
```

**Алгоритм 1.5 (ijk Matrix Multiplication)**
```py
for i=1:m :
   for j=1:n :
      for k=1:r :
         C[i,j] = C[i,j] + A[i,k]·B[k, j]
```

**Алгоритм 1.6 (Dot Product Matrix Multiplication)**

$A \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{r \times n}$, $C \in \mathbb{R}^{m \times n}$
```py
for i=1:m :
   for j=1:n :
         C[i,j] = C[i,j] + A[i,:]·B[:, j]
```


**Алгоритм 1.8 (Outer Product Matrix Multiplication)**

$A \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{r \times n}$, $C \in \mathbb{R}^{m \times n}$
```py
for k=1:r :
   for j=1:n :
      for i=1:m :
         C[i,j] = C[i,j] + A[i,k]·B[k, j]
```
можно переписать

```py
for k=1:r :
   C = C + A[:,k]·B[k, :]
```

**Алгоритм 2.1 (Triangular Matrix Multiplication)** 

$A,B,C \in \mathbb{R}^{n \times n}$ - верхние треугольные матрицы $C = C+ AB$
```py
for i=1:n :
   for j=i:n :
      for k = i:j :
         C[i,j] = C[i,j] + A[i, k]·B[k, j]
```
Алгоритм можно переписать 
```py
for i=1:n :
   for j=i:n :
      C[i,j] = C[i,j] + A[i, i:j]·B[i:j, j]
```

Разберем пример математической записи алгоритма с выделением столбцов

**Алгоритм (Modified Gram-Schmidt)**
```py
for k = 1:n :
   R[k, k] = ‖A[1:m, k]‖**2
   Q[1:m, k] = A[1:m, k]/R[k, k]
   for j = k+1:n :
      R[k, j] = Q[1:m, k]ᵀ · A[1:m, j]
      A[1:m, j] = A[1:m, j] - Q[1:m, k]·R[k, j]
```
Математическая запись алгоритмов линейно алгебры требует расшифровки, 
что происходит:

1. Выделение строк, столбцов и подматриц
`BLAS (nrm2) (column(A,k))`
-- операция nrm2 - квадратичная норма вектора, синтезируется по шаблону и 
включает цикл суммирования по строкам для столбца с номером `k`.
2. `Q(1:m, k) = A(1:m, k)/R(k, k)` 
-- операция подразумевает цыкл по строкам `i=1,...,m`
3. `R(k, j) = Q(1:m, k)^T A(1:m, j)` 
-- операция  `R[k,j] = BLAS(dot)(column(Q,k), column(A,j))`
Метод `BLAS(имя)` синтезирует по шаблону соответствующий 
метод `имя` линейной алгебры для данного математического класса.

**Алгоритм (Вычисление коэффициентов матрицы вращения Гивенса)**
```py
def givens(a,b):
   if b==0:
      c=1; s=0
   else:
      if np.abs(b)>np.abs(a):
         tau=-a/b; s=1/np.sqrt(1+tau**2); c=s*tau
      else:
         tau=-b/a; c=1/np.sqrt(1+tau**2); s=c*tau
   return c,s
```

**Алгоритм (Умножение на матрицу поворота слева)**
```py
def givens_left(A,c,s):
   a0=np.array(A[0,:]); a1=np.array(A[1,:])
   A[0,:]=c*a0[:]-s*a1[:]
   A[1,:]=s*a0[:]+c*a1[:]
```

**Алгоритм (Умножение на матрицу поворота справа)**
```py
def givens_right(A,c,s):
   a0=np.array(A[:,0]); a1=np.array(A[:,1])
   A[:,0]=c*a0[:]-s*a1[:]
   A[:,1]=s*a0[:]+c*a1[:]
```

**Алгоритм (QR-разложение методом вращений Гивенса)**
```py
def QR_givens(A):
   m,n=A.shape
   for j in range(n):
      for i in range(m-1,j,-1):
         c,s=givens(A[i-1,j],A[i,j])
         givens_left(A[i-1:i+1,j:],c,s)
```

**Алгоритм (Вычисление вектора Хаусхолдера)**
```py
def house(x):
   n=len(x); mu=np.linalg.norm(x); v=np.copy(x)
   if mu:
      beta=x[0]+(1.0 if x[0] >= 0 else -1.0)*mu
      v[1:]/=beta
   v[0]=1.0
   return v
```

**Алгоритм (умножение на матрицу Хаусхолдера слева)** $PA = A + v w^{\mathsf{T}}$, где $w = \beta A^{\mathsf{T}} v$
```py
def house_left(A,v):
   beta = -2.0 / np.dot(v,v)
   w = beta * np.matmul(A.T,v)
   A+=np.outer(v,w)
```

**Алгоритм (умножение на матрицу Хаусхолдера справа)** $AP = A + w v^{\mathsf{T}}$, где $w = \beta A v$
```py
def house_right(A,v):
   beta = -2.0 / np.dot(v,v)
   w = beta * np.matmul(A,v)
   A+=np.outer(w,v)
```

**Алгоритм (QR-разложение с использованием отражений Хаусхолдера)**
```py
def QR_house(A):
   m,n=A.shape
   for j in range(n) :
      v=house(A[j:,j])
      house_left(A[j:,j:],v)
      if j+1<m :
         A[j+1:,j]=v[1:]
```

### Преобразование эрмитовой (симметричной) матрицы к трехдиагональной

Калиткин [3] утверждает, что вычислительная сложность метода дает $(4/3)n^3+50n^2$ операций для тридиагонализации эрмитовых матриц, а все собственные вектора находятся за $2n^3 +10n^2$ операций. Наличие степени $n^3$ дает оценку максимального числа операций на операциях с одинарной точностью, точность теряется на размерности порядка 100. Для большей размерности матриц надо применять числа `double` двойной точности, что позволяет считать матрицы размером в тысячи строк. 

## Диагонализация 

**Диагонализируемые матрицы**

* Инволюции (отражения) диагонализируемы над вещественными числами (и над любым полем, характеристика которого не равна 2).
* Вещественные симметричные матрицы диагонализируемы при помощи ортогональных матриц. 
* Матрицы диагонализируемы унитарными матрицами в том и только том случае, если они нормальны. 
Примерами нормальных матриц являются вещественные симметричные (или кососимметричные) матрицы и эрмитовы матрицы.

**Недиагонализируемые матрицы**

В общем случае матрица поворота не является диагонализируемой над вещественными числами, но все матрицы поворота диагонализируемы над полем комплексных чисел. Даже если матрица недиагонализируемая, её можно привести к "наилучшему возможному виду" и создать матрицу с теми же свойствами, содержащую собственные значения на главной диагонали и единицы или нули на диагонали выше (наддиагонали).


## Бидиагонализация

Произвольная матрица A может быть разложена на множители преобразованиями подобия в форму 

$$A = U B V^{\mathsf{T}}$$

где $U$ и $V$ — ортогональные матрицы, а $B$ — бидиагональная матрица размером $N \times N$ с ненулевыми элементами только на диагонали и наддиагонали. Размер $U$ равен $M \times N$, а размер $V$ равен $N \times N$.

Бидиагонализация — это одно из унитарных (ортогональных) разложений матрицы, при котором $U^{\ast} A V = B$, где $U$ и $V$ — унитарные (ортогональные) матрицы; $B$ — верхняя бидиагональная матрица. $A$ может быть прямоугольной.

Для плотных матриц левая и правая унитарные матрицы получаются с помощью серии отражений Хаусхолдера, которые поочерёдно применяются слева и справа. Это называется бидиагонализацией Голуба-Кахана. Для больших матриц они вычисляются итеративно с помощью метода Ланцоша, который называется методом Голуба-Кахана-Ланцоша (SVDL).

Бидиагонализация очень похожа по структуре на разложение по сингулярным значениям (SVD). Однако она вычисляется за конечное число операций, в то время как для нахождения сингулярных значений в SVD требуются итерационные схемы. Это связано с тем, что квадраты сингулярных значений являются корнями характеристических многочленов $A^{\ast} A$, где $A$ предполагается большой.

* [4] Golub & Van Loan, Matrix Computations (4rd ed.), 2013

## Балансировка

В процессе балансировки матрицы применяются преобразования подобия, чтобы строки и столбцы имели сопоставимые нормы. Это полезно, например, для уменьшения ошибок округления при решении задач на собственные значения. Балансировка матрицы A заключается в замене A на подобную матрицу

$$A' = D^{-1} A D$$

где $D$ — диагональная матрица.

## Не к месту будет сказано
*Данный раздел кажется уместен в теории функции, потому что устанавливает связь между линейной алгеброй и гильбертовым пространством (бесконечномерным, непрерывным, комплексным) ортогнальных нормированных функций с определенной нормой через скаляное произведение и определенным оператором сопряжения. Мы же сталкиваемся с предгильбертовым пространством n-мерным евклидовым пространством на множестве действительных и комплексных чисел с операцией эрмитова сопряжения.*

*Чтобы понять, какие разложения нас могут интересовать в практических задачах надо пройти примерно такой путь: от теории функции до матриц линейного оператора, которые подчиняются свойствам гильбертова пространства, матриц со структурой порожденной дифференциальными уравнениями и гамильтонианами (в гамильтоновой динамике и квантовой механике) ...*

Ба́нахово пространство — нормированное векторное пространство, полное по метрике, порождённой нормой. Основной объект изучения функционального анализа [1].

В частности, Евклидовы пространства $\mathbb{R}^{n}$ и $\mathbb{C}^{n}$, с евклидовой нормой, определяемой как $\Vert x\Vert =\sqrt {\sum |x_{i}|^{2}}$, являются банаховыми пространствами.

Ба́наховой алгеброй над комплексным или действительным полем называется ассоциативная алгебра, являющаяся при этом банаховым пространством. 

$C^{\ast}$-алгебра — банахова алгебра с _инволюцией_, удовлетворяющей свойствам сопряжённого оператора.

Согласно определению, данному Гельфандом и Наймарком
$C^{\ast}$-алгебра определяется, как банахова алгебра 
$A$ над полем комплексных чисел, для каждого элемента которой 
$x\in A$ определено отображение $x\mapsto x^{\ast}$ со следующими свойствами:

* инволютивность: $x^{\ast\ast}=(x^{\ast})^{\ast}=x$,
* согласованность со сложением: $(x+y)^{\ast}=x^{\ast}+y^{\ast}$,
* согласованность с умножением: $(xy)^{\ast}=y^{\ast}x^{\ast}$,
* для всякого $\lambda \in \mathbb{C}$ выполнено $(\lambda x)^{\ast}={\overline {\lambda }}x^{\ast}$,
* выполнено так называемое $C^{\ast}$-тождество: $\Vert x^{\ast}x\Vert =\Vert x\Vert \Vert x^{\ast}\Vert$

Впервые были рассмотрены для применения в квантовой механике в качестве алгебр физически наблюдаемых объектов. Это направление исследований началось с матричной квантовой механики Вернера Гейзенберга и в более математически развитой форме с работ Паскуаля Йордана около 1933 года.
В линейной алгебре отметился Вильгельм Йордан, методы обозначаются, как Жордана (чтобы не путать).

* Эрмитовые элементы *-алгебры образуют алгебру Йордана.
* Анти-эрмитовые элементы *-алгебры образуют алгебру Ли.

**Примеры:**
* Самым известным примером *-алгебры являются комплексные числа $\mathbb{C}$ с операцией сопряжения.
* Квадратные матрицы с комплексными элементами с операцией эрмитова сопряжения.
* Эрмитово сопряжение линейного оператора в гильбертовом пространстве.

### Разложение Шура

Разложение Шура для комплексной матрицы A - это ее представление в виде произведения трех матриц:
$$A = U T U^\ast,$$
где:\
$U$ - унитарная матрица;\
$U^\ast$- эрмитово-сопряженная матрица (заметим $U^{-1} = U^\ast$);\
$T$ - верхняя треугольная матрица с собственными значениями на диагонали $T_{i,i} = \lambda_{i}(A)$.

Для случая вещественных чисел разложение для квадратной матрицы $A$ имеет вид:
$$A = Q H Q^{-1} ,$$
где:\
$Q$ - ортогональная матрица, \
$H$ - матрица Хессенберга. 

*Теорема (Real Shur Decomposition)*. Если $A \in \mathbb{R}^{n\times n}$, то существует такая
ортогональная матрица $Q \in \mathbb{R}^{n \times n}$ что разложение
```math
Q^T A Q = 
\begin{bmatrix}
R_{11}&R_{12}&\ldots& R_{1m}\\
      &R_{22}&\ldots& R_{2m}\\
      &      &\ddots& \vdots\\
      &      &      & R_{mm}
\end{bmatrix}
```
дает блочную квази-треугльную матрицу, диагональные элементы которой $R_{ii}$ имеют 1x1 вещественные собственные значения или 2x2 комплексно-споряженные собственные значения.

### Разложение Хессенберга для вещественных матриц

Произвольная вещественная матрица $A$ может быть разложена с помощью ортогональных преобразований подобия в форму

$$A = Q H Q^{\mathsf{T}}$$

где $Q$ — ортогональная матрица, а $H$ — верхняя матрица Хессенберга, то есть матрица, в которой ниже главной под-диагонали стоят нули. Приведение к матрице Хессенберга — это первый шаг в разложении Шура для несимметричной задачи о собственных значениях, но оно применяется и в других областях. На главной диагонали матрицы $H$ будут располагаться элементы - собственные значения или 2x2 комплексные? корни - разъяснить. 


### Определитель матрицы
```math
	det(Aᵀ)= det(A) \quad \\
	det(A)=det(LU)=det(L)\cdot det(U)
```
Определитель, вычисленный $det(A)$ мера масштабного коэффициента линейного преобразования, 
описанного матрицей. Когда определитель ниже нуля, матрица сингулярна, и никакая инверсия не существует.

Определитель треугольной матрицы - произведение диагональных элементов. 

Некоторые матрицы почти сингулярны, и несмотря на то, что обратная матрица существует, 
вычисление восприимчиво к числовым ошибкам. $cond(A)$ функция вычисляет число обусловленности 
для инверсии, которая дает индикацию относительно точности результатов матричной инверсии. 
Число обусловленности лежит в диапазоне от 1 для численно устойчивой матрицы к $\infty$ для 
сингулярной матрицы.

### Дополнительный минор

Дополнительный минор $M¯_{i,j}$ квадратной матрицы A - это определитель, 
составленный из элементов матрицы A путем удаления строк и столбцов c номерами ${i,j}$

### След матрицы

След матрицы $tr(A)$ - это скалярное значение, которое
равно сумме всех диагональных элементов этой матрицы
След матрицы равен сумме ее собственных значений

### Ядро и образ линейного оператора

Пусть $A:X \mapsto Y$ — линейный оператор.
Ядром линейного оператора A называется множество  $\mathrm{Ker} A=\{x\in X ∣ Ax=0 \}$

### Собственные вектора и собственные значения матрицы

Собственным вектором x и соответствующим ему собственном значением 𝜆 
матрицы A называются величины, для которых следующее равенство имеет 
ненулевое решение:
$$A v = \lambda v$$

В общем случае определение собственных значений и собственных векторов 
требует решения матричного уравнения:
$$(A − \lambda I) v = 0 ,$$

Матрица комплексная может быть разложена на произведение 
$$A = U R U^\ast,$$
где:\
$R$ - Треугольной матрицы, содержащей собственные значения на главной диагонали\
$U$ - унитарная матрица $U^\ast = U^{-1}$\
$U^\ast$ - транспонированная комплексно-сопряженная матрица 

Если $A$ - нормальная, то $AA^\ast = A^\ast A$, то матрица R- диагональная содержит собственные значения

Если $A$ - нормальная Эрмитова матрица, то собственные значения - вещественные числа. 

Методы в Mathlab и GNU Octave:
- eig  - спектральное разложение;
- eigs - собственные значения и собственные вектора матрицы;
- svd  - сингулярное разложение;
- svds - сингулярные значения и сингулярные вектора матрицы;
- lu   - LU-разложение, LUP-разложение;
- chol - LL-разложение;
- ldl  - LDL-разложение;
- schur- разложение Шура;
- qr   - QR-разложение;

Методы Pyton Numpy:
- {дописать}

Отдельно рассмотрим матрицы трехдиагональные и методы решения таких задач

Трехдиагональные (ленточные) матрицы возникают из записи системы уравнений
в конечных разностях (по сетке), в частности при численном решении 
уравнений теплопроводности

<https://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BF%D1%80%D0%BE%D0%B3%D0%BE%D0%BD%D0%BA%D0%B8>

Метод прогонки для трех-диагональных матриц. 
Матрицы записываются через три вектора. 
```
A B C = F
[B0 C0 .  .  . ]   F0
[A1 B1 C1 .  . ]   F1
[.  A2 B2 C2 . ]y= F2
[.  .  A3 B3 C3]   F3
[.  .  .  A4 B4]   F4
```

Алгоритм:
1) a[0] := -C0/B0, b[0] := F0/B0
2) Для k = 1 ... n
   d  = (Ak*a[k-1]+Bk)
   a[k] := -Ck/d
   b[k] := (Fk-Ak*b[k-1])/d
3) y[n]: = (Fn-An*b[n])/(An*a[n] + Bn)
4) Для k = n-1 ... 0
   y[k]  = a[k]*y[k+1]+b[k]

По ходу разложения можно найти детерминант трехдиагональной матрицы
$$det(A) = \prod_{i=0}^{n}(A_i \cdot a[i]-B_i)$$
* Алгоритм надо дорабатывать см Калиткин

Метод покоординатного расщепления:
Решение двумерной задачи может быть представлено покоординатно.
На каждом шаге решается методом прогонки.

Построение обратной матрицы 2x2 или 3x3: A^{-1} = adj(A) / det(A)
adj(Ф) - присоединенная составленная из алгебраических дополнений для элементов транспонированной матрицы

<https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0>

## Абстрактная линейная алгебра 

Этот раздел для расширения понятия линейная алгебра. По шаблону мы можем сформулировать линейную алгебру для иных математических классов, для которых определены операции сложения, вычитания, умножения и деления (через обратный элемент). В некоторых методах требуется определить операцию сопряжения, при переходе к комплесным парам. Базовым классом может быть класс рациональных чисел, модульная арифметика, арифметика в конечном поле, булева алгебра. Это не притянуто, все эти классы используются в практических задачах. 

## Параллельные и рекурсивные алгоритмы

Надо выделить отдельный класс алгоритмов, которые позволяют разбить общую задачу на подзадачи, т.е. алгоритмы для которых вычисление выполняется над субматрицами, см. `ReLAPACK`. 

Хочется получить алгоритмы диагонализации, алгоримты разложений над блочными матрицами. 

Алгоритм умножения матрицы на вектор может быть параллельным с редуцированием по группе.

Алгоритм умножения матриц может быть рекурсивным, с разбиением на подматрицы. $С_{i,j} = \sum\limits_k A_{i,k} B_{k,j}$

Для каждого метода который допускает рекурсию можно сформулировать порог перехода на методы линейные. Так строится библиотека BLAS.

## Базовый набор методов для линейной алгебры

* [GNU Scientific Library](https://www.gnu.org/software/gsl/doc/html/linalg.html)

Рссмотрим набор методв для работы с векторами и матрицами. Прежде всего методы относятся к двум классам объектов: вектора и матрицы.
Структура каждого класса идентична и может быть представленна классом _тензор_ размерности 1 или 2. 

Тензор может строится на различных базовых классах, имеет признак типа данных. Основные типы - float, double, complex. 
Но с тем же успехом можно определить базовый класс другой размерности. Например, модульная арифметика, 
рациональные числа, двоичные числа c булевой алгеброй, вероятностная арифметика - нечеткая логика, арифметика Галуа. 

Для каждого из этих математических классов можно определить свою арифметику (ассоциативную алгебру с инволюцией) и следовательно можно применить линейную алгебру.
Также помимо комплексных чисел, могут быть гиперкомплексные числа, матрицы-"tiles". Многие методы могут быть синтезированы по шаблону. Многие методы могуть быть представлены рекурсивно через операции над подматрицами. 

Методы линейной алгебры представлены в множестве пакетов линейной аглебры, BLAS/LAPACK.

Существует принятая система обозначений методов BLAS 
`{1.базовый тип}{2.класс матриц}{3.операция}{4.суффиксы}`:
1. Базовый класс: `S` - single-precision, `D` -- double-precision, `C` - complex single-precision, `Z` - complex double-precision.
2. `GE` - general произвольные матрицы, `GB` - general band, `SY` - symmetric, `SB` - symmetric band, `SP` - symmetric packed,  `HE` - комплексные эрмитовы матрицы, `TR` - треугольные матрицы (верхние, нижние, LU, LDU)
Возможно определение дополнительных классов, таких как ленточная или трехдиагональная.
3. `MM` - matrix multiplication, `MV` - matrix-vector product, 
`ADD`,`SUB`,`MUL`,`TR` - транспонирование и/или сопряжение,
`DOT` - скаларяное произведение, `AXPY` - сумма векторов $\alpha x + y$, `LERP` - линейная интерполяция,
`SV` - решение уравнения $A^{-1}b$, `SM` - решение матричного уравнения $A^{-1}B$.

4. Суффикс определяет формат хранения аргумента на входе и выходе операции. Суффиксов может быть несколько, если форматы различаются.

Все операции распределены по уровням

1. Level 1 - векторныне операции такие как $y = \alpha x + \beta y$
2. Level 2 - операции с матрицами и векторами $y = \alpha A x + \beta y$
3. Level 3 - разложения и операции над  матрицами вида $C = \alpha A B + C$ 

Задание матриц на входе методов может сопровождаться признаками классов или дополнительных действий над матрицами: 
верхняя/нижняя треугльная, диагональная, унитреугольная, симметричная, транспонированная, обратная, 
эрмитово-сопряженная, комплексно-сопряженная.

Операции могут быть с отложенной операцией нормировки. Т.е. вместо того чтобы выполнять за два цикла 
"операция" и "нормировка" результата, сумма/минимум/максимум или сумма квадратов считается в том же цикле, что и операция,
а применяется на следующем уровне алгоритма или в составе следующей операции. 
Так можно эффективно снизить сложность операции. Такой подход позволяет считать фрагменты матрицы независимо. 
Для таких операций как суммирование и нахождение максимума мы вводим понятие "редуцирование по группе". 

Выделение тензора (строки, столбца, фрагмента из матрицы) без копирования. Все типы приводятся к типу тензор. 
Для каждого  тензора помимо размерности следует определить параметр -  
шаг для получения адреса следующей строки матрицы или следующего элемента вектора. Всего тензор будет характеризоваться 
набором признаков: тип, размерность, шаг и содержать ссылку на массив данных. 

Разбиение матрицы (split). Для применения рекурсивных методов вводится разбиение матрицы, разбиение- аппаратно-ориентированная фукнция, которая выполняет разбиение кратное плитке, tile - неделимому элементу разбиения. Так например, можно представить операцию транспонирования матрицы, через транспонирование фрагментов. Умножение матриц можно представить через умножение подматриц. 

Размерность, при которой происходит переход от ,блочного рекурсивного метода (метод третьего уровня) 
к линейному методу (метод второго уровня), будет своя для каждого рекурсивного метода. 

**AXPY**
* $y ← y + \alpha x $

**LERP**
* $y ← (1-\mu) \odot y + \mu \odot x$

**DOT**
* $r ← x^{\mathsf{T}} y$
* $r ← x^{\mathsf{H}} y$
* $r ← \alpha + x^T y$

**ASUM**
* $r ← \sum |x|$

**NRM2**
* $r ← \sqrt{x^{\mathsf{T}} x}$

**GER**, **HER**
* $A ← A + \alpha x y^{\mathsf{T}}$
* $A ← A + \alpha x y^{\mathsf{H}}$

**GEMV**, **SYMV**, **HEMV** (level 2)
* $y ←  β y+α A x$
* $y ←  β y+α A^{\mathsf{T}} x$
* $y ←  β y+α A^{\mathsf{H}} x$

**GEMM** (level 3)
* $С ←  β С+α A B$
* $С ←  β С+α A^{\mathsf{T}} B$
* $С ←  β С+α A B^{\mathsf{T}}$

где $A^{\mathsf{T}}$ - транспонированная матрица, $A^{\mathsf{H}}$ - эрмитово сопряженая матрица.

В нейросетях LLM присуствуют операторы `FFN`, `ATTN`, `WKV`.

A neural network is a function $Φ : \mathbb{R}^n \mapsto \mathbb{R}^m$, where $n$ and $m$ is the number of inputs and outputs to the network. 
It is usually expressed through a repeated composition of affine (linear + constant) functions $Λ : \mathbb{R}^n → \mathbb{R}^m$ of the form
$y = Wx + b$ and non-linear functions $f : \mathbb{R}^m → \mathbb{R}^m$ of the form $z = f(y)$.

В операторе `FFN` применяется функция $\max(\cdot)$, в операторе `ATTN` применяется функция $softmax(\cdot)$.

Дифференциальные операторы выражаются через функцию `BLAS(LERP)`.

В реализации тензорных операций присутсвует выбор метода на основе признаков размерности и типа тензора. 
Элемент логики программы для выбора подходящего метода назовем _type traitis_. Операция побора метода может давать 
составную конструкцию, которая включает преобразование формата, или операция может быть разложена на несколько стадий. 
Так например операция транспонирования и сопряжения может выполняться в составе метода умножения или может быть представлена отдельным методом. 
