Математика для машинного обучения
=================================


* [PREV: математические классы](MATHAN.md)
* [PREV: сведения из теории вероятностей](PROB.md)
* [PREV: Цифровая обработка сигналов](DSP.md)
* [PREV: Квантование и дискретная математика](QUANT.md)
* [Квантизация моделей: рациональные и действительные числа](#рациональные-числа-и-действительные-числа)
* [Типы нейронных сетей](#типы-нейронных-сетей)

## Квантизация

*{Ввести оператор квантования. Оператор повышения и понижения разрядности.}*

Под квантизацией понимают представление чисел в формате чисел с плавающей точкой меньшей разрядности мантиссы или в целых числах. 

## Рациональные числа и действительные числа

*{Этот раздел чтобы дать понять, что архитектуру надо проектировать с учетом ошибок квантования, выбор операций и форматов данных основан на понимании, как учитывать и компенсировать ошибки. Оптимизация вычислений может быть выполнена на базе рациональных чисел и двоичных дробей, чисел с фиксированной точкой. Вычисления в рациональных дробях не создают ошибку квантования. Учет и компенсация ошибок округления очень важны, когда речь идет о точности математической модели. }*

Множество действительных чисел мы будем рассматривать, как композицию из двоичной экспоненты и рационального числа. Над рациональными числами в таком виде. 
- $A/B$, где $A,B\in [0,2^L)$ - множество рациональных чисел (дроби)
- $A/2^M$, где $A\in [0,2^L)$ - множество действительных чисел (с фиксированной точкой, двоичная дробь);
- $\pm A/2^M \cdot 2^n$, где $A\in [0,2^M)$ - множество действительных чисел (с плавающей точкой, ненормальные);
- $\pm A/2^M \cdot 2^n$, где $A\in [2^M,2^{M+1})$ - нормализованные действительные числа.

Заметим, нормализованные числа не содержат ноль (0). Кодирование нуля, и бесконечности происходит специальными константами. 
Над константами $\{-\infty, 0,+\infty\}$ можно вводить специальную арифметику: $\infty+x = \infty$; $\infty*x = \infty$; $x/\infty = 0$; и т.д.

Для множества вещественных чисел мы можем определить подмножества и операции над подмножествами.
Подмножество будет выделяться разрядностью констант (M, n). В таблице приведены константы определенные для вещественных чисел в соответствии со стандартом Си и [IEEE Std. 754-2019]. 
| тип | мантисса | $E_{min}$ | $E_{max}$ | $E_{bias}$
|:----|----:|----:|---:|---:
| E4M3     |  3 | -6   | +7  | 7
| E5M2     |  2 | -14  | +15 | 15
| binary16 | 16 | -14  | +15 | 15
| binary32 | 24 | -127 | +126
| binary32x| 32 | 
| binary64 | 53 | 
| binary64x| 64 | 
| binary128|113 | 

Внешнее представление чисел с плавающей точкой
```mermaid
packet-beta 
	0: "sign"
	1-4: "exponent (4 bits)"
	5-7: "frac (3 bits)"
```
```mermaid
packet-beta 
	0: "sign"
	1-5: "exponent (5 bits)"
	6-7: "frac (2)"
```
```mermaid
packet-beta 
	0: "sign"
	1-5: "exponent (5 bits)"
	6-15: "fraction (10 bits)"
```
```mermaid
packet-beta 
	0: "sign"
	1-8: "exponent (8 bits)"
	9-15: "fraction (7 bits)"
```
```mermaid
packet-beta 
	0: "sign"
	1-8: "exponent (8 bits)"
	9-18: "fraction (10 bits)"
```
```mermaid
packet-beta 
title Рис.1n Float8(E4M3), Float8 (E5M2), Float16 (E5M10), BFloat16 (E8M7), TF-32(E8M10), Float32 (E8M23) - форматы чисел с плавающей точкой.
	0: "sign"
	1-8: "exponent (8 bits)"
	9-31: "fraction (23 bits)"
```

В настоящее время разрабатываются форматы группы binary8, с различной разрядностью мантиссы (0..7). [IEEE SA Working Group P3109]. В системе команд Intel AVX10.2 присутствуют форматы binary8 (E5M2) and (E4M3).


Операции с вещественными числами **FloatN** создают ошибку. Мы будем рассматривать ошибку, которая возникает в процессе нормализации точно так же, как рассматривали ошибку при квантовании в целых числах. Учет ошибок это единственная возможность вытянуть слабый сигнал. Ошибку мы рассматриваем на примере операций сложения и умножения. Упаковку формата вещественных чисел (с потерей точности) для внешнего представления можно кодировать в форме (S,E,M) - разрядность экспоненты (E), мантиссы (M) и наличие знака (S). 

```mermaid
packet-beta 
	0: "sign"
	1: "exp"
    2: "frac"
```

```mermaid
packet-beta 
title Рис.1m Float3 - минимальный формат (E1M1) и (E2M0)
	0: "sign"
	1-2: "exp"
```
Минимальный формат позволяет выполнить кодирование в числах $\{-1,0,+1\}$. Специальный код используется для кодирования $\pm\infty$ (s_11) и $\pm{0}$
(s_00)

```mermaid
packet-beta 
title Рис.1m Float4 (FP4) - минимальный формат (E2M1)
	0: "sign"
	1-2: "exp"
    3: "frac"
```
Минимальный формат FP4 (E2M1) используется для сжатия моделей LLM. 


Converts brain16 to float32.
```
 bfloat16 floating point format:
   ┌sign
   │   ┌exponent
   │   │      ┌mantissa
   │┌──┴───┐┌─┴───┐
 0b0000000000000000 brain16
 ```
 
Since bf16 has the same number of exponent bits as a 32bit float,
encoding and decoding numbers becomes relatively straightforward.
 
 ```
   ┌sign
   │   ┌exponent
   │   │      ┌mantissa
   │┌──┴───┐┌─┴───────────────────┐
 0b00000000000000000000000000000000 IEEE binary32
```
For comparison, the standard fp16 format has fewer exponent bits.
```
   ┌sign
   │  ┌exponent
   │  │    ┌mantissa
   │┌─┴─┐┌─┴──────┐
 0b0000000000000000 IEEE binary16
```

*Ошибка сложения*

$$R = A\cdot 2^{e_1} + B\cdot 2^{e_2} = (A + B\cdot 2^{(e_2 - e_1)}) \cdot 2^{e_1}$$

Если $|e_2 - e_1|\geq 2^M$ - возникает потеря точности, иначе можно сформировать остаток после нормализации числа. Точное представление чисел возможно в рациональных дробях или числах с фиксированной точкой. Если бы мы говорили про проектирование новой архитектуры для нейронных сетей, следовало бы рассматривать реализацию операций с накоплением и компенсацией ошибки округления. 

*Умножение с накоплением*

Следовало бы говорить об операции сложения и умножения "diffused", где операция сложения и умножения выполняется на аккумуляторе с большей разрядностью и определенными правилами округления остатка при нормализации. Чтобы правильно выполнять операцию свертки и скалярного произведения, суммы ряда, необходимо сохранять в аккумуляторе ошибку округления и применять на следующей операции. В современных процессорах представлены операции "fused multiply-accumulate", (сокр. FMA, совмещенные операции умножения с накоплением) и подобные им векторные операции с горизонтальным суммированием результата. Подобные операции (dot, mix, fma)  как раз ориентированы на применение в свёрточных нейронных сетях.

Тенденция упаковки коэффициентов и векторов моделей сохранится. Модели должны масштабироваться в плане разрядности арифметики и давать возможность получать сравнимые по точности результаты за счет правильного учета остатков при квантизации. В отношении моделей также можно применять термин квантование модели, подразумевая разрядность коэффициентов для внешнего представления данных. 

*Ошибка умножения*
Умножение увеличивает разрядность операций. Так если мы умножаем число Float32 с разрядностью M=23 бита, то результат будет $2^M+1$ бит. 

*Эмуляция операции умножения действительных чисел*. Числа при умножении представляем в виде дроби:

$$\left(1+\frac{A}{2^M}\right) \cdot \left(1+\frac{B}{2^M}\right) \cdot 2^{e_1+e_2} = 
\left(2^M + (A+B) + rn\left[\frac{AB}{2^{M}}\right]\right) \cdot 2^{e_1+e_2-M}$$
где rn - функция округления. $rint(x) = floor(x+0.5)$

В остаток уходит величина r, полагая округление методом floor, в меньшую сторону.

$$Q = \left\lfloor\frac{AB}{2^M}\right\rfloor,\quad R = \frac{AB}{2^M} - Q$$

Например, если операция в целых числах использует 32 битные числа, то результат умножения с накоплением будет помещаться в число удвоенной разрядности. Если заранее известно, что сумма коэффициентов дает 2^M то результат свертки всегда будет помещаться в разрядность $2^M$. Но при этом вычисления должны производится на числах удвоенной разрядности с понижением разрядности результата. 

При вычислениях нормированных функций мы работает в диапазоне значений [0,1]. При такой постановке выгоднее с точки зрения производительности переводить числа в правильные двоичные дроби. Это может быть не так просто с точки зрения реализации алгоритма. 

*Правильные дроби*. Когда мы работаем с вероятностями мы используем подмножество чисел с плавающей точкой - вероятности положительно определенные функции на интервале [0,1]. Сумма вероятностей должна давать единицу. В этом смысле мы могли бы вероятности представлять в виде дробей $A_k/B$, где $A_k \in [0,B]$, $\sum\limits_k {A_k}=B$

Предлагаю в качестве упражения реализовать класс полиномы с коэффициентами $A_k \in [0,2^N)$, которые в сумме дают $2^N$. Над полиномами определена операция умножения. Кроме того, нам понадобится функция поиска обратного числа для любого элемента множества, такого что $(A\cdot\bar{A})/2^N=1$

## Алгоритмы повышения разрядности

Допустим на аппаратной платформе нам доступна только операция одинарной точности. Для замещения операции с двойной точностью можно использовать два числа с одинарной точностью таким образом чтобы результат операции $a+b = s + r$, давал сумму и остаток (s, r). 

В алгоритмах приведенных ниже символы $\oplus, \ominus, \otimes$ означают операции с округлением, одинарной точности.

Алгоритм сложения, Dekker [10]
```
Add12 (a,b):
	s = a ⊕ b
	v = s ⊖ a
	r = b ⊖ v
	return(s,r)
```
Алгоритм сложения, Д.Кнут [9]
```
Add12 (a,b):
	s = a ⊕ b
	v = s ⊖ a
	r =(a ⊖(s ⊖ v)) ⊕ (b ⊖ v)
	return(s,r)
```
* [9] D. Knuth. The Art of Computer Programming, volume 2, ”Seminumerical Algorithms”. Addison Wesley, Reading, MA, third edition edition, 1998.
* [10] T.J. Dekker. A floating point technique for extending the available precision. Numerische
Mathematik, 18(3):224–242, 1971.
* [11] Dominik Goddeke, Robert Strzodka, and Stefan Turek. Accelerating double precision fem simulations with GPUs. In Proceedings of ASIM 2005 - 18th Symposium on Simulation Technique,
September 2005.

```
Add22 (ah,al) , (bh,bl)
	r = ah ⊕ bh
	if |ah| ≥ |bh| then
		s = (((ah ⊖ r) ⊕ bh) ⊕ bl) ⊕ al
	else
		s = (((bh ⊖ r) ⊕ ah) ⊕ al) ⊕ bl
	(rh,rl) = add12(r,s)
	return(rh,rl)
```
Алгоритм разбиения числа на два меньшей разрядности Dekker [10]
```
SPLIT (a):
	c = (2^s ⊕ 1) ⊗ a
	a_big= c ⊖ a
	a_hi = c ⊖ a_big
	a_lo = a ⊖ a_hi;
	return (a_hi, a_lo)
```

Алгоритм умножения с остатком (Dekker [10])
```
Mul12 (a, b):
	x = a ⊗ b;
	(a_hi, a_lo) = SPLIT(a);
	(b_hi, b_lo) = SPLIT(b);
	err =   x ⊖ (a_hi ⊗ b_hi)
	err = err ⊖ (a_lo ⊗ b_hi)
	err = err ⊖ (a_hi ⊗ b_lo)
	y = (a_lo ⊗ b_lo) ⊖ err
	return (x,y)
```
Если на платформе аппаратно поддерживается операция fused-multiply-accumulate (FMA), то умножение может быть выполнено быстрее
```
FMA_mul12 (a,b):
	x = a ⊗ b
	y = FMA(a × b − x)
	return (x,y)
```

## Использование квантизации и обозначение квантизации моделей

Это не окончательный вариант, но он полезен при чтении исходного кода или вывода в консоль, чтобы понять, что обычно означает каждое из этих обозначений.

`<Encoding>_<Variants>`\
`<Encoding>` : Это определяет наиболее распространенную кодировку индивидуальных весов в модели
Форматы с плавающей запятой:
* `BF16`:  16-битная Bfloat16 Google Brain усечённая форма 32-битного IEEE 754 (1 знаковый бит, 8 бит показателя степени, 7 дробных бит)
* `F64`: 64-битные числа с плавающей запятой IEEE 754 (1 знаковый бит, 11 бит экспоненты, 52 дробных бита)
* `F32`: 32-битные числа с плавающей запятой IEEE 754 (1 знаковый бит, 8 бит показателя степени, 23 дробных бита)
* `F16`: 16-битные числа с плавающей запятой IEEE 754 (1 знаковый бит, 5 бит порядка, 10 дробных бит)
Целочисленные форматы:
* `I<X>`: X бит на единицу веса, где X может быть 4 (для 4 бит) или 8 (для 8 бит) и т. д...
Квантованные форматы:
* `Q<X>`: X бит на единицу веса, где X может быть 4 (для 4 бит) или 8 (для 8 бит) и т. д...
* `KQ<X>` (или `Q<X>_K`) : модели на основе k-квантов. `X` бит на весовой коэффициент, 
где `X` может быть `4` (для 4-бит) или `8` (для 8-бит) и т. д...
* `IQ<X>`: модели на основе `i`-квантов. `X` бит на вес, где `X` может быть `4` (для 4-бит) или `8` (для 8-бит) и т. д...

`<Variants>`Это различные стратегии упаковки квантованных весов в файл формата [gguf](/ggerganov/gguf). 

Следует отметить некоторую особенность использования форматов данных при запуске и исполнении моделей. Внутреннее представление модели может быть различным. Модели в высоком разрешении исполняются и сохраняются в памяти в формате Float32.Для повышения производительности на GPU внутреннее представление коэффициентов весовых матриц и векторов может быть представлено в формате Float16 или BFloat16. Внутреннее представление коэффициентов завязано на конкретные операции, такие как активация и нормализация слоя. Результатом активации может быть вектор Float32. А вот матрица коэффициентов может храниться и применяться без изменения формата данных, т.е. операция над вектором состояния может быть представлена как произведение коэффициентов матрицы в формате `Q8_1` на состояние в формате `Q8_1` или `F16`. Внутренне представление коэффициентов Может быть представлено в целых или рациональных числах. Так формат `Q8_1` представляет собой набор коэффициентов в формате `int8_t` и отдельно рассчитанный нормировочный коэффициент в формате `float`. Альтернативный способ хранения может содержать нормировочный коэффициент и минимальное (среднее) значение. Исполнение моделей на CPU и используемый при этом формат представления тензоров зависит от поддерживаемой системы команд процессора, формат выбирается исходя из наличия в системе команд тех или иных векторных инструкций. 

На платформе `x86` относительно высокая производительность модели на GPU может быть получена в системе команд с разрядностью векторных регистров 512 бит:
* `AVX512_FP16` -- поддержка операций Float16, и операций скалярного произведения векторов.
* `AVX512_BF16` -- поддержка операций BFloat16.
* `AVX512_IFMA` -- поддержка операций Fused-Multiply-Accumulate в целых числах
* `AVX512_VNNI` -- поддержка инструкций малой разрядности для нейронных сетей
* `AVX512F` -- поддержка инструкций FMA для чисел Float32.

Эти же операции могут быть представлены в системах команд AVX10 для векторов 256 бит. В настоящее время операции `AVX512_FP16` присутствуют только на серверных платформах. Без поддержки набора инструкций вычисление float16 на CPU крайне не эффективно. По этой причине предпочтительными остаются методы расчета в целых числах, формат `Q8_1` и представление результатов в формате Float32. 

Использование коэффициентов `BF16` в чем-то близко по качеству к `Q8_0` и `Q8_1`. Качество моделей `F16` заведомо выше, поскольку в формате `F16` разрядность мантиссы выше, чем разрядность `Q8` и `BF16`. Представление в рациональных числах может дать заметный выигрыш и по качеству и по производительности операций, в случае если аппаратно поддерживается операция скалярного произведения векторов _dot-product_ и сложение с накоплением _fma_, без потери точности при сложении.

На GPU представление коэффициентов может быть `Q8_1` или `F16`, варианты представления включают общую нормировку по вектору. 
Дело в том что расчет  функций типа Softmax сопровождается вычислением нормы. А нормализация слоя предусматривает два цикла - расчет суммы и нормировка каждого элемента. Таким образом норма вектора может передаваться на следующий слой, чтобы исключить операцию двойной нормировки элементов вектора. Такая техника существенно ускоряет работу модели.


## Представление вероятностной логики

Допустим у нас имеется полный набор нормированных линейно-независимых функций определенных на интервале [0,1] и возвращающих значения [0,1]. Тогда можно образовать Гильбертово или _вероятностное пространство_ с разложением по базису логических функций. Вероятностная логика рождается из определения независимых событий. 

При вводе понятия вероятности переход от абстрактного множества выполняется через сопоставление операциям над вероятностями операций над

Множествам с числовой мерой множества - вероятностью можно сопоставить логические функции.
```math
P(A\cap B)  = P(A)P(B)
P(A\cup B)  = P(A)+P(B) - P(A)P(B)
```
Такие операции "вырождаются" т.е. если на входе логической функции используются значения 0 и 1, то функции на выходе принимают значения только 0 и 1. Для записи 

## Вырождение вероятностной логики в четкую логику

Есть идея распознавать четкую логику. Для этого нужно научиться детектировать вырождение слоя.

Утверждение следующее: если  система уравнений построенная на вероятностной логике имеет вырожденные матрицы на входе то на выходе будет тоже вырожденная матрица, тогда всю структуру можно заменить на четкую логику. Для применения булевой алгебры нужны типы квантизации `Q0` и `Q1`. `Q0` - все коэффициенты в тензоре единицы, весь тензор заменяется на скаляр. `Q1` - целочисленный тензор заменяется на общий скаляр и каждый коэффициент кодируется одним битом {0,1}. Еще бывает `Q1_1` когда каждый элемент кодируется `+1` или `-1` и есть вариант `Q2`, когда может быть использована троичная логика `+1, 0, -1`. 

Есть варианты вырождения сети в дифференциальное уравнение. Это, например, когда в выражении 
$h_t = (1-\mu)\cdot h_{t-1}+ \mu \hat{h}$ вектор $\mu$ можно закодировать как `Q1` - тогда это выражение можно интерпретировать, как  систему дифференциальных уравнений и выражение может быть представлено в терминах DSP - цифровой фильтр с бесконечной импульсной характеристикой. 

Над графом состоящим из операторов можно выполнять алгебраические операции: менять порядок исполнения в соответствии с правилом коммутативности и раскладывать структуру графа из последовательного в параллельный в соответствии с правилами дистрибутивными. 

Исходя из этих принципов можно генерировать и анализировать структуру графа вычислительной сети. 

## Триггеры

Проектирование логических схем основано на некоторых базовых принципах, и прежде всего - логика должна быть синхронной. Всегда есть такой элемент, который отделяет прошлое от будущего. В цифровой схемотехнике - это D-триггер. 
