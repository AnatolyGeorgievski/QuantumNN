**Архитектура проекта**
Я занимаюсь адаптацией открытого проекта LLaMa.cpp -- это динамично развивающийся проект дающий возможность запуска моделей нейросетей на локальном сервере и на персональном компьютере. Проект LLaMa.cpp позволяет выполнить тестирование моделей. Прикладные продукты могут создаваться на базе библиотеки GGML на которой базируется LLaMa. В состав GGML входит множество бэкендов для различного аппаратного обеспечения, включая CUDS, SyCL, HIP, OpenCL, Vulkan, Metal и другие.

*Моя разработка - бэкенд OpenCL, под оборудование Intel, ARM и Qualcomm. Бэкенд OpenCL ориентирован на мобильные и встроенные приложения. Моя компетенция позволяет разобраться со структурой модели и выполнить оптимизацию методов загрузки на уровне математики и тензорных вычислений. Есть собственные разработки по квантизации моделей.* 

Помимо текстовых моделей, в проекте LLaMa.cpp поддерживаются модели с визуальным и языковым (мультимодальным) вводом: Qwen/Qwen2-VL, Google/Gemma-3, IBM/granite и другие. Проект развивается динамично и поддержка новых моделей опубликованных разработчиками, таких как LLaMa, Qwen, DeeepSeek, Grok появляются буквально на следующий день.

Для обработки визуальных данных перспективной считается модель Qwen2.5-VL. Хорошие показатели имеют модели Gemma-3, но работают с отдельными кадрами. Модели granite ориентированы на работу с текстовыми сканированными изображениям. 

Каждая модель имеет свой проектор, свой способ подготовки и загрузки изображений и требует свой загрузчик. Универсальным является только текстовый запуск, но при этом каждая модель требует своей логики формирования потока тензоров. Способ обработки изображений включает нарезку изображения на сегменты и параллельную обработку сегментов изображения с использованием методологии multiple rotary position, разработанной специально для модели Qwen-VL. В модели Qwen-VL может применяться нарезка из последовательности кадров по времени с периодичностью 40мс. Нарезка может состоять из нескольких кадров. В техническом описании модели заявлена поддержка до 16 кадров в последовательности, но данная возможность еще не реализована в проекте LLaMa.cpp.

В модели Gemma3 используется нарезка SigLIP - нейросеть для ввода изображения. 

В модели Gemma3 изображения на вход модели доставляются в определенном разрешении, для этого при загрузке выполняется ~~билинейная~~ бикубическая интерполяция кадра. Обработка кадров в тестовом приложении выполняется на CPU в однопотоковом режиме, операция не оптимизирована, обработка кадра занимает 0.2сек. Эту операцию можно выполнять в десятки раз быстрее, ели выгрузить на GPU -- требуется разработка метода.

Изображения могут подгружаться в форматах JPEG, BMP, PNG, GIF. RGB, возможно YCbCr-to-RGB conversion. Процесс декодирования может включать выделение изображений из тела сообщения и декодирование BASE64. 

Загрузка модели занимает существенное время, около 30 сек. Чтобы выполнять обработку на потоке модель должна быть загружена и находится в режиме ожидания ввода. 

Выходной формат может быть с разметкой json или markdown, содержать теги и метки времени. Требуется валидация выходного формата, некоторые ответы модели не проходят валидацию. 

Данное описание позволяет сформулировать облик системы.
Это может быть сервис с протоколом HTTP REST или CoAP принимающий на вход запросы, содержащие изображения, тексты, задания и ссылки. В ответе может быть текстовое описание и теги к изображению. Надо понимать, что в проекте есть примеры использования моделей, включая чаты и утилиты командной строки, но прикладное применение требует разработки. 

Для анализа потокового видео, сервис должен работать на потоке и выделять последовательности опорных кадров длиной до 8 секунд, декодировать формат видео и аудио с обработкой запросов в реальном времени. Архитектурно можно разделить на две службы: одна выполняет задачу декодирования и архивации и присваивает метку времени в нарезке, вторая выполняет классификацию визуальной информации в нарезке. 

Мы протестировали возможность генерации тегов по отдельным опорным кадрам с использованием сети gemma-3-27b-it. Системные требования 2xGPU с производительностью не менее RTX 3090 с суммарным размером видео памяти 32-48GB. Практические любые два GPU, кроме младших моделей можно использовать для этой задачи. Скорость генерации в большей степени зависит от объема памяти, памяти должно быть достаточно для загрузки модели и окна контекста. Окно контекста исчисляется гигабайтами. Лучше иметь запас 1,5-2 раза по размеру видео-памяти, по отношению к размеру модели. Если модель не помещается в память одного GPU, модель разбивается по слоям на несколько ускорителей. Хорошую производительность могут показать ускорители nVidia RTX 4090 и AMD RX 7900. 

Для дообучения (fine tunning) следует выбирать топовые ускорители, поскольку в процессе используются матричные операции и числа пониженной разрядности, то что не представлено в более ранних поколениях GPU. Дообучение выполняется арендованных мощностях. Дообучение выполняется однократно, если возникает необходимость. Множество моделей и датасетов публикуется в открытом доступе. Новые модели могут использовать технологии LoRA и MoE, которые дают возможность сравнительно быстрой адаптации существующих моделей. Технология LoRA использует замороженную базовую модель и считает дополнение с пониженной размерностью тензоров к базовой модели. Технология MoE (множество экспертов) позволяет добавлять несколько специализаций в модель и выполнять ассоциативное переключение между специализациями. Быстрое обучение достигается за счет использования методов понижения разрядности тензорных операций над числами с плавающей точкой (квантизацией). Так например, компания DeepSeek добилась высоких показателей по скорости обучения благодаря квантованию модели в процессе обучения. Другим фактором является собственная инфраструктура, включающая разложение модели между вычислительными узлами и файловая система для кластеризации вычислений.



## Сборка проекта LLaMa


```sh
$ git clone https://github.com/ggerganov/llama.cpp.git
$ cd llama.cpp
$ mkdir build && cmake ..
$ cmake --build . --config Release -j 16
```
Для запуска на выделенном сервере может использоваться backend RPC, для бэкенда CUDA необходимо установить драйвера и компиляторы nVidia. В конфигурации проекта `ggml/CMakeList.txt` следует выбрать опции `CUDA` и `RPC`.

* <https://developer.nvidia.com/cuda-downloads?target_os=Linux>

```sh
$ export CUDACXX=/usr/local/cuda-12/bin/nvcc
```

Тестовый стенд состоит из двух GPU RTX 3090 24GB и серверной платформы на двух CPU Intel Zeon E5-2673 с объемом памяти 256GB. 



## Установка Intel GPU на Debian 12

* <https://dgpu-docs.intel.com/driver/client/overview.html>

```sh
# Install the Intel graphics GPG public key
wget -qO - https://repositories.intel.com/gpu/intel-graphics.key | \
  sudo gpg --yes --dearmor --output /usr/share/keyrings/intel-graphics.gpg

# Configure the repositories.intel.com package repository
echo "deb [arch=amd64,i386 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu jammy unified" | \
  sudo tee /etc/apt/sources.list.d/intel-gpu-jammy.list

# Update the package repository metadata
sudo apt update

# Install the compute-related packages
apt-get install -y libze-intel-gpu1 libze1 intel-opencl-icd clinfo
```

## Запуск моделей с подгрузкой изображений

Пример:
```sh
$ ./bin/llama-gemma3-cli --mmproj models/mmproj-model-f16.gguf  -m models/gemma-3-27b-it-Q8_0.gguf -t 80  -ngl 99 -fa \
-p "классификация в json: наличие рекламы, новостные ленты, содержание текстов, название канала, персоны" \
--image dvr_thumbnail.jpg
```

## Замечания по бэкенду RPC

Этот раздел технический, описывает мою работу над проектом. Задача состоит в создании инфраструктуры сети с множеством вычислительных узлов, чтобы выполнять задачи обучения и развертывания больших языковых моделей. Данная работа является основной для проектирования IT инфраструктуры некоторой виртуальной организации ориентированной на работу с документами или работу с данными. Для аппаратного обеспечения используются серверное оборудование низко-бюджетное доступное на вторичном рынке. Такой подход кажется оправданным в условиях быстрой ротации оборудования для обучения нейросетей и непрерывно возрастающих требований к аппаратному обеспечению. 

## Тестирование пропускной способности локальной сети 10G

Для сетевой инфраструктуры могут быть выбраны адаптеры NIC Mellanox ConnectX-4, более ранние модели уже не поддерживаются по драйверам, или сетевые карты Intel X520-DA2. Для теста были выбраны карты Intel X520-DA2 и кабели DAC 10G, как самое бюджетное решение. 

1) сторона сервера
```sh
$ iperf -s -p 80
------------------------------------------------------------
Server listening on TCP port 80
TCP window size:  128 KByte (default)
------------------------------------------------------------
```
2) сторона клиента

```sh
$ iperf -c 192.168.1.125 -p 80 -t 30
```

Тестирование показывает на сетевых адаптерах Intel уровень 
* `9.5GB/s`, если сервер под управлением Linux. 
* `5-6GB/s`, если сервер под управлением Windows.

При запуске можно указать, какой из установленных GPU используется RPC сервером:
```sh
$ CUDA_VISIBLE_DEVICES=0 ./build/bin/rpc-server -c -H 192.168.1.125 -p 50051
$ CUDA_VISIBLE_DEVICES=1 ./build/bin/rpc-server -c -H 192.168.1.125 -p 50052
```

Запуск модели с использованием RPC
```sh
$ ./bin/llama-cli $GEN_OPTIONS \
 --threads $N_THREADS \
 --n-gpu-layers $N_GPU_LAYERS \
 -m "$MODEL"
 --rpc 192.168.1.125:50052,192.168.1.125:50051
```

RPC - это бэкенд, который работает с тем же backend API, что и другие модули. Все что делает RPC помимо сериализации потока тензоров - работа с хешами тензоров. RPC - это протокол передачи данных и backed API. Возможна инкапсуляция одного протокола в другой, но это не кажется эффективным решением. Основные команды протокола: GET_TENSOR, SET_TENSOR и COMPUTE_GRAPH. Все это сопровождается возможностью не пересылать тензоры по сети, если хеши совпадают.

Требуется доработка протокола RPC. Набор требований
1. Изменение формата кадра, чтобы идентификатор сессии, идентификатор команды и размер данных пересылались в одном запросе.
2. Подтверждение должно выполняться по нескольким запросам асинхронно.
3. Доработка функции хеширования тензоров для совместимости с `E-tag` использование техник `if-match`. 
4. Доработка модели данных. Каждый тензор и операция должны иметь уникальное имя и свой объектный идентификатор, по которым выполняется поиск и кэширование. Кеширование тензоров в памяти хоста. 
5. Технология ZeroCopy и mmap. Память под тензоры должна выделяться с выравниванием на блок без копирования и фрагментации, в Shared памяти предназначенной для загрузки в GPU. 

Следует рассмотреть возможность применения технологий сжатия данных на потоке и `deduplication` на примере файловой системы ZFS. Причем, надо иметь ввиду, что некоторые тензоры не упакованы, до 50% места может быть заполнено нулями в режиме interleaving, а квантизация с понижением разрядности в некоторых случаях может быть выполнена без потери данных. В целом возможность сжатия loss-less данных оценивается от 10% до 30% для плотно заполненных тензоров. Однако, есть случаи, когда метод потокового сжатия может давать хорошие результаты и существенно ускорять запуск модели. Например матрицы KV кэша (ассоциативная память в составе модели) имеют скользящее окно и обновляют на каждом цикле только малую часть тензора, по столбцам и по строкам. Таким образом, методы с фрагментацией данных могут дать существенное ускорение и повысить эффективность работы с моделью, если при упаковке используется инкрементный образ тензора. Развертывание инкрементного образа может производится непосредственно в GPU поверх существующего тензора. 

Чтобы не пересылать тензоры модели на сервер каждый раз при обращении к службе, пересылаются хеши от тензоров. В существующей реализации функция хеширования работает медленно в однопотоковом режиме. Следует рассмотреть формирование хешей на основе SHA256, и xxh64 и других некриптографических хешей, расчет выполняется только на стороне клиента. Хеши модели должны высылаться в виде единого списка, а не пересчитываться каждый раз при загрузке. 

Рекомендуется внедрение протокола CoAP.
Альтернативная реализация triton использует gRPC поверх HTTP в различными фреймворками, такими как vLLM.

### 1 Доработка хеш функции

В проекте уже присутствует реализация функций хеширования тензоров и понятие манифеста, содержащего список тензоров с хешами sha256 и xxh64. xxh64 предлагается в качестве основного. В проекте присутствует относительно быстрая реализация функции xxh64.
[Моя реализация функции xxh64](https://github.com/AnatolyGeorgievski/LZJB/blob/main/xxh64.c) - лучше и быстрее, опирается на векторное расширение языка, переносима.
В проекте представлена не оптимизированная функция sha256. 
```sh
./build/bin/llama-gguf-hash.exe --xxh64 models/google_gemma-3-12b-it-Q4_K_L.gguf 
```
Время обработки тензоров в примере составило 2.8сек. 
```sh
$ time ./build/bin/llama-gguf-hash.exe models/google_gemma-3-12b-it-Q4_K_L.gguf > nil
```
