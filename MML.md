Математика для машинного обучения
=================================

* [PREV: математические классы](MATHAN.md)
* [PREV: сведения из теории вероятностей](PROB.md)
* [PREV: Цифровая обработка сигналов](DSP.md)
* [PREV: Квантование и дискретная математика](QUANT.md)
* [Квантизация моделей: рациональные и действительные числа](FPQUANT.md)
* [Типы нейронных сетей](#типы-нейронных-сетей)

Вводая часть для данного материала. Теория вероятностей: Разлоежние по независимым событиям, математическое ожидание. 

## Операторы


1. Производящая функция числовой последовательности.
2. Разложение по задаержкам

```math
f(nT_s) = \sum\limits_{k=0}^N \alpha_k z^{k}~\text{, где} ~~n = 0...N
```
Мы принимаем z - некоторый символ который означает "задержка" на один такт в дискретной по времени системе. 
Степенной ряд можно продифференцировать n раз по z.
```math
f^{(n)}(0) = n! \cdot \alpha_k
```
Таким образом из степенного ряда можно сделать ряд по производным.

## Безразмерные графы из операторов

*В данном разделе представлен способ описания графа в виде блочной диаграммы с плечами и узлами - графический язык. Узел графа - это опеорация типа сложения(вычитания) и операция типа умножения, каскад из операторов. Ребра - направленные, направление обозначается  стрелкой. Если направление не указано, то сигналы передаются слева-направо и сверху-вниз. На ребрах графа располагаются элементы: операторы и скаляры. Операторы обозначаем овалами или прямоугольниками, а скаляры - треугольниками (символ усилителя). Элементарные операции обозначаем кружочками.*

Мы определяем линейные операторы, как математический класс функций (функционалов). Над которыми можно определить операции типа сложения и типа композиция $( +, \circ )$. 
- В графах распространяются вектора. Линии графа (ребра) могут быть направлены и над линиями можно изобразить размерность вектора, например $\mathbb{R}^n$.
- Базовый класс вектора можно менять, мы определяем операцию квантизации для перехода из множества $\mathbb{R}^n$ в множество натуральных чисел $\{0,\mathbb{N}\}$. А также определяем переход на множестве рациональных чисел $\mathbb{Q}$. 
- Над графами можно определить множество скаляров.
- Операция сложения имеет обратную операцию, через обращение . 
- Операция композиции может быть некоммутативна или антикоммутативна. Мы будем исходить из того что операция обладает коммутативностью. 
- Для перестановки местами элементов композиции требуется выполнить операцию типа "сопряжение". Коммутативность некоммутативной операции можно определить через коммутатор операций $[\hat{F},\hat{G}] = \hat{F}\hat{G} - \hat{G}\hat{F}$

Свойства операторов линейность, ассоциативность по умножению - Ассоциативная алгебра. Свойства операторов вводятся, как множество аксиом, которым удовлетворяют операции с операторами.
- $\alpha(\hat{F}+\hat{G}) = \alpha\hat{F}+\alpha\hat{G}$
- $\hat{F}\circ\hat{G} = \hat{G}\circ\hat{F} $ - коммутативность?

Над графами действует алгебра.

## Дискретизация времени

## Квантизация

*{Ввести оператор квантования. Оператор повышения и понижения разрядности.}*

Под квантизацией понимают представление чисел в формате чисел с плавающей точкой меньшей разрядности мантиссы или в целых числах. 

## Тензоры и ядра

Определим базовое понятие, с которым нам предстоит работать. Это прежде всего вектора, и матрицы. В общем у класса объектов  *Тензор* есть понятие размерность, которая в аппаратуре поддерживается до 3 степеней. Тензор в нейронных сетях это элемент графа. То что является ребром графа - тензор, а ядра являются узлами графа. Узлу соответствует некоторая функция. 

Размерность тензора мы определяем несколько странно, есть несколько размеров связанных с аппаратурой и параллельностью расчетов, и размеры связанные с матрицами. 


На конвейер команд к исполнению ставится ядро - функция, параметрами которой являются блоки памяти - тензоры. Ядро имеет свою размерность кратную размеру тензоров, ядра запускаются параллельно. Одна матрица может при этом рассчитываться фрагментами, по строкам или по "плиткам" (tiles). Множество фрагментов тензора может рассчитываться параллельно. В этом смысле мы выделяем некоторую логическую организацию ядер - рабочую группу и локальную группу. Таким образом у нас возникает привязка по минимальному и максимальному размеру группы и количеству элементов в локальной группе. Локальная группа - та у которой есть общая разделяемая локальная память. Это архитектурные особенности вычислительной сети тем не менее влияют на параметры задачи. Так например тензор, а данные в памяти должен бы выровнен на минимальную величину вектора. А разбиение задания может быть обусловлено размером плитки, которую считает ускоритель матричных операций. 

Кроме размера вектора есть еще свойство самой модели. Многие модели (большие языковые) проектируются с учетом возможности параллельного исполнения на множестве ускорителей, потому что ресурсов одного не хватает. В таких моделях предусмотрено деления на "головы" (head). Например операция нормировки требует суммирования по всей матрице, а матрица может быть безгранична. Такую нормировку можно делать параллельно, не совсем честно, или с отложенным применением результата суммирования. Каждая голова считает сумму по плитке, затем суммирует по ряду, редуцирует полученное значение по локальной группе. 

В данном разделе мы вводим структуру элемент графа, которая включает ссылки на тензоры - аргументы ядра, тип операции и размерность операции. Тензоры можно использовать повторно.

Естественным дополнением работы с тензорами является конвейер команд, который может присутствовать явно или не явно в программе хоста. Каждое ядро имеет флаг готовности (завершения операции), элементы графа выполняются на к

### Подбор подходящей функции по типу тензора

В системе мы определяем и компилируем (собираем) множество ядер - функций, все функции характеризуются двумя "суффиксами" тип тензора на входе и тип тензора на выходе. Для запуска на конвейере выбирается подходящая функция, которая наилучшим образом подходит под триплет *входной_формат-операция-выходной_формат*. Если такая функция не найдена, то выбирается способ конвертации (квантизации и деквантизации элементов тензора) на входе и выходе функции. Т.е на ребро графа может быть установлена дополнительная функция преобразования формата тензора.

## Типы нейронных сетей

- (MLP) Multi-Layer Perceptron
- (CNN) Convolutional Neural Network 
- (RNN) Recurrent Neural Networks
- (GRU) Gated Reccurence Networks
- (LSTM) Long short-term memory
- (KSN) Kolmogorov Spline Networks
- (KAN) Kolmogorov-Arnold's Networks

В 2023-24 году появились разновидности нейросетей KAN. Популярность получили Трансформеры, LLaMa, RWKV. Все они используют общие принципы проектирования элемента сети сразной степенью эффективности. 

<https://neerc.ifmo.ru/wiki/index.php?title=%D0%94%D0%BE%D0%BB%D0%B3%D0%B0%D1%8F_%D0%BA%D1%80%D0%B0%D1%82%D0%BA%D0%BE%D1%81%D1%80%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BF%D0%B0%D0%BC%D1%8F%D1%82%D1%8C>

По ссылке можно наблюдать последнюю стадию эволюции. Но к сожалению эволюция пошла не тем путем, LLM - большие языковые модели. Следующий виток развития - сети KAN (сети на базе разложения Колмогорова-Арнольда).

Теорема о разложении КАТ утверждает, что любую многомерную функцию определенную на интервале $[0,1]$ можно разложить, как сумму композиций от функций зависящих только от одного переменного и фнкции сложения (от двух переменных). 

Я бы хотел в нашем курсе рассмотреть все варианты элементов вычислительной сети с точки зрения дифференциальных операторов. Блоки (фильтры) могут быть представлены функциями двух типов: которые дают на выходе скаляр (вероятность) на множестве значений $[0,1]$ и которые дают производную (операторы). Операторы нормированные можно представить на множестве значений $(-1,+1)$. При этом ограничением является использование в составе операторов арифметики с насыщением. В то время как вероятности используют арифметику с компенсацией ошибки округления. Оператор, который дает на выходе скаляр на схеме элемента мы можем обозначить типом $\sigma$, а оператор дифференциальный тангенсом $\theta$. 

Оператор - функционал, который по такту времени $h_n = f(h_{n-1}, x_n; \Theta)$ выполняет некоторую функцию, зависяющую от состояния системы в предыдущий момент времени и вектора внешних значений $x$. 
Операторы можно раскладывать в каскад и в сумму. Необходимость разложения в сумму требует чтобы элемент давал параллельный выход на том же такте, который может быть использован как вход в другом элементе (перенос). 

$$(с_{out}, h_n) = f(c_{in}, h_{n-1}, x_n; \Theta)$$



Исходная структура элемента определяется уравнениями ([LSTM](https://arxiv.org/pdf/1601.06733))

```math
\begin{aligned}
i_{t} &=\sigma (W_{i}[h_{t-1},x_{t}])\\
f_{t} &=\sigma (W_{f}[h_{t-1},x_{t}])\\
o_{t} &=\sigma (W_{o}[h_{t-1},x_{t}])\\
\hat{c}_{t} &=\tanh (W_{c}[h_{t-1},x_{t}])\\
c_{t} &= f_t \odot c_{t-1} + i_t\odot \hat{c_t}\\
h_{t} &= o_t \odot tanh(c_t)
\end{aligned}
```

**Объединенные фильтры**
Иные модификации LSTM включают объединенные фильтры “забывания” и входные фильтры

![LSTM m1](https://neerc.ifmo.ru/wiki/images/a/ad/Lstm-mod-1.png)


$$C_{t} = f_t \odot C_{t-1} + (1-f_t) \odot \tilde{C}_n = f_n \odot C_{n-1} + \bar{f}_n \odot \tilde{C}_n$$
Фукция f определена на интерывале [0,1]. В записе выражений мы используем символ доплнения $\bar{f} = (1-f)$

Реккурентное или каскадное применение этого принципа позволяет получить B-сплайны, сплайны на базисных полиномах Бернштейна. 

![GRU](https://neerc.ifmo.ru/wiki/images/c/ce/Lstm-gru.png)

[GRU]: <https://arxiv.org/pdf/1406.1078v3>

Аналогичный элемент (сплайн) присутсвтует и в GRU: 
$$h_{n} = (1-f_n) \ast h_{n-1} + f_n \ast \tilde{h}_n~.$$
Кроме того, представлен сброс состояния "reset gate" $r_n$. Эффект сброса может лучше было бы представить в той же форме, как баланс между начальным состоянием и текущим: 
$$r_n \ast h_{n-1} + (1-r_n) \ast h_0 ~.$$
Это всего два сигнала: "забывание" и "сброс". 

Эту тему, как бездумно слепить элемент для реккурентной сети, можно продолжить. Можно предложить создавать функции активации в форме сплайнов. Отдельно следует отметить функции семейства Smoothstep. 

Возможность разложения с использованием той или иной архитектуры можно оценивать по тестовым задачам...

*Задача*: на архитектуре сети считать уравнение для цифровой обрабокти сигналов вида 

```math
(1-\bar{\alpha}z^{-1})Y = {\alpha}\tilde{H}(z)X
```
```math
Y = \bar{\alpha}z^{-1}Y + {\alpha}\tilde{H}(z)X
```
Перепишем в терминах GRU
```math
\begin{align}
h_n &= (1-\alpha)*h_{n-1} + {\alpha}*\tilde{H}(z) x_n \\
    &\approx (1-\alpha)*h_{n-1} + {\alpha}*\tanh(W\cdot [x_n, z^{-1} x_n, ..., z^{-m} x_n])
\end{align}
```
Это возможно, но крайне не эффективно. Кроме того, останется вопрос, как справиться с нелинейностью функции $\tanh$. Я не вижу такого приближения, при котором элемент может быть натренирован до точного соотвествия уравнению. Все что мы можем сделать в поддержку традиций - разрешить использовать функцию `clamp()` вместо $\tanh$ или ввести функцию квантизации. Можем ввести обобщенную функцию $\phi$.

Чтобы привести выражение (2) к вмду. Нужно выполнить нормализацию сисстемы дифференциальных уравнений, обозначив все производные X, как новые переменные состояния. Тогда выражение примет вид
$$\tilde{h}_t = \tanh(W_h\cdot[r_t*h_{t-1}, x_t])~.$$

Перепишу уравнения GRU через оператор поэлементного умножения матриц Адамара $\odot$.
```math
\begin{aligned}
z_{t} &=\sigma (W_{z}x_{t}+U_{z}h_{t-1} + b_{z})\\
r_{t} &=\sigma (W_{r}x_{t}+U_{r}h_{t-1} + b_{r})\\
{\hat {h}}_{t}&=\phi (W_{h}x_{t}+U_{h}(r_{t}\odot h_{t-1}) + b_{h})\\
h_{t}&=(1-z_{t})\odot h_{t-1}+z_{t}\odot {\hat {h}}_{t}
\end{aligned}
```

Переменные для уравнения ( $d$ - число переменных на входе, размерность, а $e$ число выходных параметров):
* $x_{t}\in \mathbb {R} ^{d}$: вектор входных значений
* $h_{t}\in \mathbb {R} ^{e}$: вектор выходных значений
* ${\hat {h}}_{t}\in \mathbb {R} ^{e}$: candidate activation vector
* $z_{t}\in (0,1)^{e}$: update gate vector
* $r_{t}\in (0,1)^{e}$: reset  gate vector
* $W\in \mathbb {R} ^{e\times d}$, $U\in \mathbb {R} ^{e\times e}$ и $b\in \mathbb {R} ^{e}$ - матрицы и вектор, которые "тренируются"/подбираются  в процессе обучения

Не все параметры внутреннего состояния системы могут быть представлены на выходе, т.е. можно разложить внутреннее состояние на два множества сигналов, которые используются для формировнаия выходных значений $y$ и которые нужны только для хранения состояния системы $s$.


При построении архитектуры мы держим несколько тезисов в голове. 
1. Сингулярное разложение (SVD). Любую матрицу $M$ можно разложить на три $M = U\Sigma V^{\ast}$ - две ортогональные и диагональную.
2. Операторы в гильбертовом простраснтве выражаются через дифференциальные уравнения и через матрицы.
3. Матрица эволюции системы. Оператор эволюции системы запишется через матричную экспоненту (см. оператор эволюции в квантовой механике). $\hat{S}(t,t_0)$ -- унитарный оператор, который можно описать унитарной матрицей и сопоставить ему матричную экспоненту вида 
$$\hat{S}(t,t_0) = T\{\exp(-i\hat{H}(t-t0)/\hbar)\}~.$$
Эту математику заимствованную из теории операторов (квантовой физики), я бы предложил рассматривать, как "идеальную" архитектуру элемента сети. Оператор Т - упорядочение системы, которое выглядит как ортогональная (унитарная) матрица коэффициентов. 

Таким образом, проектирвоание элемента сети приводит к  выражению содержащему матричную экспоненту
$\exp(-w_{t,i}) = \exp(-(t-t_0) w)$ 
с дискретными значениями параметра t. Действие опреатора можно задать рекурсивно и представить в виде диагональной матрицы , с учетом входного и выходного унитарного преобразования (1).
$$s_t = \exp(-w) s_{t-1} + v$$ 

Во многом эти соображения носят интуитивный характер. Т.е. строго говоря для представления произвольной физической системы можно рассматривать некоторую нормализованную систему дифференциальных уравнений в гильбертовом пространстве. Для перехода в это пространство требуется выполнить аффинное преобразование от входных параметров и для получения результата - выходное аффинное преобразование. Зная три размерности системы (f - размерность выходных данных, e - размерность состояния, d- размерность входного вектора) можно полжностью описать систему в общем виде, в виде матриц и дифференциальных операторов.

Если записывать в общем виде аффинное преобразование выглядит как
$$v_{t} = W_{v}x_{t} + b_{v}~,$$
где $W_v \in \mathbb{R}^{e\times d}$ - матрица аффинного преобразования, а $b_v$ - вектор смещения системы координат.

Входными данными для дифференциального оператора может быть некоторая линейная комбинация состояния и входных значений таким образом в общем виде запишем управление параметром типа "вероятность":
$$r_{t} =\sigma (W_{r}x_{t}+U_{r}h_{t-1} + b_{r})$$

А "кандидатом" на внутренне состояние становится:
$${\hat {h}}_{t} =\phi (W_{h}x_{t}+U_{h}(r_{t}\ast h_{t-1}) + b_{h})$$


Все вычисления в теле оператора мы должны считать в нормализованном виде. Помимо нормы вектора, образованной от скалярного произведения в гильбертовом пространстве разумно ввести два типа функций нормализации сигналов - функция $\sigma(x) \in [0,1]$ и $\phi(x) \in [-1, 1]$. Функция $\sigma$ применяется для перехода от величин аналоговых к вероятностям, т.е. везде где надо расчитать коэффициент - скаляр или вектор скаляров. Функция $\phi$ - в прямом смысле для квантизации сигнала в процессе нормализации выходного сигнала, применяется по правилу, что в любом функциональном блоке все входные сигналы должны быть акивированы одним из двух способов. Любой интегральный оператор - это свертка состояния и оператора.

Так можно сформулировать архитектуру элемента сети, который удовлетворяет всем этим требованиям. И вроде бы система GRU и LSTM в какой-то форме удовлетворяет этим требованиям. Но в общем случае нам может понадобится элемент, который считает внутреннее состояние в комплексных числах а результат выдает в виде свертки двух сопряженных функций. Это было бы справедливо для "квантовых" сетей.

Рассматривая практические задачи мы требуем, чтобы при обрабокте изображения выполнялись операции билинейной интерполяции и фильтрации изображений. Таким образом мы накладываем требования на два слоя сети: 1) слой билинейной интерполяции (channel-mixing) и 2) слой цифровой фильтрации изображения (time-mixing) 3) Слой выделения и классификации призников. 
Опираясь на знания из области цифровой обработки сигналов для пунктов (1) (2) можно сформулировать элемент сети, как он должен выглядить.

Нам понадобятся знания из области теории вероятностей (биномиальные расспределения) интерполяция и сплайны. Интерполяция будет строится на конечных разностях, а сплайны будут представлены B-сплайнами с возможностью подбора коэффициентов. 

Таким образом вводится базовый элемент - линейная интерполяция из двух векторов 
$$\operatorname{lerp}(h,x, \mu) = (1-\mu)\odot h + \mu\odot x ~.$$

Выше мы показали, как дифференциальное уравнение вида
```math
(1-\bar{\alpha}z^{-1})Y = {\alpha}\tilde{H}(z)X
```
переписать в форме оператора $\operatorname{lerp}$. Наверное следовало бы представить решение в форме матричной экспоненты. Это становится возможным для системы дифференциальных уравнений первого порядка.
$${d \over dt}Y = -W\cdot Y + \hat{H}X$$
Частным решеием этого уравнения будет та самая матричная экспонента $\exp(-(t-t_0)W)$.

см. [Нормальная форма дифференциальных уравнений]

С эхти позиций мы можем оценивать предложенные архитектуры. 

**"Трансформаторы"**. 

Трансфо́рмер (англ. Transformer) — архитектура глубоких нейронных сетей, представленная в 2017 году исследователями из Google Brain[1].

Успех архитектуры "Трансформеры" обусловлен двумя идеями 1) линейная связь между входом и выходом обеспечивает эффективное обучение глубоких слоев. 2) наличие оператора "внимание" и "самосозерцание" (самовнимание). 

Термин внимание применялся еще в работах по LSTM.

Для вычисления "внимания" входного вектора $X$ к вектору $Y$, вычисляются вектора 
$Q=W_{Q}X$, $K=W_{K}X$, $V=W_{V}Y$. Эти вектора используются для вычисления результата внимания по формуле:
$${\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V$$

**Softmax**
> Многопеременная логистическая функция Softmax — это обобщение логистической функции для многомерного случая. Функция преобразует вектор  $z$ размерности $K$ в вектор 
$\sigma$ той же размерности, где каждая координата $\sigma _{i}$ полученного вектора представлена вещественным числом в интервале [0,1] и сумма координат равна 1.

Координаты $\sigma _{i}$ вычисляются следующим образом:
```math
\sigma (z)_{i}={\frac {e^{z_{i}}}{\displaystyle \sum _{k\mathop {=} 1}^{K}e^{z_{k}}}}
```
Многопеременная логистическая функция применяется в машинном обучении для задач классификации, когда количество возможных классов больше двух (для двух классов используется логистическая функция). 

Координаты $\sigma _{i}$ полученного вектора при этом трактуются, как вероятности того, что объект принадлежит к классу $i$. Вектор-столбец $z$ при этом рассчитывается следующим образом:
$z=w^{T}x-\theta$,\
где $x$ — вектор-столбец признаков объекта размерности $M\times 1$; 
$w^{T}$ — транспонированная матрица весовых коэффициентов признаков, имеющая размерность 
$K\times M$; 
$\theta$ — вектор-столбец с пороговыми значениями размерности 
$K\times 1$ (см. перцептрон), где $K$— количество классов объектов, 
а $M$ — количество признаков объектов.

Часто Softmax используется для последнего слоя глубоких нейронных сетей для задач классификации. Для обучения нейронной сети при этом в качестве функции потерь используется перекрёстная энтропия.

Вектор-столбец $z=y-\max(y)$, где $y= w^Tx$.

**Attn оператор**

$$\operatorname{Attn}(Q,K,V)_t = \frac{\sum_{i=1}^t e^{q_t^\mathsf{T} k_i} \odot v_i}{\sum_{i=1}^t e^{q_t^\mathsf{T} k_i}}$$


$$\operatorname{Attn^{+}}(W,K,V)_t = \frac{\sum_{i=1}^t e^{w_{t,i}+k_i} \odot v_i}{\sum_{i=1}^t e^{w_{t,i}+k_i}}$$

*AFT*, сокращение от *Attention Free Transformer*, представляет собой подход, отличный от традиционного механизма "внимания", и включает в себя изученные парные смещения позиций, обозначаемые, как $w_{t,i}$, где каждое $w_{t,i}$ является скалярным значением.

$\odot$ - произведение Адамара (поэлементное).

[2405](https://arxiv.org/pdf/2405.06640):

**Linear Transformers** (Katharopoulos et al., 2020) establish a connection between transformers and
RNNs, generalizing the definition of attention by replacing the softmax dot-product attention v′
with a more generic "similarity" function $sim(\mathbf{q}, \mathbf{k})$ between the queries $q$ and keys $k$:

$$v'_t = \frac{\sum_{i=1}^t sim(q_t, k_i) v_i}{\sum_{i=1}^t sim(q_t, k_i)}$$

Softmax - частный случай "внимания", когда 
$$sim(\mathbf{q}, \mathbf{k}) = \exp(\frac{\mathbf{q}^{\mathsf{T}}\mathbf{k}}{\sqrt{d}})$$

Kasai et al. (2021) introduced a linear transformer uptraining procedure that converts a pre-trained
softmax transformer into an RNN by approximating the attention computation with multi-layer
perceptrons (MLPs). The method (T2R) starts with a softmax attention model, and linearizes the
softmax operation. 

RWKV (Peng et al.,2023a), Retentive Networks (Sun et al., 2023), Griffin (De et al., 2024) and RecurrentGemma (Griffin Team et al., 2024).
Mistral (Jiang et al., 2023) and Llama2 (Touvron et al., 2023)

Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and
Josh Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021.

Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao,
Weizhu Chen, and Noah A Smith. Finetuning pretrained transformers into rnns. arXiv preprint
arXiv:2103.13076, 2021.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International conference on machine
learning, pp. 5156–5165. PMLR, 2020.


**WKV оператор**
В RWKV весовые коэффициенты $w_{t,i}$ рассматриваются, как вектор затухания по каналам. Этот вектор масштабируется в зависимости от относительной позиции и уменьшается c шагом времени в соответствии с правилом: 
$$w_{t,i} = -(t-i)w$$
В итоге оператор запишется
```math
WKV_t = \frac{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \odot v_i + e^{u+k_t}\odot v_t}{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u+k_t}}
```
Для понимания почему это работает надо рассматривать антологию архитектур. Среди которых выделяется ряд: LSTM GRU и вот тут определили WKV оператор и Mu оператор. В частности определили как связан оператор из сетей Трансформеры ATTN с оператором WKV. Говоря простым языком, ATTN общая структура, которую можно представить, как Softmax и как WKV-оператор. 

Отдельно стоит упомянуть прямую связь обходное значение. Весь элемент сети выражается, как разница входа и выхода. Изначально эта архитектурная особенность появилась ради возможности тренировать глубокие слои. 

Наш подход заключается в утверждении, что любую функцию многих переменных с входными значениями $v\in [0,1]$ можно представить, как суперпозицию (сложную функцию) от функций одного переменного и функцию сложения (функция двух переменных), см. KAT - теорема Колмогорова-Арнольда. Суперпозиция функций (операторов) в плане архитектуры сети - это каскад составленный из операторов.

Другое утверждение, что из функций смешивания (mix) можно построить: а) B-сплайн и б) функцию распределения в) цифровой фильтр. Оператор Mu следует преобразовать в элемент цифрового фильтра с обратной связью. 

**Нормализация слоя**

x :=  (x - x.mean)/x.std;

* mean - среднее арифметическое
* std - средне-квадратичное отклонение от среднего - норма вектора $\|x - \bar{x}\|$. 

std(x) = sqrt(mean(abs(x - x.mean())**2));



**RWKV** - рекурентные сети RNN с использованием оператора WKV.

$\operatorname{lerp}_\square (a,b) = mix(a,b, \mu_\square) = a + (b-a) \odot \mu_\square$  
$\mu_\square \in \mathbb{R}^D$ - для векторов $\{r,k,v,g\}$

$D$- размерность модели. $D/h$ - разбиение модели на группы (work_group, "head").

$$\square_t = \operatorname{lerp}_\square(x_t, x_{t-1}) W_\square, \square \in \{r,k,v,g\}$$
$$w = \exp(-\exp(\omega)), \omega \in \mathbb{R}^{D/h}$$

Зависимость коэффициента от значений входных данных вводят через оператор $\operatorname{lora}_\square$: 
$$\mu = lora_\square(x) = \lambda_\square + \tanh(x A_\square)B_\square$$

$$\operatorname{ddlerp}_\square(a,b) = a+ (b-a)\odot \operatorname{lora}_\square (a+ (b-a)\odot \mu_x)$$

Оператор WKV может быть представленн через рекурсию
```math
\begin{aligned}
wkv' &= s + diag(u) \cdot k^\mathsf{T} \cdot v\\
s_t &= diag(w) \cdot s_{t-1} + k^\mathsf{T} \cdot v
\end{aligned}
```


[[RWKV](https://arxiv.org/pdf/2404.05892)]


## Представление вероятностной логики

Допустим у нас имеется полный набор нормированных линейно-независимых функций определенных на интервале [0,1] и возвращающих значения [0,1]. Тогда можно обазовать Гильбертово или _вероятностное пространство_ с разложением по базису. Вероятностная логика рождается из определения независимых событий. 

переход от понятия множество выполняется через сопоставление операциям с множествами операций с числовой мерой множесва, вероятностью.
$$P(A\cap B)  = P(A)P(B)$$
$$P(A\cup B)  = P(A)+P(B) - P(A)P(B)$$

## Вырождение вероятностной логики в четкую логику

Утверждение. Если на входе вероятностная логическая модель принимает значения $\{0, 1\}$, для функции множества параметров заданных на интервале $[0, 1]$, то функцию можно представить в четкой логике.

Вырождение возможно только если мы не теряем значения при квантизации! 


## Разделение вычислительной сети на тактовые домены

