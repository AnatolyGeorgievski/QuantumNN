Математика для машинного обучения
=================================

* [PREV: математические классы](MATHAN.md)
* [PREV: сведения из теории вероятностей](PROB.md)
* [PREV: Цифровая обработка сигналов](DSP.md)
* [PREV: Квантование и дискретная математика](QUANT.md)
* [Квантизация моделей: рациональные и действительные числа](FPQUANT.md)
* [Сжатие моделей](#)
* [Типы нейронных сетей](#типы-нейронных-сетей)

Вводная часть для данного материала. Теория вероятностей: Разложение по независимым событиям, математическое ожидание. 

## Операторы


1. Производящая функция числовой последовательности.
2. Разложение по задержкам

```math
f(nT_s) = \sum\limits_{k=0}^N \alpha_k z^{k}~\text{, где} ~~n = 0...N
```
Мы принимаем $z$ - некоторый символ, который означает "задержка" на один такт в дискретной по времени системе. 
Степенной ряд можно продифференцировать $n$ раз по формальной переменной $z$.
```math
f^{(n)}(0) = n! \cdot \alpha_k
```
Таким образом из степенного ряда можно сделать ряд по производным.

## Безразмерные графы из операторов

*В данном разделе представлен способ описания графа в виде блочной диаграммы с плечами и узлами - графический язык. Узел графа - это операция типа сложения(вычитания) и операция типа умножения, каскад из операторов. Ребра - направленные, направление обозначается  стрелкой. Если направление не указано, то сигналы передаются слева-направо и сверху-вниз. На ребрах графа располагаются элементы: операторы и скаляры. Операторы обозначаем овалами или прямоугольниками, а скаляры - треугольниками (символ усилителя). Элементарные операции обозначаем кружочками.*

Мы определяем линейные операторы, как математический класс функций (функционалов). Над которыми можно определить операции типа сложения и типа композиция $( +, \circ )$. 
- В графах распространяются вектора. Линии графа (ребра) могут быть направлены и над линиями можно изобразить размерность вектора, например $\mathbb{R}^n$.
- Базовый класс вектора можно менять, мы определяем операцию квантизации для перехода из множества $\mathbb{R}^n$ в множество натуральных чисел $\{0,\mathbb{N}\}$. А также определяем переход на множестве рациональных чисел $\mathbb{Q}$. 
- Над графами можно определить множество скаляров.
- Операция сложения имеет обратную операцию, через обращение . 
- Операция композиции может быть некоммутативна или анти-коммутативна. Мы будем исходить из того что операция обладает коммутативностью. 
- Для перестановки местами элементов композиции требуется выполнить операцию типа "сопряжение". Коммутативность некоммутативной операции можно определить через коммутатор операций $[\hat{F},\hat{G}] = \hat{F}\hat{G} - \hat{G}\hat{F}$

Свойства операторов: линейность, ассоциативность по умножению - Ассоциативная алгебра. Свойства операторов вводятся, как множество аксиом, которым удовлетворяют операции с операторами.
- $\alpha(\hat{F}+\hat{G}) = \alpha\hat{F}+\alpha\hat{G}$
- $\hat{F}\circ\hat{G} = \hat{G}\circ\hat{F}$ - коммутативность.

Над графами действует алгебра.

## Дискретизация времени

## Квантизация

*{Ввести оператор квантования. Оператор повышения и понижения разрядности.}*

Под квантизацией в машинном обучении понимают представление чисел в формате чисел с плавающей точкой меньшей разрядности мантиссы или в целых (рациональных) числах. [подробнее..](FPQUANT.md)

## Сжатие моделей

Другой способ уменьшения объема модели при сохранении функционала после обучения - сжатие модели. 
Сжатие с потерей точности выполняется на основе сингулярного разложения матрицы. Любую прямоугольную матрицу можно разложить на произведение трех матриц $U\Sigma V$ - Singular Value Decomposition (или аналогичных методов, таких как $LDL^T$-, QR- разложения). Подробнее см. [[Линейная алгебра - разложение матриц](LINALG.md)]. Матрица $\Sigma$ - диагональная, содержит собственные значения на диагонали, может быть отсортирована по убыванию значений. При сохранении модели можно принять критерий отбрасывания части собственных значений, тем самым мы уменьшим размер матриц (U,V). Помимо сингулярного разложения могут применять и другие способы разложения, с целью сортировки и уменьшения ошибки. 

При расчете физических моделей матрицы в разложении могут быть комплексные, ортогональные, унитарные, эрмитовы, гамильтоновы, симметричные, анти-симметричные, блочно-диагональные, ленточные и трехдиагональные. 

Для каждого класса матриц существует свой тип разложения оптимально решающий эту задачу - разложения с сортировкой диагональной матрицы и сортировкой строк и столбцов. Так например про искомые матрицы может быть заранее известно, что они принимают трехдиагональную форму и содержат случайные связи (вторая матрица сильно разряжена). Это значит, что матрицу можно разложить на две и выделить систему дифференциальных уравнений, которой она соответствует.

Различные методы разложения матриц хорошо представлены в библиотеке GSL ([GNU Scientific Library](https://www.gnu.org/software/gsl/doc/html/)). GSL - хорошая отправная точка для встраивания методов разложения и оптимизации матриц. 

## Тензоры и ядра

Определим базовое понятие, с которым нам предстоит работать. Это прежде всего вектора, и матрицы. В общем у класса объектов  *Тензор* есть понятие размерность, которая в аппаратуре поддерживается до 3 степеней. Тензор в нейронных сетях это элемент графа. То что является ребром графа - тензор, а ядра являются узлами графа. Узлу соответствует некоторая функция. 

Размерность тензора мы определяем несколько странно, есть несколько размеров связанных с аппаратурой и параллельностью расчетов, и размеры связанные с матрицами. 


На конвейер команд к исполнению ставится ядро - функция, параметрами которой являются блоки памяти - тензоры. Ядро имеет свою размерность кратную размеру тензоров, ядра запускаются параллельно. Одна матрица может при этом рассчитываться фрагментами, по строкам или по "блокам" (tiles). Множество фрагментов тензора может рассчитываться параллельно. В этом смысле мы выделяем некоторую логическую организацию ядер - рабочую группу и локальную группу. Таким образом у нас возникает привязка по минимальному и максимальному размеру группы и количеству элементов в локальной группе. Локальная группа - та у которой есть общая разделяемая локальная память. Это архитектурные особенности вычислительной сети тем не менее влияют на параметры задания. Так например тензор, данные в памяти должны быть выровнены на минимальную величину вектора. А разбиение задания может быть обусловлено размером блока, которую считает ускоритель матричных операций. 

Кроме размера вектора есть еще свойство самой модели. Многие модели (большие языковые) проектируются с учетом возможности параллельного исполнения на множестве ускорителей, потому что ресурсов одного не хватает. В таких моделях предусмотрено деления на "головы" (head). Например операция нормировки требует суммирования по всей матрице, а матрица может быть безгранична. Такую нормировку можно делать параллельно, не совсем честно, или с отложенным применением результата суммирования. Каждая голова считает сумму по плитке, затем суммирует по ряду, редуцирует полученное значение по локальной группе. 

В данном разделе мы вводим структуру элемент графа, которая включает ссылки на тип данных тензоры - аргументы ядра оператора, тип операции и размерность операции. Тензоры можно использовать повторно.

Естественным дополнением работы с тензорами является конвейер команд, который может присутствовать явно или не явно в программе хоста. Каждое ядро имеет флаг готовности (завершения операции), элементы графа выполняются на конвейере.

### Подбор подходящей функции по типу тензора

В системе мы определяем и компилируем (собираем) множество ядер - функций, все функции характеризуются двумя "суффиксами" тип тензора на входе и тип тензора на выходе. Для запуска на конвейере выбирается подходящая функция, которая наилучшим образом подходит под триплет *входной_формат-операция-выходной_формат*. Если такая функция не найдена, то выбирается способ конвертации (квантизации и деквантизации элементов тензора) на входе и выходе функции. Т.е на ребро графа может быть установлена дополнительная функция преобразования формата тензора.


Эта концепция подбора последовательности функций для приведения типа тензоров называется "type traitis"

### Выделение плиток из Тензора

Некоторые методы линейной алгебры позволяют работать с подматрицами разбивать матрицы на фрагменты  ("плитками", Tiles) и разбивать задание на подзадачи. 

Мы используем некоторую идею по работе с указателями, которая в частности применяется и при загрузке языковых моделей и при научных расчетах (в библиотеке BLAS). Рассмотрим на примере реализации в библиотеке GSL. 
```c
typedef struct _tensor {
	size_t ne[2]; 	// размерность матрицы (число строк и столбцов)
	size_t tda;  	// размер строки с выравниванием
	float * data; 	// данные в памяти
} _matrix_t;
```

* `_matrix_submatrix()` - выделяет фрагмент матрицы без копирования элементов
```c
void _matrix_submatrix (_matrix_t *m, size_t i, size_t j, size_t n1, size_t n2){
	_matrix_t s;

	s.data = m->data + (i * m->tda + j);
	s.ne[0] = n1;
	s.ne[1] = n2;
	s.tda = m->tda;
	return s;     
}
```
Представление матриц связано со способом хранения значений. Так под каждый тип тензора: `F64`, `F32`, `F16`, `Q8_0`, `Q8_1`... надо будет создавать свою функцию выделения фрагмента матрицы, свой набор базовых операций. 

```c
float _matrix_get(const matrix_t * m, size_t i, size_t j) {
  return m->data[i * m->tda + j] ;
} 
```
Базовый набор операций с матрицами включает
* `_add`, `_sub`
* `_mul` - произведение двух матриц
* `_mul_elements` -- произведение Адамара - поэлементное

К числу матричных операций можно отнести и операции $\pm A\cdot x \pm b$: `_madd` `_msub` `_nmadd`  
А также более сложные операторы, которые свойственны моделям нейросетей такие как `ffn`, `attn`, `wkv`, `lerp`, `lora` и пр.

Кроме того, надо реализовать операции квантизации и деквантизации для выбранных пар типов (тип тензора коэффициентов, тип вектора результата). Операции квантизации и деквантизации будут использоваться для загрузки тензоров весовых коэффициентов и могут быть совмещены с операцией $A\cdot x \pm b$ и операцией скалярного произведения $k \odot v$.

Пакет BLAS (пакет базовых подпрограмм линейной алгебры) включает набор масштабируемых операций с матрицами. Операции третьего уровня построены с использованием блочных и рекурсивных методов, когда на каждом шаге алгоритма матрица разбивается на блоки (подматрицы) и вычисление происходит на матрицах меньшего размера. Задается порог, после которого операция считается неделимой. Эти методы подходят для больших матриц. При переносе на архитектуру GPU можно определить соответствующий порог для применения рекурсии. 

Именование методов BLAS содержит приставку `s` - single precision или `d` - double precision. К данным классам следует добавить `h` - Float16(half). Далее имя матричного метода может содержать приставку класса матрицы: `HE` - эрмитовы, `TR` - треугольные, `SY` - симметричные, `GE` - обычные. В [курсе линейной алгебры](LINALG.md) возникают матрицы анти-симметричные, гамильтоновы, матрицы хессенберга, ленточные трехдиагональные и блочно-диагональные. 

Некоторые примеры именования методов:
* `gsl_blas_sgemm` - подсчет произведения двух матриц
* `gsl_blas_strmm` - подсчет произведения двух треугольных матриц
* `gsl_blas_strsm` - подсчет произведения обратной матрицы на матрицу.

Первый уровень - операции типа норма вектора, скалярное произведение и т.п.
* `gsl_blas_sdot`  $x^T y$
* `gsl_blas_sdsdot` $x^T y+ \alpha$
* `gsl_blas_saxpy` $y = \alpha x + y$ сложение векторов
* `gsl_blas_snrm2` расчет квадратичной нормы вектора
* `gsl_blas_sasum` расчет суммы абсолютных значений

Второй уровень:
* `gsl_blas_sgemv` оператор  $y = \alpha A x + \beta y$ или $y = \alpha A^T x + \beta y$
* `gsl_blas_strmv` оператор  $y = \alpha A x$ или $y = \alpha A^T x$
* `gsl_blas_strsv` оператор  $A = (A^T)^{-1}x$ или $A = A^{-1}x$
* `gsl_blas_sger`  оператор  $A = \alpha x y^T + A$
* `gsl_blas_ssyr`  оператор  $A = \alpha x x^T + A$
* `gsl_blas_ssyr2` оператор  $A = \alpha x y^T + \alpha y x^T + A$

Методы второго уровня предполагают использование LU разложения или LDU разложения матриц. Каждый метод имеет параметр, какую часть упакованной матрицы использовать: верхнюю треугольную или нижнюю треугольную. 

При проектировании интерфейса API следует учитывать особенности реализации функций в BLAS. Кроме того следует принять тезис, все операции на конвейере можно выстраивать таким образом, чтобы нормировка вектора-результата выполнялась на следующем слое сети. 

В структуре `vector_t` следует предусмотреть отложенную операцию нормировки. Причем, если это вектор n- размерный то операция векторного произведения помещает в нормировку - скалярное произведение. см. [кватернионы и группы вращения](QUAT.md).

Третий уровень - рекурсивные методы разложения и операции с матрицами
* `gsl_blas_sgemm` оператор $C = \alpha A B + \beta C$, $C = \alpha A^\mathsf{T} B + \beta C$ или $C = \alpha A B^\mathsf{T} + \beta C$ 
* `gsl_blas_strmm` оператор $B = \alpha B A$, $B = \alpha A B$, $B = \alpha A^\mathsf{T} B$, 
$B = \alpha B A^\mathsf{T}$
* `gsl_blas_strsm` оператор $B = \alpha B A^{-1}$, $B = \alpha (A^{-1}) B$, $B = \alpha (A^{-1})^\mathsf{T} B$, 
$B = \alpha B (A^{-1})^\mathsf{T}$

{дописать}

Флаги операций: 
`_UPPER`,`_LOW` - использовать нижнюю или верхнюю треугольную матрицу, `_DIAG` - использовать диагональную матрицу, `_TRANSPOSE` - выполнить транспонирование матрицы, `_SIDE` - левое или правое умножение.


Ко всему многообразию операций BLAS (которые могут быть определены в числах float double и complex) я бы добавил возможность по шаблону создавать набор операций для квантизованных значений $F16$, $BF16$, целых типов с отложенной нормировкой $Q8_1$, рациональные числа, модульную арифметику, булеву - двоичную арифметику, и арифметику Галуа.

{дописать}

## Типы нейронных сетей

- (MLP) Multi-Layer Perceptron
- (CNN) Convolutional Neural Network 
- (RNN) Recurrent Neural Networks
- (GRU) Gated Reccurence Networks
- (LSTM) Long short-term memory
- (KSN) Kolmogorov Spline Networks
- (KAN) Kolmogorov-Arnold's Networks

В 2023-24 году появились разновидности нейросетей KAN. Популярность получили Трансформеры, LLaMa, RWKV. Все они используют общие принципы проектирования элемента сети с разной степенью эффективности. 

<https://neerc.ifmo.ru/wiki/index.php?title=%D0%94%D0%BE%D0%BB%D0%B3%D0%B0%D1%8F_%D0%BA%D1%80%D0%B0%D1%82%D0%BA%D0%BE%D1%81%D1%80%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BF%D0%B0%D0%BC%D1%8F%D1%82%D1%8C>

По ссылке можно наблюдать последнюю стадию эволюции рекуррентных. Но к сожалению эволюция пошла не тем путем, LLM - большие языковые модели. Следующий виток развития - сети KAN (сети на базе разложения Колмогорова-Арнольда).

Теорема о разложении КАТ утверждает, что любую многомерную функцию определенную на интервале $[0,1]$ можно разложить, как сумму композиций от функций зависящих только от одного переменного и функции сложения (от двух переменных). 

Я бы хотел в нашем курсе рассмотреть все варианты элементов вычислительной сети с точки зрения дифференциальных операторов. Блоки (фильтры) могут быть представлены функциями двух типов: которые дают на выходе скаляр (вероятность) на множестве значений $[0,1]$ и которые дают производную (операторы). Операторы нормированные можно представить на множестве значений $(-1,+1)$. При этом ограничением является использование в составе операторов арифметики с насыщением. В то время как вероятности используют арифметику с компенсацией ошибки округления. Оператор, который дает на выходе скаляр на схеме элемента мы можем обозначить типом $\sigma$, а оператор дифференциальный тангенсом $\theta$. 

Оператор - функционал, который по такту времени $h_n = f(h_{n-1}, x_n; \Theta)$ выполняет некоторую функцию, зависящую от состояния системы в предыдущий момент времени и вектора внешних значений $x$. 
Операторы можно раскладывать в каскад и в сумму. Необходимость разложения в сумму требует чтобы элемент давал параллельный выход на том же такте, который может быть использован как вход в другом элементе (перенос). 

$$(с_{out}, h_n) = f(c_{in}, h_{n-1}, x_n; \Theta)$$



Исходная структура элемента определяется уравнениями ([LSTM](https://arxiv.org/pdf/1601.06733))

```math
\begin{aligned}
i_{t} &=\sigma (W_{i}[h_{t-1},x_{t}])\\
f_{t} &=\sigma (W_{f}[h_{t-1},x_{t}])\\
o_{t} &=\sigma (W_{o}[h_{t-1},x_{t}])\\
\hat{c}_{t} &=\tanh (W_{c}[h_{t-1},x_{t}])\\
c_{t} &= f_t \odot c_{t-1} + i_t\odot \hat{c_t}\\
h_{t} &= o_t \odot tanh(c_t)
\end{aligned}
```

**Объединенные фильтры**
Иные модификации LSTM включают объединенные фильтры “забывания” и входные фильтры

![LSTM m1](https://neerc.ifmo.ru/wiki/images/a/ad/Lstm-mod-1.png)


$$C_{t} = f_t \odot C_{t-1} + (1-f_t) \odot \tilde{C}_n = f_n \odot C_{n-1} + \bar{f}_n \odot \tilde{C}_n$$
Функция f определена на интервале [0,1]. В записи выражений мы используем символ дополнения $\bar{f} = (1-f)$

Рекуррентное или каскадное применение этого принципа позволяет получить B-сплайны, сплайны на базисных полиномах Бернштейна. 

![GRU](https://neerc.ifmo.ru/wiki/images/c/ce/Lstm-gru.png)

[GRU]: <https://arxiv.org/pdf/1406.1078v3>

Аналогичный элемент (сплайн) присутсвтует и в GRU: 
$$h_{n} = (1-f_n) \ast h_{n-1} + f_n \ast \tilde{h}_n~.$$
Кроме того, представлен сброс состояния "reset gate" $r_n$. Эффект сброса может лучше было бы представить в той же форме, как баланс между начальным состоянием и текущим: 
$$r_n \ast h_{n-1} + (1-r_n) \ast h_0 ~.$$
Это всего два сигнала: "забывание" и "сброс". 

Эту тему, как бездумно слепить элемент для рекуррентной сети, можно продолжить. Можно предложить создавать функции активации в форме сплайнов. Отдельно следует отметить функции семейства Smoothstep. 

Возможность разложения с использованием той или иной архитектуры можно оценивать по тестовым задачам...

*Задача*: на архитектуре сети считать уравнение для цифровой обработки сигналов вида 

```math
(1-\bar{\alpha}z^{-1})Y = {\alpha}\tilde{H}(z)X
```
```math
Y = \bar{\alpha}z^{-1}Y + {\alpha}\tilde{H}(z)X
```
Перепишем в терминах GRU
```math
\begin{align}
h_n &= (1-\alpha)*h_{n-1} + {\alpha}*\tilde{H}(z) x_n \\
    &\approx (1-\alpha)*h_{n-1} + {\alpha}*\tanh(W\cdot [x_n, z^{-1} x_n, ..., z^{-m} x_n])
\end{align}
```
Это возможно, но крайне не эффективно. Кроме того, останется вопрос, как справиться с нелинейностью функции $\tanh$. Я не вижу такого приближения, при котором элемент может быть натренирован до точного соответствия уравнению. Все что мы можем сделать в поддержку традиций - разрешить использовать функцию `clamp()` вместо $\tanh$ или ввести функцию квантизации. Можем ввести обобщенную функцию $\phi$.

Чтобы привести выражение (2) к вмду. Нужно выполнить нормализацию сисстемы дифференциальных уравнений, обозначив все производные X, как новые переменные состояния. Тогда выражение примет вид
$$\tilde{h}_t = \tanh(W_h\cdot[r_t*h_{t-1}, x_t])~.$$

Перепишу уравнения GRU через оператор поэлементного умножения матриц Адамара $\odot$.
```math
\begin{aligned}
z_{t} &=\sigma (W_{z}x_{t}+U_{z}h_{t-1} + b_{z})\\
r_{t} &=\sigma (W_{r}x_{t}+U_{r}h_{t-1} + b_{r})\\
{\hat {h}}_{t}&=\phi (W_{h}x_{t}+U_{h}(r_{t}\odot h_{t-1}) + b_{h})\\
h_{t}&=(1-z_{t})\odot h_{t-1}+z_{t}\odot {\hat {h}}_{t}
\end{aligned}
```

Переменные для уравнения ( $d$ - число переменных на входе, размерность, а $e$ число выходных параметров):
* $x_{t}\in \mathbb {R} ^{d}$: вектор входных значений
* $h_{t}\in \mathbb {R} ^{e}$: вектор выходных значений
* ${\hat {h}}_{t}\in \mathbb {R} ^{e}$: candidate activation vector
* $z_{t}\in (0,1)^{e}$: update gate vector
* $r_{t}\in (0,1)^{e}$: reset  gate vector
* $W\in \mathbb {R} ^{e\times d}$, $U\in \mathbb{R}^{e\times e}$ и $b\in \mathbb{R}^{e}$ - матрицы и вектор, которые "тренируются"/подбираются  в процессе обучения

Не все параметры внутреннего состояния системы могут быть представлены на выходе, т.е. можно разложить внутреннее состояние на два множества сигналов, которые используются для формировнаия выходных значений $y$ и которые нужны только для хранения состояния системы $s$.


При построении архитектуры мы держим несколько тезисов в голове. 
1. Сингулярное разложение (SVD). Любую матрицу $M$ можно разложить на три $M = U\Sigma V^{\ast}$ - две ортогональные и диагональную.
2. Операторы в гильбертовом простраснтве выражаются через дифференциальные уравнения и через матрицы.
3. Матрица эволюции системы. Оператор эволюции системы запишется через матричную экспоненту (см. оператор эволюции в квантовой механике). $\hat{S}(t,t_0)$ -- унитарный оператор, который можно описать унитарной матрицей и сопоставить ему матричную экспоненту вида 
$$\hat{S}(t,t_0) = T\{\exp(-i\hat{H}(t-t0)/\hbar)\}~.$$
Эту математику заимствованную из теории операторов (квантовой физики), я бы предложил рассматривать, как "идеальную" архитектуру элемента сети. Оператор Т - упорядочение системы, которое выглядит как ортогональная (унитарная) матрица коэффициентов. 

Таким образом, проектирвоание элемента сети приводит к  выражению содержащему матричную экспоненту
$\exp(-w_{t,i}) = \exp(-(t-t_0) w)$ 
с дискретными значениями параметра t. Действие опреатора можно задать рекурсивно и представить в виде диагональной матрицы , с учетом входного и выходного унитарного преобразования (1).
$$s_t = \exp(-w) s_{t-1} + v$$ 

Во многом эти соображения носят интуитивный характер. Т.е. строго говоря для представления произвольной физической системы можно рассматривать некоторую нормализованную систему дифференциальных уравнений в гильбертовом пространстве. Для перехода в это пространство требуется выполнить аффинное преобразование от входных параметров и для получения результата - выходное аффинное преобразование. Зная три размерности системы (f - размерность выходных данных, e - размерность состояния, d- размерность входного вектора) можно полжностью описать систему в общем виде, в виде матриц и дифференциальных операторов.

Если записывать в общем виде аффинное преобразование выглядит как
$$v_{t} = W_{v}x_{t} + b_{v}~,$$
где $W_v \in \mathbb{R}^{e\times d}$ - матрица аффинного преобразования, а $b_v$ - вектор смещения системы координат.

Входными данными для дифференциального оператора может быть некоторая линейная комбинация состояния и входных значений таким образом в общем виде запишем управление параметром типа "вероятность":
$$r_{t} =\sigma (W_{r}x_{t}+U_{r}h_{t-1} + b_{r})$$

А "кандидатом" на внутренне состояние становится:
```math
\hat{h}_t =\phi (W_{h}x_{t} + U_{h}(r_{t}\odot h_{t-1}) + b_{h})
```

Все вычисления в теле оператора мы должны считать в нормализованном виде. Помимо нормы вектора, образованной от скалярного произведения в гильбертовом пространстве разумно ввести два типа функций нормализации сигналов - функция $\sigma(x) \in [0,1]$ и $\phi(x) \in [-1, 1]$. Функция $\sigma$ применяется для перехода от величин аналоговых к вероятностям, т.е. везде где надо расчитать коэффициент - скаляр или вектор скаляров. Функция $\phi$ - в прямом смысле для квантизации сигнала в процессе нормализации выходного сигнала, применяется по правилу, что в любом функциональном блоке все входные сигналы должны быть акивированы одним из двух способов. Любой интегральный оператор - это свертка состояния и оператора.

Так можно сформулировать архитектуру элемента сети, который удовлетворяет всем этим требованиям. И вроде бы система GRU и LSTM в какой-то форме удовлетворяет этим требованиям. Но в общем случае нам может понадобится элемент, который считает внутреннее состояние в комплексных числах а результат выдает в виде свертки двух сопряженных функций. Это было бы справедливо для "квантовых" сетей.

Рассматривая практические задачи мы требуем, чтобы при обработке изображения выполнялись операции билинейной интерполяции и фильтрации изображений. Таким образом мы накладываем требования на два слоя сети: 1) слой билинейной интерполяции (channel-mixing) и 2) слой цифровой фильтрации изображения (time-mixing) 3) Слой выделения и классификации признаков. 
Опираясь на знания из области цифровой обработки сигналов для пунктов (1) (2) можно сформулировать элемент сети, как он должен выглядеть.

Нам понадобятся знания из области теории вероятностей (биномиальные распределения) интерполяция и сплайны. Интерполяция будет строится на конечных разностях, а сплайны будут представлены B-сплайнами с возможностью подбора коэффициентов. 

Таким образом вводится базовый элемент - линейная интерполяция из двух векторов 
$$\mathrm{lerp}(h,x, \mu) = (1-\mu)\odot h + \mu\odot x ~.$$

Выше мы показали, как дифференциальное уравнение вида
```math
(1-\bar{\alpha}z^{-1})Y = {\alpha}\tilde{H}(z)X
```
переписать в форме оператора $\mathrm{lerp}$. Наверное следовало бы представить решение в форме матричной экспоненты. Это становится возможным для системы дифференциальных уравнений первого порядка.
$${d \over dt}Y = -W\cdot Y + \hat{H}X$$
Частным решеием этого уравнения будет та самая матричная экспонента $\exp(-(t-t_0)W)$.

см. [Нормальная форма дифференциальных уравнений]

С эхти позиций мы можем оценивать предложенные архитектуры. 

**"Трансформаторы"**. 

Трансфо́рмер (англ. Transformer) — архитектура глубоких нейронных сетей, представленная в 2017 году исследователями из Google Brain[1].

Успех архитектуры "Трансформеры" обусловлен двумя идеями 1) линейная связь между входом и выходом обеспечивает эффективное обучение глубоких слоев. 2) наличие оператора "внимание" и "самосозерцание" (самовнимание). 

Термин внимание применялся еще в работах по LSTM.

Для вычисления "внимания" входного вектора $X$ к вектору $Y$, вычисляются вектора 
$Q=W_{Q}X$, $K=W_{K}X$, $V=W_{V}Y$. Эти вектора используются для вычисления результата внимания по формуле:
```math
{\text{Attention}}(Q,K,V)={\text{softmax}}\left({\frac {QK^{\mathrm {T} }}{\sqrt {d_{k}}}}\right)V
```

**Softmax**
> Многопеременная логистическая функция Softmax — это обобщение логистической функции для многомерного случая. Функция преобразует вектор  $z$ размерности $K$ в вектор 
$\sigma$ той же размерности, где каждая координата $\sigma _{i}$ полученного вектора представлена вещественным числом в интервале [0,1] и сумма координат равна 1.

Координаты $\sigma _{i}$ вычисляются следующим образом:
```math
\sigma (z)_{i} = \frac {e^{\lambda z_{i}}}{\sum _{k = 1}^{n} e^{\lambda z_{k}}}~, \lambda > 0
```
где $\lambda$ - нормировка, называется inverse temperature constant. При $\lambda=1$ нормальная функция softmax.


Обобщенная функция softmax
```math
\sigma (z)_{i} = \frac {e^{\lambda_{i} z_{i}}}{\sum _{k = 1}^{n} e^{\lambda_{k} z_{k}}}~, \lambda > 0
```
Весовые коэффициенты задаются индивидуально

Многопеременная логистическая функция применяется в машинном обучении для задач классификации, когда количество возможных классов больше двух (для двух классов используется логистическая функция). 

Координаты $\sigma _{i}$ полученного вектора при этом трактуются, как вероятности того, что объект принадлежит к классу $i$. Вектор-столбец $z$ при этом рассчитывается следующим образом:
$z=w^{T}x-\theta$,\
где $x$ — вектор-столбец признаков объекта размерности $M\times 1$; 
$w^{T}$ — транспонированная матрица весовых коэффициентов признаков, имеющая размерность 
$K\times M$; 
$\theta$ — вектор-столбец с пороговыми значениями размерности 
$K\times 1$ (см. перцептрон), где $K$— количество классов объектов, 
а $M$ — количество признаков объектов.

Часто Softmax используется для последнего слоя глубоких нейронных сетей для задач классификации. Для обучения нейронной сети при этом в качестве функции потерь используется перекрёстная энтропия.

* <https://github.com/higham/what-is/blob/master/softmax.pdf>
* Аccurate computation of the log-sum-exp and softmax functions 
<https://arxiv.org/pdf/1909.03469>

Функция softmax является градиентом функции $\mathop{\rm{lse}}(z)$:

$$\mathop{\rm{lse}}(z) = \log \sum\limits_{i=1}^n e^{z_i}$$

* On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning
<https://arxiv.org/abs/1704.00805>

-- От функции softmax можно взять векторную производную - Якобиан или вторую производную от lse - Хессиан. 


$$\sigma (z)_{i} = {\partial \over \partial z_i}\mathop{\rm{lse}}(z)$$

$$J[\sigma(z)] = \nabla^2 \mathop{\rm{lse}}(z) = \lambda(\mathop{\rm{diag}}(\sigma(z)) - \sigma(z)\sigma(z)^{\mathsf{T}})$$

Функцию softmax рекомендуется применять в сдвинутом виде, суть не меняется, но удается избежать переполенения при вычислении экспоненты. Смещение может быть выбрано константой, средним значением или нормой вектора. С точки зрения математики - эквивалентные представления.

```math
\sigma (z)_{i} = \frac {e^{z_{i} - \max(z)}}{\sum _{k = 1}^{n} e^{z_{k}-\max(z))}}
```
Вектор-столбец $z=y-\max(y)$. Аналогично сдвигается функция lse
$$\mathop{\rm{lse}}(z) = \max(z) + \log \sum\limits_{i=1}^n e^{z_{i} - \max(z)}$$

Другой подход - нормировка вектора, $\lambda = 1/{\Vert z \Vert}_2$.
```math
\sigma (z)_{i} = \frac {e^{\lambda z_{i} - \beta}}{\sum _{k = 1}^{n} e^{\lambda y_i -\beta}}
```

*Заметим, что существует гиперплоскость, в которой функця softmax - постоянна. Функцию можно определить через градиент - вектор перпендикулряный гиперплоскости.*

*Заметим, матричная функция softmax диагонализируема, сводится к векторной функции. 
Свойство матричной экспоненты $e^A = e^{V\Sigma V^\ast} = V e^\Sigma V^\ast$ и степенной функции $(VD V^\ast)^t = V D^t V^\ast$, где $V$ - унитарная матрица.
Это свойство позволяет выполнять разложение матричных функций и выносить ортогональные матрицы из выражений.* 

**FFN оператор**

```math
FFN(x) = \max(0, W_1 x + b_1)W_2 + b_2
```
Глядя на оператор FFN вспоминаю перцептрон. Как на аналоговых вычислениях сделать логическую функцию. Один слой позволяет сделать функцию И или функцию ИЛИ с использованием сложения и функции активации (пороговой функции). 

**Attn оператор**

```math
\mathop{\rm{Attn}}(Q,K,V)_t = \frac{\sum_{i=1}^t e^{q_t^\mathsf{T} k_i} \odot v_i}{\sum_{i=1}^t e^{q_t^\mathsf{T} k_i}}
```

```math
\mathop{\rm{Attn}}^{+}(W,K,V)_t = \frac{\sum_{i=1}^t e^{w_{t,i}+k_i} \odot v_i}{\sum_{i=1}^t e^{w_{t,i}+k_i}}
```

*AFT*, сокращение от *Attention Free Transformer*, представляет собой подход, отличный от традиционного механизма "внимания", и включает в себя изученные парные смещения позиций, обозначаемые, как $w_{t,i}$, где каждое $w_{t,i}$ является скалярным значением.

$\odot$ - произведение Адамара (поэлементное).

[2405](https://arxiv.org/pdf/2405.06640):

**Linear Transformers** (Katharopoulos et al., 2020) establish a connection between transformers and
RNNs, generalizing the definition of attention by replacing the softmax dot-product attention v′
with a more generic "similarity" function $sim(\mathbf{q}, \mathbf{k})$ between the queries $q$ and keys $k$:

$$v'_t = \frac{\sum_{i=1}^t sim(q_t, k_i) v_i}{\sum_{i=1}^t sim(q_t, k_i)}$$

Softmax - частный случай "внимания", когда 
$$sim(\mathbf{q}, \mathbf{k}) = \exp(\frac{\mathbf{q}^{\mathsf{T}}\mathbf{k}}{\sqrt{d}})$$

Kasai et al. (2021) introduced a linear transformer uptraining procedure that converts a pre-trained
softmax transformer into an RNN by approximating the attention computation with multi-layer
perceptrons (MLPs). The method (T2R) starts with a softmax attention model, and linearizes the
softmax operation. 

RWKV (Peng et al.,2023a), Retentive Networks (Sun et al., 2023), Griffin (De et al., 2024) and RecurrentGemma (Griffin Team et al., 2024).
Mistral (Jiang et al., 2023) and Llama2 (Touvron et al., 2023)

Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and
Josh Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021.

Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao,
Weizhu Chen, and Noah A Smith. Finetuning pretrained transformers into rnns. arXiv preprint
arXiv:2103.13076, 2021.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International conference on machine
learning, pp. 5156–5165. PMLR, 2020.


**WKV оператор**
В RWKV весовые коэффициенты $w_{t,i}$ рассматриваются, как вектор затухания по каналам. Этот вектор масштабируется в зависимости от относительной позиции и уменьшается c шагом времени в соответствии с правилом: 
$$w_{t,i} = -(t-i)w$$
В итоге оператор запишется
```math
WKV_t = \frac{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \odot v_i + e^{u+k_t}\odot v_t}{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u+k_t}}
```
Для понимания почему это работает надо рассматривать антологию архитектур. Среди которых выделяется ряд: LSTM GRU и вот тут определили WKV оператор и Mu оператор. В частности определили как связан оператор из сетей Трансформеры ATTN с оператором WKV. Говоря простым языком, ATTN общая структура, которую можно представить, как Softmax и как WKV-оператор. 

Отдельно стоит упомянуть прямую связь обходное значение. Весь элемент сети выражается, как разница входа и выхода. Изначально эта архитектурная особенность появилась ради возможности тренировать глубокие слои. 

Наш подход заключается в утверждении, что любую функцию многих переменных с входными значениями $v\in [0,1]$ можно представить, как суперпозицию (сложную функцию) от функций одного переменного и функцию сложения (функция двух переменных), см. KAT - теорема Колмогорова-Арнольда. Суперпозиция функций (операторов) в плане архитектуры сети - это каскад составленный из операторов.

Другое утверждение, что из функций смешивания (mix) можно построить: а) B-сплайн и б) функцию распределения в) цифровой фильтр. Оператор Mu следует преобразовать в элемент цифрового фильтра с обратной связью. 

**Нормализация слоя**

x :=  (x - x.mean)/x.std;

* mean - среднее арифметическое
* std - средне-квадратичное отклонение от среднего - норма вектора $\|x - \bar{x}\|$. 

std(x) = sqrt(mean(abs(x - x.mean())**2));



**RWKV** - рекурентные сети RNN с использованием оператора WKV.

$\mathop{\rm{lerp}}_\square (a,b) = mix(a,b, \mu_\square) = a + (b-a) \odot \mu_\square$  
$\mu_\square \in \mathbb{R}^D$ - для векторов $\{r,k,v,g\}$

$D$- размерность модели. $D/h$ - разбиение модели на группы (work_group, "head").

$$\square_t = \mathop{\rm{lerp}}_\square(x_t, x_{t-1}) W_\square, \square \in \{r,k,v,g\}$$
$$w = \exp(-\exp(\omega)), \omega \in \mathbb{R}^{D/h}$$

Зависимость коэффициента от значений входных данных вводят через оператор $\mathop{\rm{lora}}_\square$: 
$$\mu = lora_\square(x) = \lambda_\square + \tanh(x A_\square)B_\square$$

$$\mathop{\rm{ddlerp}}_\square(a,b) = a+ (b-a)\odot \mathop{\rm{lora}}_\square (a+ (b-a)\odot \mu_x)$$

Оператор WKV может быть представленн через рекурсию
```math
\begin{aligned}
wkv' &= s + diag(u) \cdot k^\mathsf{T} \cdot v\\
s_t &= diag(w) \cdot s_{t-1} + k^\mathsf{T} \cdot v
\end{aligned}
```


[[RWKV](https://arxiv.org/pdf/2404.05892)]


## Представление вероятностной логики

Допустим у нас имеется полный набор нормированных линейно-независимых функций определенных на интервале [0,1] и возвращающих значения [0,1]. Тогда можно обазовать Гильбертово или _вероятностное пространство_ с разложением по базису. Вероятностная логика рождается из определения независимых событий. 

переход от понятия множество выполняется через сопоставление операциям с множествами операций с числовой мерой множесва, вероятностью.
$$P(A\cap B)  = P(A)P(B)$$
$$P(A\cup B)  = P(A)+P(B) - P(A)P(B)$$

## Вырождение вероятностной логики в четкую логику

Утверждение. Если на входе вероятностная логическая модель принимает значения $\{0, 1\}$, для функции множества параметров заданных на интервале $[0, 1]$, то функцию можно представить в четкой логике.

Вырождение возможно только если мы не теряем значения при квантизации! 


## Разделение вычислительной сети на тактовые домены

